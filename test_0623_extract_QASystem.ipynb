{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e363fa31",
   "metadata": {},
   "source": [
    "# 프로세스 설명\n",
    "1. RAG를 LLM으로 개체와 관계를 추출 ->  지식 그래프를 구축\n",
    "\n",
    "2. 질문의 개체와 엔티티에서 정확히 일치하는 개체를 그래프에서 찾음.\n",
    "\n",
    "3. 정확히 일치하는 개체가 있다면 해당 개체로 검색. 없다면 벡터 유사도가 임계치 이상인 엔티티들로 검색\n",
    "\n",
    "4. 정확히 일치하는 관계가 있다면 해당 관계로 검색. 없다면 벡터 유사도가 임계치 이상인 관계로 검색\n",
    "\n",
    "5. 검색된 개체 또는 관계의 메타데이터에 포함된 페이지 번호로 실제 문서에서 검색된 개체 또는 관계가 포함된 문장을 찾고, 앞뒤에 있는 n문장을 LLM에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea483678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./data/split_file/anatomy/1_Embryology.md...\n",
      "Extraction for 1_Embryology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/1_Embryology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/2_Osteology.md...\n",
      "Extraction for 2_Osteology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/2_Osteology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/3_Syndesmology.md...\n",
      "Extraction for 3_Syndesmology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/3_Syndesmology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/4_Myology.md...\n",
      "Extraction for 4_Myology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/4_Myology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/5_Angiology.md...\n",
      "Extraction for 5_Angiology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/5_Angiology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/9_Neurology.md...\n",
      "Extraction for 9_Neurology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/9_Neurology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/11_Splanchnology.md...\n",
      "Extraction for 11_Splanchnology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/11_Splanchnology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/6_The_Arteries.md...\n",
      "Extraction for 6_The_Arteries.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/6_The_Arteries_extracted.txt\n",
      "Processing ./data/split_file/anatomy/7_The_Veins.md...\n",
      "Extraction for 7_The_Veins.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/7_The_Veins_extracted.txt\n",
      "Processing ./data/split_file/anatomy/8_The_Lymphatic_System.md...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from langchain.schema import Generation, LLMResult\n",
    "# from langchain.llms.base import LLM\n",
    "# from model_loader.config import generation_loader\n",
    "# from langchain_core.documents import Document\n",
    "# from model_loader.local_loader import LocalModelLoader\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformerj\n",
    "\n",
    "# def extract_entities_and_relations(input_dir, output_dir, pages_to_process=5):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     graph_generator = LLMGraphTransformer(llm=CustomLLM(generation_loader=generation_loader), strict_mode=False)\n",
    "\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if filename.endswith(\".md\"):\n",
    "#             filepath = os.path.join(input_dir, filename)\n",
    "#             print(f\"Processing {filepath}...\")\n",
    "#             with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 content = f.read()\n",
    "\n",
    "#             pages = content.split('\\n\\n') \n",
    "            \n",
    "#             extracted_text = []\n",
    "#             for i, page_content in enumerate(pages[:pages_to_process]):\n",
    "#                 if not page_content.strip():\n",
    "#                     continue\n",
    "\n",
    "#                 try:\n",
    "#                     documents = [Document(page_content=page_content)]\n",
    "                    \n",
    "#                     graph_documents = graph_generator.convert_to_graph_documents(documents)\n",
    "\n",
    "#                     for graph_document in graph_documents:\n",
    "#                         extracted_text.append(f\"--- Page {i+1} ---\")\n",
    "#                         extracted_text.append(\"Entities:\")\n",
    "#                         for entity in graph_document.nodes:\n",
    "#                             extracted_text.append(f\"  - {entity.type}: {entity.id}\")\n",
    "                        \n",
    "#                         extracted_text.append(\"\\nRelationships:\")\n",
    "#                         for relationship in graph_document.relationships:\n",
    "#                             extracted_text.append(f\"  - ({relationship.source.id})-[{relationship.type}]->({relationship.target.id})\")\n",
    "#                         extracted_text.append(\"\\n\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing page {i+1} of {filename}: {e}\")\n",
    "#                     extracted_text.append(f\"--- Error processing Page {i+1}: {e} ---\")\n",
    "                \n",
    "#             output_filename = os.path.splitext(filename)[0] + \"_extracted.txt\"\n",
    "#             output_filepath = os.path.join(output_dir, output_filename)\n",
    "#             with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "#                 out_f.write(\"\\n\".join(extracted_text))\n",
    "#             print(f\"Extraction for {filename} completed. Results saved to {output_filepath}\")\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     generation_loader: object\n",
    "#     is_local_model: bool = False\n",
    "#     llm_model: object = None\n",
    "#     llm_tokenizer: object = None\n",
    "\n",
    "#     def __init__(self, generation_loader, **kwargs):\n",
    "#         super().__init__(generation_loader=generation_loader, **kwargs) \n",
    "#         self.generation_loader = generation_loader\n",
    "#         self.is_local_model = isinstance(self.generation_loader, LocalModelLoader)\n",
    "#         if self.is_local_model:\n",
    "#             self.llm_model = self.generation_loader.model\n",
    "#             self.llm_tokenizer = self.generation_loader.tokenizer\n",
    "\n",
    "#     def _call(self, prompt: str, stop=None) -> str:\n",
    "#         if self.is_local_model:\n",
    "#             inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n",
    "#             max_new_tokens = 512\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.llm_model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens = max_new_tokens,\n",
    "#                     num_return_sequences = 1,\n",
    "#                     do_sample = True,\n",
    "#                     temperature = 0.4,\n",
    "#                     top_p = 0.9,\n",
    "#                     repetition_penalty = 1.2,\n",
    "#                     eos_token_id = self.llm_tokenizer.eos_token_id\n",
    "#                 )\n",
    "#             generated_text = self.llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "#         else:\n",
    "#             generated_text = self.generation_loader.generate(prompt)\n",
    "#         return generated_text\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom_generation_loader_llm\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_directory = \"./data/split_file/anatomy\"\n",
    "#     output_directory = \"./data/extracted_results/LLMGraphTransformer\"\n",
    "#     extract_entities_and_relations(input_directory, output_directory, pages_to_process=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43848d40",
   "metadata": {},
   "source": [
    "# 개체-관계 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fb26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685ea5442e954ef9a1a701245ce0a08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./data/split_file/anatomy/1_Embryology.md...\n",
      "Extraction for 1_Embryology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/1_Embryology_extracted_1.txt\n",
      "Processing ./data/split_file/anatomy/2_Osteology.md...\n",
      "Extraction for 2_Osteology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/2_Osteology_extracted_1.txt\n",
      "Processing ./data/split_file/anatomy/3_Syndesmology.md...\n",
      "Extraction for 3_Syndesmology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/3_Syndesmology_extracted_1.txt\n",
      "Processing ./data/split_file/anatomy/4_Myology.md...\n",
      "Extraction for 4_Myology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/4_Myology_extracted_1.txt\n",
      "Processing ./data/split_file/anatomy/5_Angiology.md...\n",
      "Extraction for 5_Angiology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/5_Angiology_extracted_1.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re \n",
    "from langchain.schema import Generation, LLMResult\n",
    "from langchain.llms.base import LLM\n",
    "from model_loader.config import generation_loader\n",
    "from langchain_core.documents import Document\n",
    "from model_loader.local_loader import LocalModelLoader\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# MY_NODE_TYPES = ['ADJECTIVE', 'ALIAS', 'ANATOMICAL_FORAMEN', 'ANATOMICAL_GROUP', 'ANATOMICAL_LANDMARK', 'ANATOMICAL_PROCESS', 'ANATOMICALREGION', 'ANATOMICALSPACE', 'ANATOMICAL_SURFACE', 'ANATOMICAL_TERM', 'ANATOMICALSTRUCTURE', 'ANATOMY', 'ANIMAL', 'APONEUROSIS', 'ARTIFACT', 'AXIS', 'BIOLOGICAL_MATERIAL', 'BIOLOGICAL_PROCESS', 'BIOLOGICALENTITY', 'BIOLOGICALPROCESS', 'BODYFLUID', 'BONE', 'BONE_AXIS', 'BONE_DESCRIPTION', 'BONE_SYSTEM', 'BONE_TISSUE', 'BONE_TYPE', 'BORDER', 'BOUNDARY', 'BRAIN_STRUCTURE', 'BRAINSTEM', 'CANAL', 'CAPSULE', 'CARTILAGE', 'CELL', 'CELL_TYPE', 'CELLSTRUCTURE', 'CELLTYPE', 'CHARACTERISTIC', 'CHEMICAL', 'CIRCLE', 'CIRCULATION', 'CIRCULATORY_SYSTEM', 'CREST', 'CURVE', 'DATA', 'DATE', 'DEPRESSION', 'DESCRIPTION', 'DIRECTION', 'DISK', 'DIVERTICULUM', 'DUCT', 'DYE', 'ELEMENT', 'EMINENCE', 'ENVIRONMENT', 'EVENT', 'EXTREMITY', 'FASCIA', 'FIBER', 'FIELD_OF_STUDY', 'FLUID', 'FORAMEN', 'FORMATION', 'GENDER', 'GEOMETRICPROPERTY', 'GLAND', 'GROOVE', 'GROUP', 'INFORMATION', 'JOINT', 'JUNCTION', 'LAW', 'LAYER', 'LIFE_STAGE', 'LIGAMENT', 'LINE', 'MANNER', 'MATERIALPROPERTY', 'MEATUS', 'MECHANISM', 'MEDICALCONDITION', 'MEMBRANE', 'MOLECULE', 'MUSCLE', 'MUSCLE_ACTION', 'NERVE', 'NERVOUS_SYSTEM', 'NETWORK', 'NODE', 'NOMENCLATURE', 'NOTCH', 'OBSERVATION', 'ORGAN', 'ORGAN_SYSTEM', 'ORGANELLE', 'ORGANISM', 'ORIGIN', 'PATH', 'PERSON', 'PHYSICAL_CONTACT', 'PHYSICALACTION', 'PHYSICALPROPERTY', 'PHYSICALSUPPORT', 'PLANE', 'PLATE', 'POINT', 'PROCESS', 'PROFESSION', 'PROPERTY', 'PROTEIN', 'PUBLICATION', 'QUANTITY', 'RIDGE', 'SECTION', 'SEPTUM', 'SHAPE', 'SINUS', 'SKIN', 'SPATIALARRANGEMENT', 'SPINE', 'STATE', 'SUBSTANCE', 'SURFACE', 'SYSTEM', 'TENDENCY', 'TENDON', 'THEORY', 'TISSUE', 'TOOL', 'TOOTH', 'TOPIC', 'TUBEROSITY', 'VALVE', 'VESICLE', 'VESSEL', 'WORK', 'ANATOMY', 'BODY_PART', 'LIGAMENT', 'NERVE']\n",
    "\n",
    "# MY_RELATIONSHIP_TYPES = ['ABDUCT', 'ABSORB', 'ACCOMPLISH', 'ACCUMULATE', 'ACTION', 'ADAPT', 'ADMIT', 'AFFECT', 'ALLOW', 'ANTAGONIZE', 'APPEAR', 'ARCHES_OVER', 'ARCH_FORWARD', 'ARE', 'ATTACH', 'BRING', 'CHARACTERIZE', 'ESTABLISH', 'EXPAND', 'FORM', 'FUSE', 'MAKE', 'OCCUPY', 'ORIFICE', 'OSSIFY', 'REPLACE', 'ARISE', 'ARRANGE', 'ARTICULATE', 'ASSIST', 'ASSUME', 'ATTACHMENT', 'AUTHORED', 'AUTHOR_OF', 'SACROCOCCYGEAL_LIGAMENT_POSTERIOR', 'BEAR', 'BEGIN', 'BELIEVE', 'BELOW', 'BEND', 'BIND', 'BLEND', 'BOUND', 'BREAK', 'BRIDGE', 'DO', 'CAPABLE_OF', 'CARRY', 'CAUSE', 'CHANGE', 'CHECK', 'CLOSE', 'CLOTH', 'COME', 'COMMENCE', 'COMMUNICATE', 'COMPLETE', 'COMPOSE', 'COMPRESS', 'CONNECT', 'CONSTITUTE', 'CONTINUE', 'CONTRACT', 'CONTRIBUTE', 'CONVERGE', 'CONVERT', 'CORRESPOND', 'COVER', 'CREATE', 'CROSS', 'CURVE', 'CUT', 'SACROCOCCYGEAL_LIGAMENT_LATERAL', 'PUBIC_ARCH_FORMATION', 'SUPERIOR_PUBIC_LIGAMENT', 'DECREASE', 'DEEPEN', 'DEGENERATE', 'DEPOSIT', 'DERIVE_FROM', 'DESCEND', 'DESCRIBE', 'DETERMINE', 'DEVELOPE', 'DIFFER', 'DIMINISH', 'DIRECT', 'DISTRIBUTE', 'DIVIDE', 'CONTRIBUTE', 'DRAIN', 'DRAW', 'SACROCOCCYGEAL_LIGAMENT_ANTERIOR', 'EFFECT', 'ELONGATE', 'EMBRACE', 'ENCIRCLE', 'ENCLOSE', 'END', 'ENSHEATHED_BY', 'ENTER', 'ENVELOP', 'EVERT', 'EXAMINE', 'EXCLUDE', 'EXHIBIT', 'EXIST', 'EXPAND', 'EXTEND', 'FAIL', 'FILL', 'FIT', 'FIX', 'FLARE', 'FLOW', 'FOLLOW', 'ATTACH', 'PROTECT', 'FUNCTION', 'FUSE', 'GIVE', 'GLIDE', 'GROOVE', 'GROUP', 'GROW', 'GUIDE', 'HARMONIZE', 'HAS', 'HAS_ABNORMALITY', 'HAS_ADAPTATION', 'HAS_AGE', 'HAS_ALIAS', 'HAS_ANGLE', 'HAS_APPEARANCE', 'HAS_ARCH', 'HAS_ARRANGEMENT', 'HAS_ARTICULATION', 'HAS_ATTACHMENT', 'HAS_BASE', 'HAS_BONE', 'HAS_CENTER', 'HAS_CIRCUMFERENCE', 'HAS_CONDITION', 'HAS_COUNT', 'HAS_DATE', 'HAS_DEPRESSION', 'HAS_DIAMETER', 'HAS_DISLOCATION', 'HAS_ELASTICITY', 'HAS_ENDS', 'HAS_ESTIMATED_AGE', 'HAS_EXAMPLE', 'HAS_EXCEPTION', 'HAS_FACET', 'HAS_FACETS', 'HAS_FEATURE', 'HAS_FIBERS', 'HAS_FLOOR_OF', 'HAS_FORCE', 'HAS_FUNCTION', 'HAS_FUNCTION_OF', 'HAS_GROOVE', 'HAS_GROWTH', 'HAS_IMPRESSION', 'HAS_INSERTION', 'HAS_JOINT', 'HAS_LAYER', 'HAS_LENGTH', 'HAS_LIGAMENT', 'HAS_LIMITED', 'HAS_LINE', 'HAS_LINING', 'HAS_LOCATION', 'HAS_MAXIMUM', 'HAS_NUCLEI', 'HAS_NUMBER', 'HAS_NUMBER_OF_BONES', 'HAS_ORIGIN', 'HAS_ORIGIN_ON', 'HAS_PLAN', 'HAS_POSITION', 'HAS_PRIMARY_CENTER', 'HAS_PROCESS', 'HAS_PROPERTY', 'HAS_PULL', 'HAS_QUANTITY', 'HAS_RAMIFICATIONS', 'HAS_RESISTANCE', 'HAS_SCALE', 'HAS_SECONDARY_CENTER', 'HAS_SECTION', 'HAS_SEGMENTATION', 'HAS_SEGMENTS', 'HAS_SHAPE', 'HAS_SHEATH', 'HAS_SIMILAR_ARRANGEMENT_TO', 'HAS_SIZE', 'HAS_SMOOTHNESS', 'HAS_SPACES', 'HAS_STAGE', 'HAS_SURFACE', 'HAS_THICKNESS', 'HAS_TYPE', 'HAS_VARIETY', 'HAS_VOLUME', 'HAVE', 'HOLD', 'IMBED', 'IMPLANT', 'IMPORTANT', 'INCREASE', 'INDENT', 'INDICATE', 'INHERIT', 'INSERT', 'INTERPOSE', 'INTERRUPT', 'INTERVENE', 'INVADE', 'INVEST', 'INVOLVE', 'CONTACT', 'IS', 'ACCOMPANY', 'ACCURATE', 'BE_KNOWN_AS', 'APPLY', 'PROLONG', 'BEHIND', 'BLEND', 'BUILD', 'CALL', 'CARRY', 'CHECK', 'BE_CLOSER_TO', 'COMPLETE', 'COMPLICATE', 'CONCAVE', 'BE_CONDITION', 'CONTINUE', 'CONVERT', 'CONVEY', 'COVER', 'DEPRESS', 'DEVELOP', 'DIRECT', 'DIVIDE', 'DRAW', 'EFFECT', 'ELEVATE', 'ENCLOSE', 'ENLARGE', 'BE_EXCEPTION', 'EXPAND', 'EXPEL', 'FILL', 'FIX', 'BE_FOR', 'FORM', 'BE_FREE_FROM', 'FUSE', 'BE_HOMOLOGUE_OF', 'BE_IN', 'INCLUDE', 'INCREASE', 'INDICATE', 'INFRONT_OF', 'BE_INTERMEDIATE', 'INVOLVE', 'LARGER_THAN', 'BE_LIABLE_TO', 'LIMIT', 'LINE', 'LODGE', 'BE_MADE_OF', 'BE_MORE_EXTENSIVE_BETWEEN', 'NARROW', 'CONNECTED_TO', 'NOT_COVERED_BY', 'NOT_RECOGNIZE', 'BE_OF_SIZE', 'PERFORATE', 'PIERCE', 'PLACE', 'BE_POINT_OF', 'BE_POSSIBLE_BY', 'PRESENT_AS', 'PRESS', 'PREVENT', 'PUBLISH', 'RAISE', 'BE_RARE_IN', 'REDUCE', 'REGARD', 'RELAX', 'TENSE', 'RETURN', 'ROOF', 'SEGMENT', 'SHOW', 'BE_SIMILAR_TO', 'BE_SMALLER_THAN', 'STRAIGHTEN', 'STRETCH', 'SUBDIVIDE', 'TERM', 'BE_THICKEST_ALONG', 'TILT', 'BE_TRACE_OF', 'VESTIGIAL', 'MOVE_TRUNK_FORWARD', 'LACK', 'LEAVE', 'LIE', 'LIMIT', 'LINE', 'LIVE_FROM', 'LOCATE', 'LODGE', 'LOOK', 'LOSE', 'LIE_BETWEEN', 'MAGNIFY_AT', 'MAINTAIN', 'MAKE', 'MARK', 'UTILIZE_FOR', 'CONSTITUTE', 'LIE_ON', 'OCCUR_IN', 'MEET_WITH', 'MEET', 'FUSE', 'MENTION', 'MIGRATE_TO', 'MISTRANSLATE_AS', 'MIX', 'MODIFY_TO', 'MOST_COMMON_IN', 'MOVE', 'NOT_PRESENT_IN', 'NUMBER', 'OBLITERATE', 'OCCUPY', 'OCCUR', 'OPEN', 'ORIGIN', 'OSSIFY_FROM', 'OVERCOME', 'OVERLAP', 'OWE', 'PARALLEL_TO', 'INSERT', 'PART_OF', 'PASS', 'PASSAGE', 'PERFORM', 'PERMIT', 'PERSIST', 'PLACE_AT', 'POINT_OUT', 'POSSESS', 'PRECEDE', 'PRESENT', 'PROJECT', 'PROLONG_FROM', 'PROPOSE', 'PROTECT', 'PROTRUDE', 'PROVIDE', 'PUBLISH_IN', 'PUMP_THROUGH', 'RADIATE_FROM', 'RAISE', 'RANGE_FROM', 'REACHE', 'RECEIVE', 'REFLECT', 'REGARD', 'REGULATE', 'RELEASE', 'REPRESENT', 'RESEMBLE', 'RESIST', 'REST_ON', 'RETAIN', 'RETARD', 'RETRACT', 'RETURN_TO', 'RETURN', 'REVOLVE_UPON', 'RISE_FROM', 'ROLL_UPON', 'ROTATE', 'RUDIMENT_OF', 'RUN', 'SEPARATE', 'SHRIVEL', 'SIMILAR_TO', 'SITUATE', 'SPREAD_OVER', 'SPRING_FROM', 'STAIN_WITH', 'START_AT', 'STATE', 'STEADY', 'STRENGTHEN', 'STRETCH', 'SUFFICE_TO_RETAIN', 'SUPPLY', 'SUPPORT', 'SURROUND', 'SUSPEND_FROM', 'SYNCHRONIZE_WITH', 'TAKE_FROM', 'TEND_TO', 'THICKEST_AT', 'TRACE', 'TRANSFER', 'TRANSFORM', 'TRANSMIT', 'TURN_AROUND', 'TYPE_OF', 'UNDERGO', 'UNION_TAKE_PLACE', 'UNITE', 'USE', 'CLOSE_ABOUT', 'VARY_IN', 'VISUALIZE_IN', 'ACCOMPANY', 'AFFPRD_SURFACE_FOR', 'BE_KNOWN_AS', 'ANTAGONIST_OF', 'APPEAR_IN', 'CLOSE_AT', 'COMPOSED', 'DISTRIBUTE', 'BE_FOUND_IN', 'ARISE_FROM', 'ARRANGE', 'ATTACHMENT', 'AUTHOR', 'SEPARATED', 'BEHIND', 'ACT_ON', 'BLEND', 'BLOOD_FROM', 'BRANCH_OF', 'SEPARATE_FROM', 'CAVITY_OF', 'COLLECT', 'COMMUNICATE_WITH', 'COMPLETE', 'COMPOSE', 'COMPRISE', 'CONSTITUTE', 'CONTAIN', 'CONTINUOUS_WITH', 'CONVERGE_TO', 'CONVERT_TO', 'CROSS', 'DEFICIENT_IN', 'DESCEND', 'DESCRIBE_AS', 'DISAPPEAR_UP_TO', 'DISTRIBUTED_TO', 'DRAW', 'ENCIRCLE', 'ENCLOSE', 'END_IN', 'OPEN_INTO', 'ENLARGED', 'ENTER', 'ENTRANCE', 'ESTABLISH', 'EXIST_IN_MIDDLE_OF', 'EXIT', 'EXPANSION_FROM_TENDON', 'FALL_ON', 'FILL', 'FIRST_INDICATION_OF', 'FIX', 'FIND_IN', 'FUSE', 'FUTURE', 'GIVE', 'GROOVED_FOR', 'HAVE', 'HIDE', 'HOLD', 'INDICATE', 'INNERVATE', 'INSERTION', 'INTEGRAL_PART_OF', 'INTERPOSE_BETWEEN', 'INTERRUPT', 'INTERVENE', 'BLEND', 'INVOLVE', 'CONTINUOUS_WITH', 'BE_IN', 'LOCATE', 'COMPOSE', 'NOT_RETURN_TO', 'CONVEY_BY', 'CONVEY_TO', 'SITUATE_BETWEEN', 'JUNCTION_OF', 'LACK', 'LIE', 'LINE', 'LOCATION', 'LODGE', 'LOOSENESS', 'LOWER_BORDER_OF', 'MARK', 'MEASURE', 'MIGRATE_OVER', 'MOVEMENT', 'OCCUPY', 'ONE_FOR', 'OPEN', 'OPPONENT', 'ORIGIN', 'PASS_FORWARD_TO', 'PASSAGE', 'PERFORM', 'PERMIT', 'PLUG', 'PRESENT', 'EXERT_ON', 'PRODUCE', 'PROJECT', 'PROLONGATION_UPWARD_OF', 'PULL', 'PURPOSE', 'PUSH', 'RECEIVE', 'REGULATION', 'RELATION', 'REPRESENT', 'RESIST', 'RETURN_BLOOD_TO', 'RISE', 'ROOF', 'SEPARATE', 'SERVE', 'SITUATE', 'COMPARE_SIZE', 'BE_STRONGEST', 'BE_MOST_DISTINCTSTRUCTURE', 'SUBDIVIDE', 'PREVENT', 'SUPPORT', 'PROTECT', 'SURMOUNT', 'TERMINATE', 'THICKEST_IN', 'THIN', 'TRANSMIT', 'TRAVERSE', 'TRUNK_OF', 'UNDER_SURFACE_OF', 'UNDERGO', 'UNITE', 'VARY', 'VISIBLE']\n",
    "\n",
    "def extract_entities_and_relations(input_dir, output_dir, pages_to_process=None): \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    graph_generator = LLMGraphTransformer(\n",
    "        llm=CustomLLM(generation_loader=generation_loader),\n",
    "        strict_mode=False, \n",
    "        # allowed_nodes=MY_NODE_TYPES,\n",
    "        # allowed_relationships=MY_RELATIONSHIP_TYPES\n",
    "    )\n",
    "\n",
    "    target_files = [\n",
    "        \"1_Embryology.md\",\n",
    "        \"2_Osteology.md\",\n",
    "        \"3_Syndesmology.md\",\n",
    "        \"4_Myology.md\",\n",
    "        \"5_Angiology.md\"\n",
    "    ]\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename in target_files and filename.endswith(\".md\"):\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            print(f\"Processing {filepath}...\")\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            page_contents = re.findall(r'(#### Page \\d+\\n.*?)(?=#### Page \\d+\\n|$)', content, re.DOTALL)\n",
    "            \n",
    "            extracted_text = []\n",
    "            pages_to_iterate = page_contents if pages_to_process is None else page_contents[:pages_to_process]\n",
    "            \n",
    "            for i, page_content_raw in enumerate(pages_to_iterate): \n",
    "                if not page_content_raw.strip():\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    match = re.search(r'#### Page (\\d+)', page_content_raw)\n",
    "                    page_number = match.group(1) if match else str(i+1)\n",
    "\n",
    "                    documents = [Document(page_content=page_content_raw, metadata={\"page_number\": page_number, \"source_file\": filename})]\n",
    "                    \n",
    "                    graph_documents = graph_generator.convert_to_graph_documents(documents)\n",
    "\n",
    "                    for graph_document in graph_documents:\n",
    "                        doc_page_number = graph_document.source.metadata.get(\"page_number\", \"Unknown\")\n",
    "                        doc_source_file = graph_document.source.metadata.get(\"source_file\", \"Unknown\")\n",
    "\n",
    "                        extracted_text.append(f\"--- Page {doc_page_number} (File: {doc_source_file}) ---\") \n",
    "                        extracted_text.append(\"Entities:\")\n",
    "                        for entity in graph_document.nodes:\n",
    "                            extracted_text.append(f\"  - {entity.type}: {entity.id} (Page: {doc_page_number})\")\n",
    "                        \n",
    "                        extracted_text.append(\"\\nRelationships:\")\n",
    "                        for relationship in graph_document.relationships:\n",
    "                            extracted_text.append(f\"  - ({relationship.source.id})-[{relationship.type}]->({relationship.target.id}) (Page: {doc_page_number})\")\n",
    "                        extracted_text.append(\"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    match = re.search(r'#### Page (\\d+)', page_content_raw)\n",
    "                    page_number = match.group(1) if match else str(i+1)\n",
    "                    print(f\"Error processing page {page_number} of {filename}: {e}\")\n",
    "                    extracted_text.append(f\"--- Error processing Page {page_number}: {e} ---\")\n",
    "                \n",
    "            output_filename = os.path.splitext(filename)[0] + \"_extracted_1.txt\" ####\n",
    "            output_filepath = os.path.join(output_dir, output_filename)\n",
    "            with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                out_f.write(\"\\n\".join(extracted_text))\n",
    "            print(f\"Extraction for {filename} completed. Results saved to {output_filepath}\")\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    generation_loader: object\n",
    "    is_local_model: bool = False\n",
    "    llm_model: object = None\n",
    "    llm_tokenizer: object = None\n",
    "\n",
    "    def __init__(self, generation_loader, **kwargs):\n",
    "        super().__init__(generation_loader=generation_loader, **kwargs) \n",
    "        self.generation_loader = generation_loader\n",
    "        self.is_local_model = isinstance(self.generation_loader, LocalModelLoader)\n",
    "        if self.is_local_model:\n",
    "            self.llm_model = self.generation_loader.model\n",
    "            self.llm_tokenizer = self.generation_loader.tokenizer\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        if self.is_local_model:\n",
    "            inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n",
    "            max_new_tokens = 512\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.llm_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    num_return_sequences = 1,\n",
    "                    do_sample = True,\n",
    "                    temperature = 0.4,\n",
    "                    top_p = 0.9,\n",
    "                    repetition_penalty = 1.2,\n",
    "                    eos_token_id = self.llm_tokenizer.eos_token_id\n",
    "                )\n",
    "            generated_text = self.llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        else:\n",
    "            generated_text = self.generation_loader.generate(prompt)\n",
    "        return generated_text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_generation_loader_llm\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"./data/split_file/anatomy\"\n",
    "    output_directory = \"./data/extracted_results/LLMGraphTransformer\"\n",
    "    extract_entities_and_relations(input_directory, output_directory, pages_to_process=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416e8ed",
   "metadata": {},
   "source": [
    "# 유일한 개체-관계 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83772a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities = ['Abnormality', 'Action', 'Adjective', 'Age', 'Alias', 'Anatomical Feature', 'Anatomical Foramen', 'Anatomical Group', 'Anatomical Landmark', 'Anatomical Location', 'Anatomical Part', 'Anatomical Process', 'Anatomical Region', 'Anatomical Space', 'Anatomical Structure', 'Anatomical Surface', 'Anatomical Term', 'AnatomicalLocation', 'AnatomicalStructure', 'Anatomical_Structure', 'Anatomy', 'Angle', 'Animal', 'Aponeurosis', 'Area', 'Artery', 'Article', 'Articulation', 'Articulation Type', 'Artifact', 'Author', 'Axis', 'Behavior', 'Biological Entity', 'Biological Material', 'Biological Process', 'Biological Structure', 'Biological Substance', 'Biological Tissue', 'BiologicalEntity', 'BiologicalProcess', 'BiologicalStructure', 'Body Part', 'Body Region', 'BodyFluid', 'BodyPart', 'Body_Part', 'Body_Tissue', 'Body_part', 'Bone', 'Bone Axis', 'Bone Description', 'Bone Feature', 'Bone Part', 'Bone Region', 'Bone Section', 'Bone Structure', 'Bone Surface', 'Bone System', 'Bone Tissue', 'Bone Type', 'BoneGroup', 'BonePart', 'Bone_Part', 'Book', 'Border', 'Boundary', 'Brain Structure', 'Brainstem', 'Canal', 'Capsule', 'Cartilage', 'Cavity', 'Cell', 'Cell Component', 'Cell Type', 'CellStructure', 'CellType', 'Characteristic', 'Chemical', 'Circle', 'Circulation', 'Circulatory System', 'Condition', 'Connective Tissue', 'Connective_Tissue', 'Crest', 'Curve', 'Data', 'Date', 'Deformity', 'Depression', 'Description', 'Diameter', 'Direction', 'Disease', 'Disk', 'Diverticulum', 'Duct', 'Dye', 'Element', 'Eminence', 'Environment', 'Event', 'Extremity', 'Fascia', 'Feature', 'Fiber', 'Field of Study', 'Figure', 'Fluid', 'Foramen', 'Force', 'Formation', 'Function', 'Gender', 'GeometricProperty', 'Gland', 'Groove', 'Group', 'Image', 'Information', 'Injury', 'Joint', 'Joint Type', 'JointType', 'Junction', 'Law', 'Layer', 'Life Stage', 'Ligament', 'Line', 'Location', 'Manner', 'Mass', 'MaterialProperty', 'Measurement', 'Meatus', 'Mechanism', 'Medical Condition', 'Medical Finding', 'MedicalCondition', 'Medical_Concept', 'Membrane', 'Molecule', 'Motion', 'Movement', 'Muscle', 'Muscle Action', 'Muscle Fiber', 'Muscle Group', 'MuscleGroup', 'MusclePart', 'Muscle_Group', 'Muscle_Tendon', 'Nerve', 'Nervous System', 'Network', 'Node', 'Nomenclature', 'Notch', 'Number', 'Observation', 'Organ', 'Organ System', 'Organelle', 'Organism', 'Origin', 'Part', 'Path', 'Person', 'Physical Contact', 'Physical Force', 'Physical Property', 'PhysicalAction', 'PhysicalProperty', 'PhysicalQuantity', 'PhysicalSupport', 'Physiological Process', 'PhysiologicalProcess', 'Plane', 'Plate', 'Point', 'Process', 'Profession', 'Property', 'Protein', 'Publication', 'Quantity', 'Region', 'Ridge', 'Scientific Work', 'Section', 'Septum', 'Shape', 'Sinus', 'Size', 'Skin', 'Space', 'SpatialArrangement', 'Spine', 'State', 'Structure', 'Substance', 'Surface', 'Symptom', 'System', 'Tendency', 'Tendon', 'Tendons', 'Theory', 'Time', 'TimeDuration', 'Tissue', 'TissueType', 'Tool', 'Tooth', 'Topic', 'Tuberosity', 'Value', 'Valve', 'Vein', 'Vesicle', 'Vessel', 'Volume', 'Volume Number', 'VolumeNumber', 'Work', 'Year', 'anatomical structure', 'anatomy', 'body_part', 'bone', 'ligament', 'muscle', 'nerve', 'tissue', 'vessel']\n",
      "relations = ['ABDUCTS', 'ABSORB', 'ABSORBED_BY', 'ABSORBS', 'ACCOMPLISHES', 'ACCUMULATES_IN', 'ACROSS', 'ACTIONS', 'ACTS_AS', 'ACTS_ON', 'ACTS_UPON', 'ADAPTS', 'ADHERES_TO', 'ADMIT', 'AFFECT', 'AFFECTS', 'AFFORDS_ATTACHMENT_TO', 'ALLOWS', 'ALLOWS_PASSAGE_OF', 'ANTAGONIZES', 'APEX_CONNECTED_WITH', 'APPEAR', 'APPEARS', 'APPEARS_AT', 'APPEARS_AT_WEEK', 'APPEARS_IN', 'ARCHES_OVER', 'ARCH_FORWARD', 'ARE', 'ARE_ATTACHED_TO', 'ARE_BROUGHT_INTO', 'ARE_CHARACTERIZED_BY', 'ARE_ESTABLISHED_BY', 'ARE_EXPANDED_FOR', 'ARE_FORMED_FROM', 'ARE_FOUND_IN', 'ARE_FUSED', 'ARE_IN_RELATION_WITH', 'ARE_LAST_FORMED', 'ARE_MADE_OF', 'ARE_OCCUPIED_BY', 'ARE_OF', 'ARE_ORIFICES_OF', 'ARE_OSSIFIED', 'ARE_PART_OF', 'ARE_REPLACED_BY', 'ARE_SURROUNDED_BY', 'ARISE', 'ARISES_FROM', 'ARISE_FROM', 'ARRANGED_INTO', 'ARRANGE_IN', 'ARRANGE_INTO', 'ARTICULATE', 'ARTICULATES', 'ARTICULATES_WITH', 'ARTICULATE_WITH', 'ASSIST', 'ASSISTS', 'ASSISTS_BY', 'ASSISTS_IN', 'ASSISTS_IN_FORMING', 'ASSISTS_IN_OPENING', 'ASSOCIATED_WITH', 'ASSUME', 'ASSUMES', 'ATTACHED_ABOVE', 'ATTACHED_BELOW', 'ATTACHED_TO', 'ATTACHES_TO', 'ATTACHMENT', 'ATTACHMENTS', 'ATTACHMENT_SITE', 'ATTACHMENT_TO', 'AUTHORED', 'AUTHOR_OF', 'Arises from the margin of the lower orifice of the sacral canal and descends to be inserted into the posterior surface of the coccyx.', 'BEARS', 'BECOMES', 'BEGINS', 'BEGINS_AT', 'BEGINS_TO_OSSIFY', 'BEGIN_APPEARANCE', 'BELIEVE', 'BELOW', 'BENDS', 'BINDS', 'BLENDED_WITH', 'BLENDS_WITH', 'BOUNDED_BY', 'BOUND_TO', 'BREAKS_DOWN_INTO', 'BRIDGES_OVER', 'CAN_CAUSE', 'CAN_DO', 'CAPABLE_OF', 'CARRIED_FROM', 'CARRIED_IN', 'CARRIES', 'CARRIES_TO', 'CARRY', 'CARRY_BLOOD_TO', 'CAUSED_BY', 'CAUSES', 'CHANGES', 'CHECKS', 'CIRCULATES_THROUGH', 'CLOSED', 'CLOSES', 'CLOTHS', 'COME_INTO', 'COME_INTO_CONTACT', 'COMMENCES_AT', 'COMMUNICATES_BETWEEN', 'COMMUNICATES_WITH', 'COMPLETE', 'COMPLETED_BY', 'COMPONENT_OF', 'COMPOSED_OF', 'COMPRESSES', 'CONNECT', 'CONNECTED_BY', 'CONNECTED_TO', 'CONNECTS', 'CONNECTS_TO', 'CONNECTS_WITH', 'CONSISTS_OF', 'CONSIST_OF', 'CONSTITUTE', 'CONSTITUTES', 'CONTAIN', 'CONTAINED_IN', 'CONTAINS', 'CONTINUED_FROM', 'CONTINUED_ON', 'CONTINUES', 'CONTINUOUS_WITH', 'CONTRACTS', 'CONTRIBUTES_TO', 'CONVERGE_TO', 'CONVERTED_INTO', 'CONVERTS', 'CONVERTS_INTO', 'CONVEYS', 'CONVEYS_TO', 'CORRESPONDS_TO', 'CORRESPONDS_WITH', 'CORRESPOND_TO', 'COVER', 'COVERED', 'COVERED_BY', 'COVERS', 'CREATE', 'CREATED', 'CROSS', 'CROSSES', 'CURVES_OVER', 'CUT_OFF', 'Connects the transverse process of the coccyx to the lower lateral angle of the sacrum.', 'Connects the two pubic bones below, forming the upper boundary of the pubic arch.', 'Connects the two pubic bones superiorly, extending laterally to the pubic tubercles.', 'DECREASES_IN', 'DECREASE_IN', 'DEEPENS', 'DEGENERATES_INTO', 'DEPOSITED_IN', 'DERIVED_FROM', 'DERIVES_NUTRIMENT_FROM', 'DESCENDS_FROM', 'DESCENDS_INTO', 'DESCENDS_TO', 'DESCRIBES', 'DETERMINES', 'DEVELOPED_FROM', 'DEVELOPED_IN', 'DEVELOPED_INTO', 'DEVELOPED_WITHIN', 'DEVELOPES_INTO', 'DEVELOPMENT', 'DEVELOPS_FROM', 'DEVELOPS_INTO', 'DIFFERENTIATED_FROM', 'DIFFERENTIATES_INTO', 'DIFFERENT_FROM', 'DIFFERES_BETWEEN', 'DIFFERS_FROM', 'DIMINISHES', 'DIRECTED', 'DISTRIBUTED_TO', 'DISTRIBUTES', 'DIVIDED_BY', 'DIVIDED_INTO', 'DIVIDES', 'DIVIDES_INTO', 'DIVIDES_TO_ENCLOSE', 'DOES_NOT_CONTRIBUTE_TO', 'DRAINS', 'DRAW', 'DRAWN_OUT', 'DRAWS_FORWARD', 'DRAW_BACKWARD', 'DRAW_FORWARD', 'Descends from the anterior surface of the sacrum to the front of the coccyx.', 'EFFECT', 'EFFECTS', 'ELEVATES', 'ELONGATES', 'EMBRACE', 'ENCIRCLES', 'ENCLOSED_BY', 'ENCLOSED_IN', 'ENCLOSED_WITHIN', 'ENCLOSES', 'END', 'ENDS_AT', 'ENDS_IN', 'END_IN', 'ENSHEATHED_BY', 'ENTERS', 'ENVELOP', 'ENVELOPED_BY', 'ENVELOPS', 'EVERTS', 'EXAMINED', 'EXCLUDES', 'EXHIBITS', 'EXISTS', 'EXIST_AT', 'EXPANDED', 'EXPANDED_FROM', 'EXPANDS', 'EXPANDS_INTO', 'EXTENDS', 'EXTENDS_ALONG', 'EXTENDS_BETWEEN', 'EXTENDS_FROM', 'EXTENDS_INTO', 'EXTENDS_IN_MANNER', 'EXTENDS_THROUGH', 'EXTENDS_TO', 'EXTEND_BEYOND', 'EXTEND_FROM', 'EXTEND_INTO', 'EXTEND_TO', 'EXTENSION_OF', 'FAIL_TO_UNITE', 'FALLS_IN_FRONT_OF', 'FILLED_BY', 'FILLS', 'FIRST_TO_OSSIFY', 'FITS_INTO', 'FIXED_BY', 'FIXED_TO', 'FIXES', 'FLARED', 'FLEXES', 'FLOWS_FROM', 'FLOWS_INTO', 'FLOW_TO', 'FOLLOWS', 'FOR', 'FORM', 'FORMED', 'FORMED_AROUND', 'FORMED_BY', 'FORMED_FROM', 'FORMED_WITH', 'FORMS', 'FORMS_CENTER_OF', 'FORMS_CHANNEL_FOR', 'FORMS_CHIEF_BOND', 'FORMS_FROM', 'FORMS_PART_OF', 'FORMS_SHEATHS_FOR', 'FOR_ATTACHMENT_OF', 'FOR_PASSAGE_OF', 'FOR_PROTECTION_OF', 'FOR_TRANSMISSION_OF', 'FOUND', 'FOUND_ON', 'FUNCTION', 'FUNCTION_OF', 'FUSE', 'FUSED_WITH', 'FUSES_WITH', 'FUSE_WITH', 'GAVE', 'GIVEN_OFF', 'GIVEN_TO', 'GIVES_ATTACHMENT_TO', 'GIVES_OFF_EXPANSIONS', 'GIVES_ORIGIN_TO', 'GIVES_PASSAGE_TO', 'GIVES_RISE_TO', 'GIVE_BRANCHES', 'GIVE_OFF', 'GIVE_RISE_TO', 'GLIDES_ON', 'GROOVED_BY', 'GROUPED', 'GROUPED_INTO', 'GROW', 'GROWS', 'GROWS_FROM', 'GROW_INTO', 'GROW_OUT_FROM', 'GUIDED_BY', 'HARMONIZES', 'HAS', 'HAS_ABNORMALITY', 'HAS_ACTION', 'HAS_ADAPTATION', 'HAS_AGE', 'HAS_ALIAS', 'HAS_ANGLE', 'HAS_APPEARANCE', 'HAS_ARCH', 'HAS_ARRANGEMENT', 'HAS_ARTICULATION', 'HAS_ATTACHMENT', 'HAS_ATTRIBUTE', 'HAS_BASE', 'HAS_BONE', 'HAS_CENTER', 'HAS_CHARACTERISTIC', 'HAS_CIRCUMFERENCE', 'HAS_COMPONENT', 'HAS_CONDITION', 'HAS_COUNT', 'HAS_DATE', 'HAS_DEPRESSION', 'HAS_DIAMETER', 'HAS_DISLOCATION', 'HAS_ELASTICITY', 'HAS_ENDS', 'HAS_ESTIMATED_AGE', 'HAS_EXAMPLE', 'HAS_EXCEPTION', 'HAS_FACET', 'HAS_FACETS', 'HAS_FEATURE', 'HAS_FIBERS', 'HAS_FLOOR_OF', 'HAS_FORCE', 'HAS_FORM', 'HAS_FUNCTION', 'HAS_FUNCTION_OF', 'HAS_GROOVE', 'HAS_GROWTH', 'HAS_IMPRESSION', 'HAS_INFLUENCE', 'HAS_INSERTION', 'HAS_JOINT', 'HAS_LAYER', 'HAS_LENGTH', 'HAS_LIGAMENT', 'HAS_LIMITED', 'HAS_LINE', 'HAS_LINING', 'HAS_LOCATION', 'HAS_MAXIMUM', 'HAS_MOVEMENT', 'HAS_NUCLEI', 'HAS_NUMBER', 'HAS_NUMBER_OF_BONES', 'HAS_ORIGIN', 'HAS_ORIGIN_ON', 'HAS_PART', 'HAS_PLAN', 'HAS_POSITION', 'HAS_PRIMARY_CENTER', 'HAS_PROCESS', 'HAS_PROPERTY', 'HAS_PULL', 'HAS_QUANTITY', 'HAS_RAMIFICATIONS', 'HAS_RESISTANCE', 'HAS_SCALE', 'HAS_SECONDARY_CENTER', 'HAS_SECTION', 'HAS_SEGMENTATION', 'HAS_SEGMENTS', 'HAS_SHAPE', 'HAS_SHEATH', 'HAS_SIMILAR_ARRANGEMENT_TO', 'HAS_SIZE', 'HAS_SMOOTHNESS', 'HAS_SPACES', 'HAS_STAGE', 'HAS_SURFACE', 'HAS_THICKNESS', 'HAS_TYPE', 'HAS_VARIETY', 'HAS_VOLUME', 'HAVE', 'HELD_TOGETHER_BY', 'IMBEDDED_IN', 'IMPLANTED_INTO', 'IMPORTANT_FOR', 'INCLUDES', 'INCREASES', 'INCREASES_IN', 'INCREASE_IN', 'INDENT', 'INDICATES', 'INHERITED_FROM', 'INSERTED_IN', 'INSERTED_INTO', 'INSERTED_WITH', 'INSERTION', 'INSERTS_INTO', 'INSERT_INTO', 'INTERACTS_WITH', 'INTERPOSED', 'INTERPOSES', 'INTERRUPTS', 'INTERRUPTS_IN', 'INTERVENES', 'INVADE', 'INVEST', 'INVESTED_BY', 'INVESTMENT_OF', 'INVESTS', 'INVOLVES', 'IN_CONTACT_WITH', 'IN_RELATION_WITH', 'IS', 'IS_A', 'IS_ACCOMPANIED_BY', 'IS_ACCURATE', 'IS_ADJACENT_TO', 'IS_ALSO_KNOWN_AS', 'IS_APPLIED_TO', 'IS_ATTACHED_BY', 'IS_ATTACHED_TO', 'IS_A_PART_OF', 'IS_A_PROLONGATION_OF', 'IS_BEHIND', 'IS_BETWEEN', 'IS_BLENDED_WITH', 'IS_BUILT_UP', 'IS_CALLED', 'IS_CARRIED', 'IS_CHECKED', 'IS_CHECKED_BY', 'IS_CLOSER_TO', 'IS_COMPLETED_BY', 'IS_COMPLICATED_WITH', 'IS_COMPOSED_OF', 'IS_CONCAVE', 'IS_CONCAVE_WHERE', 'IS_CONDITION', 'IS_CONNECTED_BY', 'IS_CONNECTED_TO', 'IS_CONNECTED_WITH', 'IS_CONSTITUTED_BY', 'IS_CONTINUOUS_WITH', 'IS_CONVERTED_FROM', 'IS_CONVERTED_INTO', 'IS_CONVEYED_TO', 'IS_COVERED_BY', 'IS_DEPRESSED_BY', 'IS_DEVELOPED_FROM', 'IS_DEVELOPED_IN', 'IS_DIRECTED', 'IS_DIVIDED_INTO', 'IS_DRAWN', 'IS_DRAWN_FORWARD_BY', 'IS_EFFECTED_BY', 'IS_ELEVATED_BY', 'IS_ENCLOSED_IN', 'IS_ENLARGED', 'IS_ENTIRELY', 'IS_EXCEPTION', 'IS_EXPANDED', 'IS_EXPELLED', 'IS_FILLED_BY', 'IS_FILLED_WITH', 'IS_FIXED_TO', 'IS_FOR', 'IS_FORMED', 'IS_FORMED_BY', 'IS_FORMED_FROM', 'IS_FOUND_IN', 'IS_FREE_FROM', 'IS_FUSED_WITH', 'IS_HOMOLOGUE_OF', 'IS_IN', 'IS_INCLUDED_WITH', 'IS_INCREASED', 'IS_INDICATED_BY', 'IS_INFRONT_OF', 'IS_INTERMEDIATE', 'IS_INVOLVED_IN', 'IS_IN_CONTACT_WITH', 'IS_IN_RELATION_TO', 'IS_IN_RELATION_WITH', 'IS_LARGER_THAN', 'IS_LIABLE_TO', 'IS_LIMITED', 'IS_LIMITED_BY', 'IS_LIMITED_TO', 'IS_LINED_BY', 'IS_LOCATED', 'IS_LOCATED_AT', 'IS_LOCATED_BELOW', 'IS_LOCATED_BETWEEN', 'IS_LOCATED_IN', 'IS_LOCATED_IN_FRONT_OF', 'IS_LOCATED_NEAR', 'IS_LOCATED_ON', 'IS_LOCATION_OF', 'IS_LODGED_IN', 'IS_MADE_OF', 'IS_MORE_EXTENSIVE_BETWEEN', 'IS_NARROWED', 'IS_NOT_CONNECTED_TO', 'IS_NOT_COVERED_BY', 'IS_NOT_RECOGNIZED_AS', 'IS_OF_SIZE', 'IS_ON', 'IS_PART_OF', 'IS_PERFORATED_BY', 'IS_PIERCED_BY', 'IS_PLACED', 'IS_POINT_OF', 'IS_POSSIBLE_BY', 'IS_PRESENT_AS', 'IS_PRESSED', 'IS_PREVENTED', 'IS_PUBLISHED_IN', 'IS_RAISED', 'IS_RARE_IN', 'IS_REDUCED_TO', 'IS_REGARDED_AS', 'IS_RELAXED_DURING', 'IS_RENDERED_TENSE_DURING', 'IS_RETURNED_BY', 'IS_ROOFED_BY', 'IS_SEGMENTED_INTO', 'IS_SHAPED_AS', 'IS_SHOWN_IN', 'IS_SIMILAR_TO', 'IS_SITUATED', 'IS_SITUATED_AT', 'IS_SITUATED_BETWEEN', 'IS_SITUATED_ON', 'IS_SMALLER_THAN', 'IS_STRAIGHTENED', 'IS_STRENGTHENED_BY', 'IS_STRETCHED_DURING', 'IS_SUBDIVIDED_INTO', 'IS_SUPPLIED_BY', 'IS_SUPPORTED_ON', 'IS_SURROUNDED_BY', 'IS_TERMED', 'IS_THICKEST_ALONG', 'IS_TILTED', 'IS_TRACE_OF', 'IS_TYPE_OF', 'IS_VESTIGIAL', 'If both arms be fixed, the two muscles may assist the abdominal muscles and Pectorales in suspending and drawing the trunk forward, as in climbing.', 'JOINED', 'JOINED_TO', 'JOINS', 'JOIN_TO_FORM', 'LACKS', 'LEADS_TO', 'LEAVES', 'LIES_BELOW', 'LIES_BETWEEN', 'LIES_IN', 'LIES_INSIDE', 'LIES_UNDER', 'LIE_ON', 'LIMITED_BY', 'LIMITS', 'LINED_BY', 'LINES', 'LIVED_FROM', 'LOCATED_AROUND', 'LOCATED_AT', 'LOCATED_BELOW', 'LOCATED_BENEATH', 'LOCATED_BETWEEN', 'LOCATED_IN', 'LOCATED_IN_FRONT_OF', 'LOCATED_NEAR', 'LOCATED_ON', 'LOCATED_UNDER', 'LOCATED_UPON', 'LODGE', 'LODGED_IN', 'LODGES', 'LOOKS', 'LOSES', 'LOST', 'LYING_BETWEEN', 'Lies between the pubic bones.', 'MADE_UP_OF', 'MAGNIFIED_AT', 'MAINTAINS', 'MAKES', 'MARKS', 'MAY_BE_UTILIZED_FOR', 'MAY_CONSTITUTE', 'MAY_LIE_ON', 'MAY_OCCUR_IN', 'MEETS_WITH', 'MEET_AND_FUSE', 'MENTIONED', 'MENTIONED_BY', 'MIGRATE_TO', 'MISTRANSLATED_AS', 'MIX', 'MIXES_WITH', 'MODIFIES_TO', 'MOST_COMMON_IN', 'MOVES', 'MOVES_ON', 'MOVES_SIMULTANEOUSLY', 'MOVES_WITH', 'MOVE_WITH', 'NEAR', 'NOT_PRESENT_IN', 'NUMBER', 'OBLITERATED', 'OCCUPIED_BY', 'OCCUPIES', 'OCCURS_AROUND', 'OCCURS_BETWEEN', 'OCCURS_IN', 'OPENS_BY', 'OPENS_INTO', 'OPENS_ON', 'OPEN_INTO', 'OPEN_TO', 'ORIGIN', 'ORIGINALLY', 'ORIGINATE', 'ORIGINATES_AS', 'ORIGINATES_FROM', 'ORIGINATE_FROM', 'ORIGIN_OF', 'OSSFIED_FROM', 'OSSFIED_IN', 'OSSIFIED_FROM', 'OSSIFIES_AT', 'OSSIFIES_FROM', 'OSSIFIES_IN', 'OSSIFIES_TO_FORM', 'OVERCOMES', 'OVERLAPPED_BY', 'OVERLAPS', 'OWES', 'PARALLEL_TO', 'PART-OF', 'PARTLY_INSERTED', 'PART_OF', 'PART_OF_NOMENCLATURE', 'PASS', 'PASSAGE', 'PASSES', 'PASSES_ACROSS', 'PASSES_BETWEEN', 'PASSES_FROM', 'PASSES_THROUGH', 'PASSES_TO', 'PASS_BETWEEN', 'PASS_FROM', 'PASS_OVER', 'PASS_THROUGH', 'PASS_TO', 'PERFORATED_BY', 'PERFORMS', 'PERMITS', 'PERMITTED_BETWEEN', 'PERSISTS_AS', 'PERSISTS_BETWEEN', 'PERSIST_AS', 'PERSIST_IN', 'PLACED_AT', 'POINTED_OUT', 'POSSESSES', 'PRECEDES', 'PRESENTS', 'PRESENT_AT', 'PRESENT_IN', 'PREVENTS', 'PRODUCES', 'PROJECT', 'PROJECTS', 'PROJECTS_FROM', 'PROJECTS_INTO', 'PROJECTS_THROUGH', 'PROJECTS_TO', 'PROJECT_BACKWARD', 'PROJECT_FROM', 'PROJECT_INTO', 'PROJECT_LATERALLY', 'PROLONGED_FROM', 'PROPOSED', 'PROTECTS', 'PROTRUDES', 'PROVIDE', 'PROVIDES', 'PROVIDES_ATTACHMENT_FOR', 'PROVIDES_INSERTION', 'PROVIDE_PASSAGE', 'PUBLISHED_IN', 'PUMPED_THROUGH', 'Passes across the front of the articulation between the pubic bones.', 'RADIATE_FROM', 'RAISE', 'RAISES', 'RANGE_FROM', 'REACHES', 'RECEIVES', 'RECEIVES_FIBERS_FROM', 'RECEIVES_SLIPS_FROM', 'REFLECTED_FROM', 'REGARDED_AS', 'REGULATES', 'RELATED_TO', 'RELEASES', 'REPRESENT', 'REPRESENTS', 'RESEMBLE', 'RESEMBLES', 'RESIST', 'RESTS_ON', 'RESULTS_FROM', 'RETAINS', 'RETARDS', 'RETRACTS', 'RETURNED_TO', 'RETURNS', 'RETURNS_BLOOD_FROM', 'RETURNS_TO', 'RETURN_BLOOD_TO', 'REVOLVES_UPON', 'RISE_FROM', 'ROLLS_UPON', 'ROTATED', 'ROTATES', 'ROTATES_AROUND', 'ROTATES_AT', 'ROTATES_UPON', 'ROTATES_WITH', 'RUDIMENTS_OF', 'RUNS_ACROSS', 'RUNS_ACROSS_AND_CONNECTS', 'RUNS_BETWEEN', 'RUNS_THROUGH', 'RUNS_TO', 'RUN_THROUGH', 'SEPARATE', 'SEPARATED_BY', 'SEPARATED_FROM', 'SEPARATES', 'SERVES', 'SERVES_PURPOSE', 'SHRIVELS', 'SIMILAR_TO', 'SITUATED_AT', 'SITUATED_IN', 'SITUATED_ON', 'SPREADS_OVER', 'SPRINGS_FROM', 'SPRING_FROM', 'STAIN_WITH', 'STARTS_AT', 'STATES', 'STEADIES', 'STRENGTHEN', 'STRENGTHENED_BY', 'STRENGTHENS', 'STRETCHES_ACROSS', 'STRETCHES_BETWEEN', 'SUFFICES_TO_RETAIN', 'SUPPLIED_BY', 'SUPPLIED_THROUGH', 'SUPPLIES', 'SUPPLY', 'SUPPORT', 'SUPPORTS', 'SURROUNDED_BY', 'SURROUNDS', 'SUSPENDED_FROM', 'SYNCHRONIZES_WITH', 'TAKEN_FROM', 'TENDS_TO', 'TERMINATES_IN', 'THICKEST_AT', 'TILTS', 'TRACED_BY', 'TRANSFERRED_TO', 'TRANSFORMS_INTO', 'TRANSMITS', 'TRANSMITS_THROUGH', 'TRANSMITS_TO', 'TRANSMITTED_THROUGH', 'TRAVERSES', 'TURNS_AROUND', 'The Rhomboidei carry the inferior angle backward and upward, thus producing a slight rotation of the scapula upon the side of the chest, the Rhomboideus major acting especially on the inferior angle of the scapula, through the tendinous arch by which it is inserted.', 'The Rhomboidei, acting together with the middle and inferior fibers of the Trapezius, will retract the scapula.', 'The pubic bones articulate to form the pubic symphysis.', 'The sacrum and coccyx are connected by ligaments.', 'UNDERGO', 'UNDERGOES', 'UNION_TAKES_PLACE', 'UNITE', 'UNITED_TO', 'UNITED_WITH', 'UNITES_WITH', 'UNITE_IN', 'UNITE_TO_FORM', 'UNITE_WITH', 'USES', 'USUALLY_CLOSED_ABOUT', 'Unite the cornua of the sacrum and coccyx.', 'Unites the two pubic bones posteriorly.', 'VARY_IN', 'VISUALIZED_IN', 'WITH', 'abduct', 'accompany', 'action', 'adduct', 'adheres to', 'affords surfaces for', 'also known as', 'antagonist of', 'appear in', 'appear in region of', 'are', 'are closed at', 'are composed of', 'are distributed to', 'are found in', 'arise from', 'arise from upper part of odontoid process', 'arises from', 'arises_from', 'arranged around', 'arranged in two general systems, compressive and tensile', 'arranged parallel to', 'articulate with', 'articulates', 'articulates with', 'articulation with', 'ascending veins return blood from', 'assists', 'assists in forming', 'attached to', 'attached_to', 'attaches to', 'attaches_to', 'attachment', 'attachment for', 'attachment of', 'attachment to', 'author of', 'authored', 'become separated', 'behind', 'believed to act almost entirely on', 'between', 'blended with', 'blends with cranial dura mater', 'blood from', 'borders', 'bounded_by', 'bounds', 'branches of', 'can be separated from', 'carries', 'carries out', 'carry blood into', 'cavity of', 'collect blood from', 'collects between', 'communicates with', 'completed by', 'composed of', 'comprises', 'connect', 'connected by Articular Capsules', 'connected by Membrana Tectoria', 'connected to', 'connected with', 'connection', 'connects', 'connects vertebral column with the cranium', 'connects_to', 'consists of', 'constitutes', 'contained in', 'containing', 'contains', 'continuous with', 'converges to', 'converts_to', 'convey blood from', 'conveys', 'conveys blood to', 'covered by', 'crossed', 'crossed by', 'deficient in', 'derived from', 'descending veins return blood from', 'descends from', 'described as', 'developed from', 'developed_as', 'disappears up to', 'distributed to', 'divided by', 'divides', 'divides into', 'divides into capillary-like vessels (sinusoids)', 'drains plexus into', 'draws down', 'draws down and backward', 'during', 'effects', 'elevates', 'encircles', 'enclosed by', 'encloses', 'ends in', 'enlarged and opens into', 'entering', 'entrance for', 'entrance of', 'established between', 'exists in the middle of', 'exit for', 'exit of', 'expansion from tendon', 'extend', 'extend between', 'extend over', 'extended', 'extends from', 'extends from tip of odontoid process to anterior margin of foramen magnum', 'falls on', 'fill', 'first indication of', 'fixed', 'fixed into', 'fixed to', 'flex', 'form', 'formed', 'formed by', 'formed by expansion from', 'formed from', 'formed in', 'formed mainly by', 'forms', 'forms the summit of', 'found in', 'four for', 'fuse to form', 'fused with', 'future', 'gave number of chromosomes', 'gives', 'gives passage to', 'grooved for', 'has', 'has diameter', 'has part', 'has_part', 'have', 'hidden by', 'holds together', 'in relation with', 'indicates', 'indicating', 'innervates', 'inserted into', 'inserted into radial side of', 'inserted into rough depressions on medial sides of condyles', 'inserted into the same side of', 'inserted into the same side of the first phalanx', 'inserted into ulnar side of', 'inserted_into', 'insertion', 'insertion to', 'integral part of', 'interposed between', 'interrupted connection with', 'intervene between', 'intervenes between', 'intimately blended with', 'into', 'involves', 'is', 'is a', 'is associated with', 'is continuous with', 'is in', 'is located on', 'is made up of', 'is not returned directly to the heart, but is conveyed by the portal vein to the liver', 'is part of', 'is situated between', 'is the most prominent point of', 'junction of', 'lacks', 'lie side by side', 'lies between', 'lies near', 'ligament', 'lined_by', 'lines', 'located at', 'located in', 'located on', 'located_on', 'location', 'lodge', 'lodges', 'looseness', 'lower border of', 'marked by', 'marking off', 'measures', 'measures about', 'migrate over', 'movement', 'muscle', 'muscle_insertion', 'muscle_origin', 'not part of', 'occupies', 'of', 'on', 'one for', 'open into', 'open_by', 'openings of', 'opens into', 'opponent of', 'origin', 'origin for', 'origin from', 'originally consists of', 'ossified from', 'ossified in', 'ossifies_from', 'part of', 'part_of', 'pass forward to', 'passage for', 'passage of', 'passage through', 'passes into', 'passes through', 'performs', 'permits', 'plugs', 'presents', 'pressure exerted on under surface', 'produces', 'projects below', 'projects from', 'prolongation upward of', 'property_of', 'pulls', 'pulls upward', 'purpose', 'pushed up by tension', 'raphe', 'received into', 'receives', 'receives blood from', 'receives fibers from', 'reception of', 'regulation', 'relation', 'represent', 'resist', 'return blood to', 'rises', 'roofs in', 'rotate and flex', 'sac', 'separated by', 'separated by fibrous arch for passage of', 'separated from', 'separates', 'separating', 'serves for passage of', 'situated', 'size comparison', 'strongest and most distinct', 'structure', 'subdivided by', 'supplied by', 'supplied_with', 'supplies', 'support and prevent movement', 'supported by', 'supports', 'supports and protects', 'surmounted by', 'surrounds', 'tendons of the Interossei pass to their insertions', 'termination of', 'thickest in', 'thinned', 'transmission of', 'transmit', 'transmits', 'transmitted through', 'traversed by', 'trunk of', 'type of', 'under surface of', 'under surfaces of', 'undergoes development', 'undergoes_atrophy', 'undergoes_changes', 'united by', 'unites', 'varies from', 'visible_as']\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# def extract_and_print_unique_types_as_lists(input_dir):\n",
    "#     unique_entity_types = set()\n",
    "#     unique_relationship_types = set()\n",
    "\n",
    "#     relationship_pattern = re.compile(r\"\\-\\[(.+?)\\]\\->\")\n",
    "#     entity_pattern = re.compile(r\"^\\s*-\\s*([^:]+):\")\n",
    "\n",
    "#     if not os.path.isdir(input_dir):\n",
    "#         print(f\"오류: '{input_dir}' 디렉토리를 찾을 수 없습니다.\")\n",
    "#         return\n",
    "\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if filename.endswith(\"_extracted.txt\"):\n",
    "#             filepath = os.path.join(input_dir, filename)\n",
    "#             with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     relationship_match = relationship_pattern.search(line)\n",
    "#                     if relationship_match:\n",
    "#                         relationship_type = relationship_match.group(1).strip()\n",
    "#                         unique_relationship_types.add(relationship_type)\n",
    "#                     else:\n",
    "#                         entity_match = entity_pattern.match(line)\n",
    "#                         if entity_match:\n",
    "#                             entity_type = entity_match.group(1).strip()\n",
    "#                             if '(' not in entity_type and ')' not in entity_type:\n",
    "#                                 unique_entity_types.add(entity_type)\n",
    "\n",
    "#     sorted_entities = sorted(list(unique_entity_types))\n",
    "#     sorted_relationships = sorted(list(unique_relationship_types))\n",
    "\n",
    "#     print(\"entities =\", sorted_entities)\n",
    "#     print(\"relations =\", sorted_relationships)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_directory = \"./data/extracted_results/LLMGraphTransformer\"\n",
    "\n",
    "#     extract_and_print_unique_types_as_lists(input_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503f426",
   "metadata": {},
   "source": [
    "# 결과물 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def add_newline_after_comma(filepath) :\n",
    "#     try :\n",
    "#         with open(filepath, 'r', encoding=\"utf-8\") as f :\n",
    "#             content = f.read()\n",
    "\n",
    "#         modified_content = content.replace(\", \", \",\\n\")\n",
    "#         modified_content = content.replace(\",\", \",\\n\")\n",
    "\n",
    "#         with open(filepath, 'w', encoding=\"utf-8\") as f :\n",
    "#             f.write(modified_content)\n",
    "\n",
    "#     except FileNotFoundError :\n",
    "#         print(f\"{filepath}를 찾을 수 없음\")\n",
    "#     except Exception as e :\n",
    "#         print(f\"{filepath} 처리 중 오류 발생\")\n",
    "\n",
    "# if __name__ == \"__main__\" :\n",
    "#     files_to_process = [\"./relations.txt\", \"./entities.txt\"]\n",
    "#     for file in files_to_process :\n",
    "#         add_newline_after_comma(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff2c66",
   "metadata": {},
   "source": [
    "# 그래프 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3fe061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 처리 중: ./data/extracted_results/LLMGraphTransformer/1_Embryology_extracted_1.txt\n",
      "파일 처리 중: ./data/extracted_results/LLMGraphTransformer/2_Osteology_extracted_1.txt\n",
      "파일 처리 중: ./data/extracted_results/LLMGraphTransformer/3_Syndesmology_extracted_1.txt\n",
      "파일 처리 중: ./data/extracted_results/LLMGraphTransformer/4_Myology_extracted_1.txt\n",
      "파일 처리 중: ./data/extracted_results/LLMGraphTransformer/5_Angiology_extracted_1.txt\n",
      "\n",
      "--- 그래프 구축 완료 ---\n",
      "총 노드 수: 4061\n",
      "총 엣지 수: 4091\n",
      "\n",
      "그래프가 ./data/knowledge_graph/knowledge_graph_1.graphml 에 저장되었습니다.\n",
      "\n",
      "노드 수가 500개를 초과하여 자동 시각화를 건너뜁니다. (너무 오래 걸릴 수 있음)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def build_knowledge_graph(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    entity_pattern = re.compile(r\"\\s*-\\s*([^:]+):\\s*(.*?)\\s*\\((?:Page|페이지):\\s*(\\d+)\\)\")\n",
    "    relation_pattern = re.compile(r\"\\s*-\\s*\\((.*?)\\)-\\[(.*?)\\]->\\((.*?)\\)\\s*\\((?:Page|페이지):\\s*(\\d+)\\)\")\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\"_extracted_1.txt\"): ###############\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            print(f\"파일 처리 중: {filepath}\")\n",
    "\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            for match in entity_pattern.finditer(content):\n",
    "                node_type, node_id, page = match.groups()\n",
    "                node_id = node_id.strip()\n",
    "                if not G.has_node(node_id):\n",
    "                    G.add_node(node_id, type=node_type, source_page=page)\n",
    "                else:\n",
    "                    if isinstance(G.nodes[node_id]['source_page'], list):\n",
    "                        if page not in G.nodes[node_id]['source_page']:\n",
    "                            G.nodes[node_id]['source_page'].append(page)\n",
    "                    else:\n",
    "                        if G.nodes[node_id]['source_page'] != page:\n",
    "                           G.nodes[node_id]['source_page'] = [G.nodes[node_id]['source_page'], page]\n",
    "\n",
    "\n",
    "            for match in relation_pattern.finditer(content):\n",
    "                source_id, rel_type, target_id, page = match.groups()\n",
    "                source_id = source_id.strip()\n",
    "                target_id = target_id.strip()\n",
    "\n",
    "                if G.has_node(source_id) and G.has_node(target_id):\n",
    "                    G.add_edge(source_id, target_id, type=rel_type, source_page=page)\n",
    "\n",
    "    print(\"\\n--- 그래프 구축 완료 ---\")\n",
    "    print(f\"총 노드 수: {G.number_of_nodes()}\")\n",
    "    print(f\"총 엣지 수: {G.number_of_edges()}\")\n",
    "\n",
    "    for node, data in G.nodes(data=True) :\n",
    "        for key, value in data.items() :\n",
    "            if isinstance(value, list) :\n",
    "                G.nodes[node][key] = ','.join(map(str, value))\n",
    "\n",
    "    for u, v, data in G.edges(data=True) :\n",
    "        for key, value in data.items() :\n",
    "            if isinstance(value, list) :\n",
    "                G.edges[u, v][key] = ','.join(map(str, value))\n",
    "\n",
    "    graph_output_path = os.path.join(output_dir, \"knowledge_graph_1.graphml\") ###############\n",
    "    nx.write_graphml(G, graph_output_path)\n",
    "    print(f\"\\n그래프가 {graph_output_path} 에 저장되었습니다.\")\n",
    "\n",
    "    if G.number_of_nodes() > 500:\n",
    "        print(\"\\n노드 수가 500개를 초과하여 자동 시각화를 건너뜁니다. (너무 오래 걸릴 수 있음)\")\n",
    "        return\n",
    "\n",
    "    print(\"그래프 시각화 생성 중...\")\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    try:\n",
    "        font_path = \"c:/Windows/Fonts/malgun.ttf\"  \n",
    "        font_prop = fm.FontProperties(fname=font_path)\n",
    "        plt.rcParams['font.family'] = font_prop.get_name()\n",
    "    except FileNotFoundError:\n",
    "        print(\"맑은 고딕 폰트를 찾을 수 없습니다. 기본 폰트로 시각화합니다. (한글이 깨질 수 있음)\")\n",
    "\n",
    "    pos = nx.spring_layout(G, k=0.15, iterations=20, seed=42)\n",
    "    \n",
    "    node_labels = {node: node for node in G.nodes()}\n",
    "    edge_labels = nx.get_edge_attributes(G, 'type')\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='skyblue', alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=G.edges(), width=1.5, alpha=0.7, edge_color='gray', arrows=True, arrowsize=20)\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8, font_family=plt.rcParams['font.family'])\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6, font_color='red')\n",
    "\n",
    "    plt.title(\"지식 그래프 시각화\", fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    image_output_path = os.path.join(output_dir, \"knowledge_graph_1.png\") ###############\n",
    "    plt.savefig(image_output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"그래프 시각화 이미지가 {image_output_path} 에 저장되었습니다.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"./data/extracted_results/LLMGraphTransformer\"\n",
    "    output_directory = \"./data/knowledge_graph\"\n",
    "\n",
    "    build_knowledge_graph(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bcc31",
   "metadata": {},
   "source": [
    "# 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e91c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: What are the two essential components of a higher organism cell as defined in the text?\n",
      "답변: The two essential components of a higher organism cell are a soft jelly-like material called cytoplasm and a small spherical body called a nucleus (Page 6).\n",
      "\n",
      "질문: Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\n",
      "답변: 관련 정보를 지식 그래프에서 찾을 수 없습니다.\n",
      "\n",
      "질문: What is the primary role of the yolk-sac in the embryo's early development?\n",
      "답변: According to the text, the primary role of the yolk-sac in the embryo's early development is to provide **nourishment**. It is filled with vitelline fluid which *possibly* utilized for the embryo’s nutrition, and it establishes the **vitelline circulation** – a system where blood conveys nutrients from the yolk-sac to the developing embryo. \n",
      "\n",
      "The text also states that a small part of the yolk-sac is enclosed within the embryo and constitutes the primitive digestive tube, further emphasizing its importance in early development. While it eventually reduces in size, it initially plays a key role in providing sustenance to the embryo.\n",
      "\n",
      "질문: How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\n",
      "답변: The embryo is more completely separated from the yolk-sac (Page 33). The provided text does not state what the enclosed part of the yolk-sac forms.\n",
      "\n",
      "질문: What significant developments occur in a human embryo during the Second Week?\n",
      "답변: During the second week, the mesoderm outside the embryonic disk splits into two layers, enclosing an extra-embryonic coelom, and there is no trace of an intra-embryonic coelom at this stage (Page 31). Additionally, the limbs begin to appear as small elevations or buds at the side of the trunk (Page 30).\n",
      "\n",
      "질문: What are the key characteristics of the human embryo by the end of the Third Week?\n",
      "답변: By the end of the third week, the limbs begin to appear as small elevations or buds at the side of the trunk (Page 30). Additionally, by this stage, the human embryo has no trace of an intra-embryonic cœlom, and the mesoderm outside the embryonic disk is split into two layers enclosing an extra-embryonic cœlom (Page 31).\n",
      "\n",
      "질문: What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\n",
      "답변: The cells of a primitive segment differentiate into three groups: the trophoblast, which does not contribute to the formation of the embryo proper, and an inner cell-mass, from which the embryo is developed (Page 13). Additionally, there is a third layer of cells called the mesoderm, which extends laterally between the ectoderm and entoderm (Page 13).\n",
      "\n",
      "질문: How is each vertebral body formed from primitive segments during development?\n",
      "답변: Each vertebral body is formed from the posterior portion of one primitive segment and the anterior part of the segment immediately behind it (Page 38). Cells from the posterior mass grow into the intervals between the myotomes of the corresponding and succeeding segments (Page 38). These cells extend both dorsally and ventrally, with the dorsal extensions surrounding the neural tube to form the future vertebral arch, and the ventral extensions extending into the body-wall as the costal processes (Page 38).\n",
      "\n",
      "질문: What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\n",
      "답변: The sphenoidal air sinuses are two large cavities located within the interior of the body (corpus sphenoidale) of the sphenoid bone (Page 87). These cavities are separated from each other by a septum (Page 87).\n",
      "\n",
      "질문: Describe the sphenoidal rostrum and its articulation.\n",
      "답변: I am sorry, but the provided context does not contain information about the sphenoidal rostrum or its articulation. Therefore, I cannot answer your question based on the given text. I am answering based on the graph structure alone.\n",
      "\n",
      "질문: What is the tibia, and where is it located in the human leg?\n",
      "답변: The tibia, also known as the shin bone, is located at the medial side of the leg(Page 158). It is the longest bone in the skeleton, excluding the femur(Page 158). It is prismoid in form, expanded above at the knee-joint, contracted in the lower third, and again enlarged but to a lesser extent below(Page 158).\n",
      "\n",
      "질문: Describe the superior articular surface of the tibia's upper extremity.\n",
      "답변: The tibia is expanded above where it enters into the knee-joint (Page 158). Its superior articular surface is quadrilateral and smooth for articulation with the talus (Page 159). It is concave from before backward, broader in front than behind, and traversed from before backward by a slight elevation, separating two depressions (Page 159).\n",
      "\n",
      "질문: What are joints or articulations, and how are immovable joints characterized?\n",
      "답변: According to the provided text:\n",
      "\n",
      "**What are joints or articulations?**\n",
      "\n",
      "Joints or articulations are connections between bones.\n",
      "\n",
      "**How are immovable joints characterized?**\n",
      "\n",
      "Immovable joints are characterized by a lack of motion. The text doesn't provide specific details *about* how they are formed, but states they *are* characterized by being immovable.\n",
      "\n",
      "질문: How does the articular lamella differ from ordinary bone tissue?\n",
      "답변: I am sorry, but the provided context does not contain information about the articular lamella or how it differs from ordinary bone tissue. Therefore, I cannot answer your question based on the given text.\n",
      "\n",
      "질문: Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\n",
      "답변: The provided text does not contain information about the synovial membrane, glenoid cavity, humerus, or their interaction with the Biceps brachii tendon. However, the text does mention that one sesamoid bone is sometimes seen in the tendon of the Biceps brachii opposite the radial tuberosity (Page 173). \n",
      "\n",
      "I am answering based on the graph structure alone, as the question's topic is not covered in the provided context.\n",
      "\n",
      "질문: List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\n",
      "답변: This text does **not** provide a comprehensive list of bursae near the shoulder joint, nor does it specify which ones communicate with the synovial cavity. \n",
      "\n",
      "However, it *does* mention **one bursa**:\n",
      "\n",
      "*   A bursa is interposed between the tendon of the Supraspinatus and the coracoacromial ligament.\n",
      "\n",
      "The text **does not** state whether this bursa communicates with the synovial cavity. It also doesn’t mention any other bursae in the shoulder region.\n",
      "\n",
      "질문: What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\n",
      "답변: The plantar calcaneonavicular ligament supports the head of the talus, and is principally concerned in maintaining the arch of the foot (Page 236). If this ligament yields, the head of the talus is pressed downward, medialward, and forward, resulting in a flattened, expanded, and laterally turned foot, known as flat-foot (Page 236).\n",
      "\n",
      "질문: How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\n",
      "답변: I am sorry, but the provided text does not contain information about the navicular bone, the three cuneiform bones, or their connection and permitted movements. Therefore, I cannot answer your question based on the given context.\n",
      "\n",
      "질문: How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\n",
      "답변: The provided text does not contain information about how the nervous system indicates the origin and migration paths of developing muscles, nor does it mention whether or not it influences muscle differentiation. Therefore, I cannot answer your question based on the given context. I am answering based on the graph structure alone.\n",
      "\n",
      "질문: Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\n",
      "답변: The provided context describes the atrioventricular bundle of His, not striped or voluntary muscle. Therefore, I cannot answer your question using the given information. I am answering based on the graph structure alone, and the provided text does not contain information about striped or voluntary muscle structure.\n",
      "\n",
      "질문: What is the triangular ligament and where is it located?\n",
      "답변: The provided context does not contain information about a \"triangular ligament\". It describes the Sphenomandibular Ligament (ligamentum sphenomandibulare) which is attached above to the spina angularis of the sphenoid bone and below to the lingula of the mandibular foramen (Page 189). It also details the Temporomandibular Ligament (ligamentum temporomandibulare) attached to the zygomatic arch and the neck of the mandible (Page 189), and the medial palpebral ligament which is part of the origin of the Orbicularis oculi muscle (Page 257).\n",
      "\n",
      "질문: What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\n",
      "답변: The provided context does not contain information about the structures that perforate the superficial layer of the urogenital diaphragm. Therefore, I cannot answer your question based on the given text. I am answering based on the graph structure alone.\n",
      "\n",
      "질문: Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\n",
      "답변: Here's the answer, based on the provided text:\n",
      "\n",
      "**Origin of Extensor digitorum longus:**\n",
      "\n",
      "The Extensor digitorum longus originates from:\n",
      "\n",
      "*   The lateral condyle and upper half or two-thirds of the lateral surface of the body of the tibia\n",
      "*   The adjoining part of the interosseous membrane\n",
      "*   The deep surface of the fascia\n",
      "*   The intermuscular septum between it and the Tibialis anterior.\n",
      "\n",
      "**Structures between Extensor digitorum longus and Tibialis anterior:**\n",
      "\n",
      "The intermuscular septum is located between the Extensor digitorum longus and the Tibialis anterior. Specifically, the text states the Extensor digitorum longus originates *from the intermuscular septum between it and the Tibialis anterior*.\n",
      "\n",
      "질문: What is the Peronæus tertius, and where is it inserted?\n",
      "답변: The Peronæus tertius flexes the foot and is antagonized by the Peronæi longus and brevis and the Tibialis posterior (Page 325). The text does not mention where the Peronæus tertius is inserted.\n",
      "\n",
      "질문: What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\n",
      "답변: The text describes that arteries are composed of three coats: an internal or endothelial coat (tunica intima of Kölliker), a middle or muscular coat (tunica media), and an external or connective-tissue coat (tunica adventitia) (Page 333). However, the provided context does *not* detail the characteristics of the tunica media or how its composition varies with vessel size. Therefore, I cannot answer your question based on the provided text.\n",
      "\n",
      "질문: Describe the composition and variations of the external coat (tunica adventitia) in arteries.\n",
      "답변: The external coat (tunica adventitia) of arteries is composed of connective tissue (Page 333). When a ligature is tied around an artery, the external coat remains undivided, but the two inner coats are divided in the track of the ligature and can be dissected from the outer coat (Page 333).\n",
      "\n",
      "질문: How do the Vitelline Veins develop into parts of the portal and hepatic veins?\n",
      "답변: Here's a breakdown of how the vitelline veins develop into the portal and hepatic veins, based on the provided text:\n",
      "\n",
      "**Early Development:**\n",
      "\n",
      "*   Initially, the vitelline veins carry blood from the yolk sac to the heart.\n",
      "*   As the liver develops, it *interrupts* the direct connection of the vitelline veins to the sinus venosus (early heart structure).\n",
      "*   The portions of the veins above this interruption are broken up into a plexus of small, capillary-like vessels called *sinusoids* within the developing liver.\n",
      "\n",
      "**Formation of Portal & Hepatic Veins:**\n",
      "\n",
      "*   The branches conveying blood *to* this sinusoidal plexus are called *venæ advehentes*, and these become the *branches of the portal vein*.\n",
      "*   The vessels *draining* the plexus into the sinus venosus are called *venæ revehentes*, and these form the *future hepatic veins*.\n",
      "*   Over time, the left vena revehens loses its direct connection to the sinus venosus and opens into the right vena revehens.\n",
      "*   The upper venous ring above the superior mesenteric vein forms the trunk of the portal vein.\n",
      "\n",
      "\n",
      "\n",
      "In essence, the vitelline veins don't *become* the entire portal and hepatic veins, but rather are *reorganized* by the developing liver into the structural components of these veins. The liver's growth and vascularization *transforms* the original vitelline vein network into the portal system that delivers blood to the liver and the hepatic veins that drain blood *from* the liver.\n",
      "\n",
      "질문: What happens to the Umbilical Veins during embryonic development and after birth?\n",
      "답변: During embryonic development, the umbilical veins, like the vitelline veins, undergo interruption in the developing liver, and the blood returned by them passes through this organ before reaching the heart (Page 340). Ultimately, the right umbilical vein shrivels up and disappears (Page 340). \n",
      "\n",
      "Additionally, the umbilical veins carry blood from the placenta to the fetus, traversing the liver before entering the inferior vena cava, contributing to the large size of the liver early in fetal life (Page 360).\n",
      "\n",
      "질문: What are the three phases of a cardiac cycle and what happens during each?\n",
      "답변: 관련 정보를 지식 그래프에서 찾을 수 없습니다.\n",
      "\n",
      "질문: What are the main peculiarities observed in the fetal heart's vascular system?\n",
      "답변: I am sorry, but the provided context does not contain information about the fetal heart's vascular system. It primarily discusses peculiarities in the pelvic bones (Page 149, 60). Therefore, I cannot answer your question based on the given text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import chromadb\n",
    "import nltk\n",
    "import json\n",
    "import nltk.downloader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from model_loader.config import *\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "generation_loader = generation_loader\n",
    "\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, graphml_path: str, md_path: str, vector_db_path: str = \"./chroma_db\", similarity_threshold: float = 0.5):\n",
    "        self.graphml_path = graphml_path\n",
    "        self.md_path = md_path\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.llm_loader = None\n",
    "        self.kge_model=None\n",
    "        self.triples_factory=None\n",
    "        \n",
    "        self.graph = nx.read_graphml(graphml_path)\n",
    "        self.client = chromadb.PersistentClient(path=vector_db_path)\n",
    "        \n",
    "        self.entity_collection = self.client.get_or_create_collection(name=\"entities\")\n",
    "        self.relation_collection = self.client.get_or_create_collection(name=\"relations\")\n",
    "        self.entity_relation_extraction_prompt_template = \"\"\"\n",
    "            Extract entities and their relations from the following sentence.\n",
    "\n",
    "            **Entities** should be **unique nouns or concepts**, extracted as **noun phrases** whenever possible. Identify **concrete objects or concepts** rather than complex activities or phenomena as entities.\n",
    "\n",
    "            **Relations** should clearly describe the connection between two entities, preferring **reusable predicate verbs** for a knowledge graph. Use **concise verbs** or clear, hyphenated forms like **'part_of' or 'includes'**.\n",
    "\n",
    "            Output the result **only in the following JSON format**, with no other explanations or text:\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"entities\": [\n",
    "                    {{\"name\": \"Entity1\", \"type\": \"Type (e.g., Organ, System, Substance, Function, Disease)\"}},\n",
    "                    {{\"name\": \"Entity2\", \"type\": \"Type\"}}\n",
    "                ],\n",
    "                \"relations\": [\n",
    "                    {{\"head\": \"Entity1\", \"relation\": \"Relation_Type (e.g., part_of, causes)\", \"tail\": \"Entity2\"}},\n",
    "                    {{\"head\": \"Entity3\", \"relation\": \"generates\", \"tail\": \"Entity4\"}}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            sentence : \"{text_to_analyze}\"\n",
    "            JSON result :\n",
    "        \"\"\"\n",
    "\n",
    "        self._initialize_vector_db()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        return text.upper().replace(' ', '_')\n",
    "\n",
    "    def _initialize_vector_db(self):\n",
    "        if self.entity_collection.count() == 0:\n",
    "            nodes_to_add = []\n",
    "            unique_nodes = set()\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                processed_node = self._preprocess_text(node)\n",
    "                if processed_node not in unique_nodes :\n",
    "                    metadata = {k: str(v) for k, v in data.items()}\n",
    "                    metadata['original_name'] = node\n",
    "                    nodes_to_add.append({'id': processed_node, 'document': processed_node, 'metadata': metadata})\n",
    "                    unique_nodes.add(processed_node)\n",
    "            \n",
    "            if nodes_to_add:\n",
    "                ids = [item['id'] for item in nodes_to_add]\n",
    "                documents = [item['document'] for item in nodes_to_add]\n",
    "                metadatas = [item['metadata'] for item in nodes_to_add]\n",
    "                self.entity_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "        if self.relation_collection.count() == 0:\n",
    "            edges_to_add = []\n",
    "            unique_processed_relations = set()\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                relation_type = data.get('type')\n",
    "                if relation_type:\n",
    "                    processed_relation = self._preprocess_text(relation_type)\n",
    "                    if processed_relation not in unique_processed_relations :\n",
    "                        metadata = {'original_name': relation_type}\n",
    "                        edges_to_add.append({'id': processed_relation, 'document': processed_relation, 'metadata': metadata})\n",
    "                        unique_processed_relations.add(processed_relation)\n",
    "\n",
    "            if edges_to_add:\n",
    "                ids = [item['id'] for item in edges_to_add]\n",
    "                documents = [item['document'] for item in edges_to_add]\n",
    "                metadatas = [item['metadata'] for item in edges_to_add]\n",
    "                self.relation_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    \n",
    "    def _extract_entities_relations(self, question) :\n",
    "        prompt = self.entity_relation_extraction_prompt_template.format(text_to_analyze=question)\n",
    "        raw_llm_output = self._call_llm_generate(prompt)\n",
    "\n",
    "        try :\n",
    "            json_start = raw_llm_output.find(\"{\")\n",
    "            json_end = raw_llm_output.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1 and json_end > json_start :\n",
    "                json_str = raw_llm_output[json_start:json_end]\n",
    "                extracted_data = json.loads(json_str)\n",
    "                return extracted_data.get(\"entities\", []), extracted_data.get(\"relations\", [])\n",
    "            else :\n",
    "                print(f\"LLM 답변에서 유효한 JSON 형태를 찾을 수 없음 : {raw_llm_output}\")\n",
    "                return [], []\n",
    "            \n",
    "        except json.JSONDecodeError as e :\n",
    "            print(f\"개체 추출 과정에서 JSON 디코딩 오류 발생: {e}\")\n",
    "            print(f\"오류 발생 원문: {raw_llm_output}\")\n",
    "            return [], []\n",
    "\n",
    "    def _search_knowledge_graph(self, entities: List[str], relations: List[str]) -> List[Dict[str, Any]]:\n",
    "        processed_entities = [self._preprocess_text(e[\"name\"]) for e in entities if \"name\" in e]\n",
    "        processed_relations = [self._preprocess_text(r[\"relation\"]) for r in relations if \"relation\" in r]\n",
    "\n",
    "        found_results = []\n",
    "\n",
    "        similar_entities = []\n",
    "        if processed_entities:\n",
    "            entity_results = self.entity_collection.query(\n",
    "                query_texts=processed_entities,\n",
    "                n_results=2, \n",
    "                include=[\"metadatas\", \"distances\"]\n",
    "            )\n",
    "            if entity_results['distances']:\n",
    "                for i, dists in enumerate(entity_results['distances']):\n",
    "                    for j, dist in enumerate(dists):\n",
    "                        if dist <= self.similarity_threshold:\n",
    "                            meta = entity_results['metadatas'][i][j]\n",
    "                            similar_entities.append(meta['original_name'])\n",
    "        \n",
    "        similar_relations = []\n",
    "        if processed_relations:\n",
    "            relation_results = self.relation_collection.query(\n",
    "                query_texts=processed_relations,\n",
    "                n_results=1,\n",
    "                include=[\"metadatas\", \"distances\"]\n",
    "            )\n",
    "            if relation_results['distances'] and relation_results['distances'][0]:\n",
    "                if relation_results['distances'][0][0] <= self.similarity_threshold:\n",
    "                    meta = relation_results['metadatas'][0][0]\n",
    "                    similar_relations.append(meta['original_name'])\n",
    "\n",
    "        similar_entities = list(set(similar_entities))\n",
    "\n",
    "        inferred_relations = []\n",
    "        if self.kge_model and self.triples_factory and entities :\n",
    "            for entity_data in entities :\n",
    "                entity_name = entity_data[\"name\"]\n",
    "                head_emb = self._get_kge_embedding(entity_name)\n",
    "                if head_emb is not None :\n",
    "                    for rel_name in self.triples_factory.relation_to_id.keys() :\n",
    "                        rel_emb = self._get_kge_relation_embedding(rel_name)\n",
    "                        if rel_emb is not None :\n",
    "                            if rel_name not in similar_relations :\n",
    "                                inferred_relations.append(rel_name)\n",
    "\n",
    "        all_relevant_relations = list(set(similar_relations + inferred_relations))\n",
    "\n",
    "        if similar_entities and all_relevant_relations:\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                if (u in similar_entities or v in similar_entities) and data.get('type') in all_relevant_relations:\n",
    "                    result = data.copy()\n",
    "                    result['source_node'] = u\n",
    "                    result['target_node'] = v\n",
    "                    if 'source_page' in result:\n",
    "                        found_results.append(result)\n",
    "        \n",
    "        if not found_results and similar_entities:\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                if u in similar_entities or v in similar_entities:\n",
    "                    result = data.copy()\n",
    "                    result['source_node'] = u\n",
    "                    result['target_node'] = v\n",
    "                    if 'source_page' in result:\n",
    "                        found_results.append(result)\n",
    "        \n",
    "        if not found_results:\n",
    "            return []\n",
    "            \n",
    "        unique_results = []\n",
    "        seen = set()\n",
    "        for res in found_results:\n",
    "            identifier = (res.get('source_node'), res.get('target_node'), res.get('type'))\n",
    "            if identifier not in seen:\n",
    "                unique_results.append(res)\n",
    "                seen.add(identifier)\n",
    "                \n",
    "        return unique_results\n",
    "    \n",
    "    def _retrieve_context_from_md(self, search_results: List[Dict[str, Any]], n_sentences: int = 2) -> Tuple[str, List[str]]:\n",
    "        context = \"\"\n",
    "        pages = sorted(list(set(res.get('source_page') for res in search_results if res.get('source_page'))))\n",
    "        \n",
    "        if not pages:\n",
    "            return \"\", []\n",
    "\n",
    "        all_md_files = [f for f in os.listdir(self.md_path) if f.endswith('.md')]\n",
    "        \n",
    "        page_texts = {}\n",
    "        for page_num_str in pages:\n",
    "            page_num = int(page_num_str)\n",
    "            for md_file in all_md_files:\n",
    "                with open(os.path.join(self.md_path, md_file), 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                match = re.search(rf\"####\\s+Page\\s+{page_num}\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "                if match:\n",
    "                    page_texts[page_num_str] = match.group(1).strip()\n",
    "                    break\n",
    "        \n",
    "        context_parts = []\n",
    "        for result in search_results:\n",
    "            page_num = result.get('source_page')\n",
    "            page_content = page_texts.get(page_num)\n",
    "            \n",
    "            if not page_content:\n",
    "                continue\n",
    "                \n",
    "            source_node = result.get('source_node')\n",
    "            target_node = result.get('target_node')\n",
    "            \n",
    "            sentences = sent_tokenize(page_content)\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if source_node and source_node in sent and target_node and target_node in sent:\n",
    "                    start = max(0, i - n_sentences)\n",
    "                    end = min(len(sentences), i + n_sentences + 1)\n",
    "                    context_snippet = \" \".join(sentences[start:end])\n",
    "                    context_parts.append(f\"... {context_snippet} ... (출처: Page {page_num})\")\n",
    "                    break\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        return context, pages\n",
    "\n",
    "    # def _build_llm_prompt(self, question: str, context: str, pages: List[str]) -> str:\n",
    "    def _build_llm_prompt(self, question: str, context: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant who answers questions based on the provided context.\n",
    "        You MUST cite the source page number for every piece of information you use.\n",
    "\n",
    "        **Instructions:**\n",
    "        1. Answer the user's question clearly and concisely using ONLY the provided context and knowledge graph information.\n",
    "        2. For every statement, you MUST provide the source page number in parentheses, like this: (Page XX).\n",
    "        3. If a single piece of information is supported by multiple pages, cite all of them: (Page X, Y, Z).\n",
    "        4. If no context is available, state that you are answering based on the graph structure alone.\n",
    "\n",
    "        **Example of a GOOD answer:**\n",
    "        The ductus arteriosus degenerates into the ligamentum arteriosum after birth(page 360). This is a normal physiological change that happens post-delivery(page 361).\n",
    "\n",
    "        **Example of a BAD answer:** -> (This is a bad answer because it lacks the mandatory citation)\n",
    "        The ductus arteriosus becomes the ligamentum arteriosum.\n",
    "\n",
    "        ---\n",
    "        **Context:**\n",
    "        {context}\n",
    "        ---\n",
    "        **Question:**\n",
    "        {question}\n",
    "        ---\n",
    "        **Answer:**\n",
    "        \"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def _call_llm_generate(self, prompt: str) -> str:\n",
    "        if self.llm_loader:\n",
    "            if hasattr(self.llm_loader, \"tokenizer\") and hasattr(self.llm_loader, \"model\"):\n",
    "                tokenizer = self.llm_loader.tokenizer\n",
    "                model = self.llm_loader.model\n",
    "\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    num_beams=3,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_ids = output[0][input_ids.shape[-1]:]\n",
    "                raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                return raw_answer\n",
    "            else:\n",
    "                raw_answer = self.llm_loader.generate(prompt)\n",
    "                return raw_answer\n",
    "        else:\n",
    "            print(\"generation_loader가 로드되지 않음\")\n",
    "            return \"LLM 로더가 설정되지 않았습니다.\"\n",
    "\n",
    "    def generate_response(self, question: str) -> Tuple[str, str]:\n",
    "        entities, relations = self._extract_entities_relations(question)\n",
    "        if not entities and not relations:\n",
    "            return \"질문에서 유효한 엔티티나 릴레이션을 추출할 수 없습니다.\", \"\"\n",
    "        \n",
    "        search_results = self._search_knowledge_graph(entities, relations)\n",
    "        if not search_results:\n",
    "            return \"관련 정보를 지식 그래프에서 찾을 수 없습니다.\", \"\"\n",
    "            \n",
    "        context, pages = self._retrieve_context_from_md(search_results)\n",
    "        if not context:\n",
    "            no_context_message = f\"관련 정보를 {', '.join(pages) if pages else 'N/A'}에서 발견할 수 없음\"\n",
    "            return no_context_message, \"\"\n",
    "        \n",
    "        prompt = self._build_llm_prompt(question, context)\n",
    "        \n",
    "        answer = self._call_llm_generate(prompt)\n",
    "        return answer, context\n",
    "\n",
    "def save_results_to_file(question: str, answer: str, context: str, output_dir: str, file_index: int):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%H%M%S_%f\")\n",
    "    file_name = f\"result_{file_index}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"[질문]\\n{question}\\n\\n\")\n",
    "        f.write(f\"[근거]\\n{context}\\n\\n\")\n",
    "        f.write(f\"[답변]\\n{answer}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qa_system = QASystem(\n",
    "        graphml_path=\"./data/knowledge_graph/knowledge_graph.graphml\",\n",
    "        md_path=\"./data/split_file/anatomy/\"\n",
    "    )\n",
    "    \n",
    "    qa_system.llm_loader = generation_loader\n",
    "    \n",
    "    questions = [\n",
    "        ############## 1_Embryology.md\n",
    "        \"What are the two essential components of a higher organism cell as defined in the text?\", # 7페이지\n",
    "        \"Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\", # 7페이지\n",
    "        \"What is the primary role of the yolk-sac in the embryo's early development?\", # 20페이지\n",
    "        \"How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\", # 19페이지\n",
    "        \"What significant developments occur in a human embryo during the Second Week?\", # 33페이지\n",
    "        \"What are the key characteristics of the human embryo by the end of the Third Week?\", # 33페이지\n",
    "        \n",
    "        ############## 2_Osteology.md\n",
    "        \"What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\", # 38페이지\n",
    "        \"How is each vertebral body formed from primitive segments during development?\", # 38페이지\n",
    "        \"What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\", # 88페이지\n",
    "        \"Describe the sphenoidal rostrum and its articulation.\",# 88\n",
    "        \"What is the tibia, and where is it located in the human leg?\", # 158\n",
    "        \"Describe the superior articular surface of the tibia's upper extremity.\", # 158\n",
    "\n",
    "        ############## 3_Syndesmology.md\n",
    "        \"What are joints or articulations, and how are immovable joints characterized?\", # 174\n",
    "        \"How does the articular lamella differ from ordinary bone tissue?\", # 174\n",
    "        \"Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\", # 207\n",
    "        \"List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\", # 207\n",
    "        \"What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\", # 236\n",
    "        \"How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\", # 236\n",
    "\n",
    "        ############## 4_Myology.md\n",
    "        \"How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\", # 250\n",
    "        \"Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\", # 250\n",
    "        \"What is the triangular ligament and where is it located?\", # 290\n",
    "        \"What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\", # 290\n",
    "        \"Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\", # 322\n",
    "        \"What is the Peronæus tertius, and where is it inserted?\", # 322\n",
    "\n",
    "        ############## 5_Angiology.md\n",
    "        \"What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\", # 334\n",
    "        \"Describe the composition and variations of the external coat (tunica adventitia) in arteries.\", # 334\n",
    "        \"How do the Vitelline Veins develop into parts of the portal and hepatic veins?\", # 345\n",
    "        \"What happens to the Umbilical Veins during embryonic development and after birth?\", # 345\n",
    "        \"What are the three phases of a cardiac cycle and what happens during each?\", # 358\n",
    "        \"What are the main peculiarities observed in the fetal heart's vascular system?\" # 359\n",
    "    ]   \n",
    "\n",
    "    today = datetime.now()\n",
    "    folder_name = f\"{today.month}월{today.day}일\"\n",
    "    output_dir = os.path.join(\"./result\", \"knowledge_graph\", folder_name)\n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"질문: {q}\")\n",
    "        response, context = qa_system.generate_response(q)\n",
    "        print(f\"답변: {response}\\n\")\n",
    "        save_results_to_file(q, response, context, output_dir, i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ac00",
   "metadata": {},
   "source": [
    "# 검색(KGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c5a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "WARNING:pykeen.utils:using automatically assigned random_state=305158478\n",
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [423, 827]\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 261816877.\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 트리플 : 4132\n",
      "첫 5개 : [('germinal cells', 'LOCATED_IN', 'ovaries'), ('granules', 'STAIN_WITH', 'eosin'), ('nucleus', 'IS_SURROUNDED_BY', 'protoplasm'), ('nucleus', 'HAS_COUNT', 'twenty-four'), ('hyaloplasm', 'IS_PART_OF', 'cytoplasm')]\n",
      "Numpy 배열의 형태 : (4132, 3)\n",
      "TriplesFactory 생성 완료. 엔티티 수 : 3756, 관계 수 : 2148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b5b7707ccc4375aedc425b03e5c82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174a3c867b0342aea1211c13212b9f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97130107893457c9fa9e8ec62695085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba3f1ad78a34e0dafaa5841a759cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a2cdb74bd64743bcd02bf48ce1b01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab8fdc478e943cb99eb057b90ea7370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e01788f57444f0fbffad1870a6f11ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22ede9c513643249194589f4c081eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451ad2433d8b401da1df051a7347380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe1e25f07242bb83f7afcaaf85be72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c14834d8f57451784b23d5f97a34d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0daf67b59f42e3b1e45a7d402f693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a81886cd2e419fbc3b6e0b2d03c37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559e7e39ad4d41ee91be08a0358d1167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b862776197d3459eb1cf7ea101d32c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332186c1ed114218bd064b61613aca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae356f8573e648d1ba0a588916e51df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286a0a8831f84e03b5bbedf2e83c7597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7826eecbd0684392ab70145207b5312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f780aeb3a204f22a3a72bb7a84e3a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c2c3c794c4c6fb97d8da9d7792048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b7254b2c7e433ea0a430e47b692cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375e445789ff402899be1beb8c424db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f1fdd7d5784685b79acd058b7e10a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14472befc034634b67f20b63483c697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d392cd2fd949d6a9c18037ed14d45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8050f1077242bd8c343a805ce994c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf08810fb7784fcb9638d4326ec675d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa55f7afdf844956813dc5cf6620316b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6a4a6ba1c34d478197784a94eac1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a271e75d2d1c419aba8553a006a65450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c001dee64ef46e993bcc8c536bae117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42240124700471db030e18454e2bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7743a28a734531a1cefe2527bfb456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f62eca3ed047eeacd7150fc3abeda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5630f005e54be59f8916d4cd10ef9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42863fe7386429ebf73c3fe91c9d9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426498e69d944607a057c48aa2ccacbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73242a35a3344fba37b31c55b99f7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae77c4dbc2ab40d4aa67d9fd20baa544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fbab7136f445eb8f7cfb1df7049ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4692bcffd50a439388b7127e0624602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007b994b8c8f416a9f6650859ff60f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67b26f6ac23462e939c23fbd83b14c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d26af8c06614c4db2fa5cb64e237b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9089d9aa403c4680b60f135746a60db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e04dbecffe4c19bfe8670ace277dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833b75cf9b2847c19b94c5bb110cd941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2e0c042e4744508131d2291d6dd6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c5392561ad43bbb74c559284042986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88b623a9d334570b41ac5578ad8354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c7cff08e8c45b8beee8c7341c59b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a385cc9366e4408081eeda758cc60cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7875ab89bad647e7b9cfd451e47bf6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c1090edd964526aa55a82f26eb1a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1bb602b8c844cc9df4f510482d39d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f7f1ee61a484cba8189cc66890912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db347ae99a4661ba27d6867c1a9f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2f650807634074a07f67a81d5251f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f380325a6c49d78022c510a17afcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582905447bee41ddbff13bfbb00c7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59318fcc03ea4ccf8a62795835bf359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d5d054d02d43bc9649585e453fd235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874f7932be1945d990fb3f4558c853f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09e425a135e4b1d8e0b6597dcd8210b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7951d84521c4ee3b3dd0e10dec5a092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa41092cfd24ad2a03d4841b3e2f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266c316db3ca4da68f74d5efce619de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf76e3cc71e45fd9c9110ed175dfc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c2d202260c41b8b7553fe1ee1d8a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df262ae4700d4001a0e0db230439b1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe32bc2771e14d97baabcd3a540a9398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103bad7bf9d246bd84be6f37eef91ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6ad0cfbf364e6395ba6ded0b460c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2210948294754c7fbb3f013053d59f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0586b5042f90449d8b0a2667c9bfbef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90173bd3a5946e0a27f57f6892c51aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becf48546d714552a48ed6f4a47e95f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46897d043bba4c0ca9fa3b0eded95a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1b064c808a4ee4bc5b4783a970fdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab7332328584bba9c0856817d03eea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6b9e0fcaf54de4bcaf73162cecb970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dca6b954f84b0881c3cc96f75e61df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44a91ba84784d7388bd4e43577ebc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b439d78a68244519cfe34f4efcb2645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8089feb2fcf94f42a12b59c4536acb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5653c6e17d493e9c524ca6a373a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2adcf33f55144628c49ddf9228f6a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7041a89409cf4b8993eef4973c2c4751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14bf2f7ec0144c4bedae7dbf251274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561c539ff9604d4e8fea1edb476d65ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283c958cbb7d4be9b0863115917d709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a7ce0c7cd6465f83e913eb1b8cfb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139243032e484b169283dc97a2e3afc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1609f46ca3d40ce9e6936b19b868372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6ee80f4497455a9d7c2acfe02768ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a28f9d3375444ca04a147d9e9e671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f61b536d7e495f8b002c74143f1135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1c3dada88144d3bbc8979f704129e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8417cb7477c41748dbff16deb732fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb76fd462df42459135ebcf00180996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cuda:0:   0%|          | 0.00/26.0 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169117dc95f242998aba186684422673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/827 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.05s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGE 모델 학습 완료\n",
      "질문: What are the two essential components of a higher organism cell as defined in the text?\n",
      "답변: The two essentials of a cell in higher organisms are a soft jelly-like material called cytoplasm and a small spherical body called a nucleus (Page 6).\n",
      "\n",
      "질문: Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\n",
      "답변: 관련 정보를 지식 그래프에서 찾을 수 없습니다.\n",
      "\n",
      "질문: What is the primary role of the yolk-sac in the embryo's early development?\n",
      "답변: According to the text, the primary role of the yolk-sac in the embryo's early development is to provide **nourishment**. \n",
      "\n",
      "Specifically, the text states that the yolk-sac is filled with **vitelline fluid** which \"possibly may be utilized for the nourishment of the embryo during the earlier stages of its existence.\" It also describes **vitelline circulation** where blood conveys nutrients from the yolk-sac to the embryo's heart. \n",
      "\n",
      "While it initially forms part of the digestive tube and later reduces in size, its core function is providing nutrients in the early stages of development.\n",
      "\n",
      "질문: How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\n",
      "답변: Here's how the embryo separates from the yolk-sac, based on the provided text:\n",
      "\n",
      "The embryo separates from the yolk-sac due to the growth of cephalic (head) and caudal (tail) folds. The circumference of the embryonic disk grows slower than the embryo itself, creating a constriction between the embryo and the majority of the yolk-sac.\n",
      "\n",
      "The enclosed part of the yolk-sac becomes incorporated into the embryo and forms the **primitive digestive tube**. Specifically:\n",
      "\n",
      "*   The forward growth of the head encloses a diverticulum of the yolk-sac, forming the **fore-gut**.\n",
      "*   The tail fold encloses another diverticulum, forming the **hind-gut**. \n",
      "*   The space between the fore-gut and hind-gut initially remains open to the yolk-sac, but this opening gradually reduces the yolk-sac to a small pear-shaped sac (umbilical vesicle) connected by the **vitelline duct**.\n",
      "\n",
      "질문: What significant developments occur in a human embryo during the Second Week?\n",
      "답변: During the second week, the mesoderm outside the embryonic disk splits into two layers, enclosing an extra-embryonic celom, and there is no trace of an intra-embryonic coelom at this stage(Page 31). Also, the limbs begin to appear as small elevations or buds at the side of the trunk during the third week, which is closely following the second week(Page 30).\n",
      "\n",
      "질문: What are the key characteristics of the human embryo by the end of the Third Week?\n",
      "답변: By the end of the third week, the limbs begin to appear as small elevations or buds at the side of the trunk (Page 30). Additionally, by this stage, the mesoderm outside the embryonic disk is split into two layers enclosing an extra-embryonic celom, with no trace of an intra-embryonic celom initially (Page 31). Four cavities are formed within the embryo—one on either side within the mesoderm of the pericardial area, and one in either lateral mass of the general mesoderm—though these are initially independent of each other and the extra-embryonic celom (Page 31).\n",
      "\n",
      "질문: What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\n",
      "답변: The cells of a primitive segment differentiate into three groups: the cervical (8), thoracic (12), lumbar (5), sacral (5), and coccygeal (5-8) segments (Page 18). These segments lie under the ectoderm, lateral to the neural tube and notochord, and are connected to the lateral mesoderm by the intermediate cell-mass (Page 18).\n",
      "\n",
      "질문: How is each vertebral body formed from primitive segments during development?\n",
      "답변: Each vertebral body is formed from the posterior portion of one primitive segment and the anterior part of the segment immediately behind it (Page 38). Cells from the posterior mass grow into the intervals between the myotomes of corresponding and succeeding segments, extending both dorsally and ventrally (Page 38). The dorsal extensions surround the neural tube and represent the future vertebral arch, while the ventral extensions extend into the body-wall as the costal processes (Page 38).\n",
      "\n",
      "질문: What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\n",
      "답변: The sphenoidal air sinuses are two large cavities located within the body of the sphenoid bone (Page 87). These cavities are separated from each other by a septum (Page 87).\n",
      "\n",
      "질문: Describe the sphenoidal rostrum and its articulation.\n",
      "답변: I am sorry, but the provided context does not contain information about the sphenoidal rostrum or its articulation (Page 178, 179, 180, 191, 203, 240). Therefore, I cannot answer your question based on the given text.\n",
      "\n",
      "질문: What is the tibia, and where is it located in the human leg?\n",
      "답변: The tibia, also known as the shin bone, is the longest bone in the skeleton, excluding the femur (Page 158). It is located at the medial side of the leg (Page 158). It is prismoid in form, expanded above at the knee-joint, contracted in the lower third, and again enlarged below (Page 158).\n",
      "\n",
      "질문: Describe the superior articular surface of the tibia's upper extremity.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk.downloader\n",
    "from datetime import datetime\n",
    "from pykeen.models import ComplEx\n",
    "from model_loader.config import *\n",
    "from pykeen.hpo import hpo_pipeline\n",
    "from pykeen.pipeline import pipeline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pykeen.optimizers import AdamW as PyKeenAdamW\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from pykeen.triples import TriplesFactory\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "generation_loader = generation_loader\n",
    "\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, graphml_path: str, md_path: str, vector_db_path: str = \"./chroma_db\", similarity_threshold: float = 0.5):\n",
    "        self.graphml_path = graphml_path\n",
    "        self.md_path = md_path\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.llm_loader = None\n",
    "        \n",
    "        self.graph = nx.read_graphml(graphml_path)\n",
    "        self.client = chromadb.PersistentClient(path=vector_db_path)\n",
    "        \n",
    "        self.entity_collection = self.client.get_or_create_collection(name=\"entities\")\n",
    "        self.relation_collection = self.client.get_or_create_collection(name=\"relations\")\n",
    "        self.entity_relation_extraction_prompt_template = \"\"\"\n",
    "            Extract entities and their relations from the following sentence.\n",
    "\n",
    "            **Entities** should be **unique nouns or concepts**, extracted as **noun phrases** whenever possible. Identify **concrete objects or concepts** rather than complex activities or phenomena as entities.\n",
    "\n",
    "            **Relations** should clearly describe the connection between two entities, preferring **reusable predicate verbs** for a knowledge graph. Use **concise verbs** or clear, hyphenated forms like **'part_of' or 'includes'**.\n",
    "\n",
    "            Output the result **only in the following JSON format**, with no other explanations or text:\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"entities\": [\n",
    "                    {{\"name\": \"Entity1\", \"type\": \"Type (e.g., Organ, System, Substance, Function, Disease)\"}},\n",
    "                    {{\"name\": \"Entity2\", \"type\": \"Type\"}}\n",
    "                ],\n",
    "                \"relations\": [\n",
    "                    {{\"head\": \"Entity1\", \"relation\": \"Relation_Type (e.g., part_of, causes)\", \"tail\": \"Entity2\"}},\n",
    "                    {{\"head\": \"Entity3\", \"relation\": \"generates\", \"tail\": \"Entity4\"}}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            sentence : \"{text_to_analyze}\"\n",
    "            JSON result :\n",
    "        \"\"\"\n",
    "\n",
    "        self._initialize_vector_db()\n",
    "        self.kge_model, self.triples_factory = self._train_kge_model()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        return text.upper().replace(' ', '_')\n",
    "\n",
    "    def _initialize_vector_db(self):\n",
    "        if self.entity_collection.count() == 0:\n",
    "            nodes_to_add = []\n",
    "            unique_nodes = set()\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                processed_node = self._preprocess_text(node)\n",
    "                if processed_node not in unique_nodes :\n",
    "                    metadata = {k: str(v) for k, v in data.items()}\n",
    "                    metadata['original_name'] = node\n",
    "                    nodes_to_add.append({'id': processed_node, 'document': processed_node, 'metadata': metadata})\n",
    "                    unique_nodes.add(processed_node)\n",
    "            \n",
    "            if nodes_to_add:\n",
    "                ids = [item['id'] for item in nodes_to_add]\n",
    "                documents = [item['document'] for item in nodes_to_add]\n",
    "                metadatas = [item['metadata'] for item in nodes_to_add]\n",
    "                self.entity_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "        if self.relation_collection.count() == 0:\n",
    "            edges_to_add = []\n",
    "            unique_processed_relations = set()\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                relation_type = data.get('type')\n",
    "                if relation_type:\n",
    "                    processed_relation = self._preprocess_text(relation_type)\n",
    "                    if processed_relation not in unique_processed_relations :\n",
    "                        metadata = {'original_name': relation_type}\n",
    "                        edges_to_add.append({'id': processed_relation, 'document': processed_relation, 'metadata': metadata})\n",
    "                        unique_processed_relations.add(processed_relation)\n",
    "\n",
    "            if edges_to_add:\n",
    "                ids = [item['id'] for item in edges_to_add]\n",
    "                documents = [item['document'] for item in edges_to_add]\n",
    "                metadatas = [item['metadata'] for item in edges_to_add]\n",
    "                self.relation_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    \n",
    "    def _train_kge_model(self) :\n",
    "        triples = []\n",
    "        for u, v, data in self.graph.edges(data=True) :\n",
    "            relation_type = data.get(\"type\")\n",
    "            if relation_type and isinstance(relation_type, str):\n",
    "                triples.append((str(u), str(relation_type), str(v)))\n",
    "\n",
    "        if not triples :\n",
    "            print(\"KGE 모델 학습을 위한 트리플이 없음\")\n",
    "            return None, None\n",
    "        print(f\"생성된 트리플 : {len(triples)}\")\n",
    "        if len(triples) > 0 :\n",
    "            print(f\"첫 5개 : {triples[:5]}\")\n",
    "\n",
    "        triples_array = np.array(triples)\n",
    "        print(f\"Numpy 배열의 형태 : {triples_array.shape}\")\n",
    "\n",
    "        training_triples_factory = TriplesFactory.from_labeled_triples(\n",
    "            triples=triples_array,\n",
    "            create_inverse_triples=True\n",
    "        )\n",
    "        print(f\"TriplesFactory 생성 완료. 엔티티 수 : {training_triples_factory.num_entities}, 관계 수 : {training_triples_factory.num_relations}\")\n",
    "\n",
    "        training_set, testing_set = training_triples_factory.split()\n",
    "\n",
    "        result = pipeline(\n",
    "            training=training_set,\n",
    "            testing=testing_set,\n",
    "            model=ComplEx,\n",
    "            optimizer=PyKeenAdamW,\n",
    "            training_kwargs=dict(num_epochs=100, batch_size=256),\n",
    "            optimizer_kwargs=dict(lr=0.01),\n",
    "        )\n",
    "\n",
    "        print(\"KGE 모델 학습 완료\")\n",
    "        return result.model, training_triples_factory\n",
    "    \n",
    "    def _get_kge_embedding(self, entity_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if entity_name in self.triples_factory.entity_to_id :\n",
    "            entity_id = self.triples_factory.entity_to_id[entity_name]\n",
    "            return self.kge_model.entity_representations[0](torch.tensor([entity_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "    \n",
    "    def _get_kge_relation_embedding(self, relation_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if relation_name in self.triples_factory.relation_to_id :\n",
    "            relation_id = self.triples_factory.relation_to_id[relation_name]\n",
    "            return self.kge_model.relation_representations[0](torch.tensor([relation_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "\n",
    "    def _extract_entities_relations(self, question) :\n",
    "        prompt = self.entity_relation_extraction_prompt_template.format(text_to_analyze=question)\n",
    "        raw_llm_output = self._call_llm_generate(prompt)\n",
    "\n",
    "        try :\n",
    "            json_start = raw_llm_output.find(\"{\")\n",
    "            json_end = raw_llm_output.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1 and json_end > json_start :\n",
    "                json_str = raw_llm_output[json_start:json_end]\n",
    "                extracted_data = json.loads(json_str)\n",
    "                return extracted_data.get(\"entities\", []), extracted_data.get(\"relations\", [])\n",
    "            else :\n",
    "                print(f\"LLM 답변에서 유효한 JSON 형태를 찾을 수 없음 : {raw_llm_output}\")\n",
    "                return [], []\n",
    "            \n",
    "        except json.JSONDecodeError as e :\n",
    "            print(f\"개체 추출 과정에서 JSON 디코딩 오류 발생: {e}\")\n",
    "            print(f\"오류 발생 원문: {raw_llm_output}\")\n",
    "            return [], []\n",
    "\n",
    "    def _search_knowledge_graph(self, entities: List[str], relations: List[str]) -> List[Dict[str, Any]]:\n",
    "        processed_entities = [self._preprocess_text(e[\"name\"]) for e in entities if \"name\" in e]\n",
    "        processed_relations = [self._preprocess_text(r[\"relation\"]) for r in relations if \"relation\" in r]\n",
    "\n",
    "        found_results = []\n",
    "\n",
    "        similar_entities = []\n",
    "        if processed_entities:\n",
    "            entity_results = self.entity_collection.query(\n",
    "                query_texts=processed_entities,\n",
    "                n_results=2, \n",
    "                include=[\"metadatas\", \"distances\"]\n",
    "            )\n",
    "            if entity_results['distances']:\n",
    "                for i, dists in enumerate(entity_results['distances']):\n",
    "                    for j, dist in enumerate(dists):\n",
    "                        if dist <= self.similarity_threshold:\n",
    "                            meta = entity_results['metadatas'][i][j]\n",
    "                            similar_entities.append(meta['original_name'])\n",
    "        \n",
    "        similar_relations = []\n",
    "        if processed_relations:\n",
    "            relation_results = self.relation_collection.query(\n",
    "                query_texts=processed_relations,\n",
    "                n_results=1,\n",
    "                include=[\"metadatas\", \"distances\"]\n",
    "            )\n",
    "            if relation_results['distances'] and relation_results['distances'][0]:\n",
    "                if relation_results['distances'][0][0] <= self.similarity_threshold:\n",
    "                    meta = relation_results['metadatas'][0][0]\n",
    "                    similar_relations.append(meta['original_name'])\n",
    "\n",
    "        similar_entities = list(set(similar_entities))\n",
    "\n",
    "        inferred_relations = []\n",
    "        if self.kge_model and entities :\n",
    "            for entity_data in entities :\n",
    "                entity_name = entity_data[\"name\"]\n",
    "                head_emb = self._get_kge_embedding(entity_name)\n",
    "                if head_emb is not None :\n",
    "                    for rel_name in self.triples_factory.relation_to_id.keys() :\n",
    "                        rel_emb = self._get_kge_relation_embedding(rel_name)\n",
    "                        if rel_emb is not None :\n",
    "                            if rel_name not in similar_relations :\n",
    "                                inferred_relations.append(rel_name)\n",
    "\n",
    "        all_relevant_relations = list(set(similar_relations + inferred_relations))\n",
    "\n",
    "        if similar_entities and all_relevant_relations:\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                if (u in similar_entities or v in similar_entities) and data.get('type') in all_relevant_relations:\n",
    "                    result = data.copy()\n",
    "                    result['source_node'] = u\n",
    "                    result['target_node'] = v\n",
    "                    if 'source_page' in result:\n",
    "                        found_results.append(result)\n",
    "        \n",
    "        if not found_results and similar_entities:\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                if u in similar_entities or v in similar_entities:\n",
    "                    result = data.copy()\n",
    "                    result['source_node'] = u\n",
    "                    result['target_node'] = v\n",
    "                    if 'source_page' in result:\n",
    "                        found_results.append(result)\n",
    "        \n",
    "        if not found_results:\n",
    "            return []\n",
    "            \n",
    "        unique_results = []\n",
    "        seen = set()\n",
    "        for res in found_results:\n",
    "            identifier = (res.get('source_node'), res.get('target_node'), res.get('type'))\n",
    "            if identifier not in seen:\n",
    "                unique_results.append(res)\n",
    "                seen.add(identifier)\n",
    "                \n",
    "        return unique_results\n",
    "    \n",
    "    def _retrieve_context_from_md(self, search_results: List[Dict[str, Any]], n_sentences: int = 2) -> Tuple[str, List[str]]:\n",
    "        context = \"\"\n",
    "        pages = sorted(list(set(res.get('source_page') for res in search_results if res.get('source_page'))))\n",
    "        \n",
    "        if not pages:\n",
    "            return \"\", []\n",
    "\n",
    "        all_md_files = [f for f in os.listdir(self.md_path) if f.endswith('.md')]\n",
    "        \n",
    "        page_texts = {}\n",
    "        for page_num_str in pages:\n",
    "            page_num = int(page_num_str)\n",
    "            for md_file in all_md_files:\n",
    "                with open(os.path.join(self.md_path, md_file), 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                match = re.search(rf\"####\\s+Page\\s+{page_num}\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "                if match:\n",
    "                    page_texts[page_num_str] = match.group(1).strip()\n",
    "                    break\n",
    "        \n",
    "        context_parts = []\n",
    "        for result in search_results:\n",
    "            page_num = result.get('source_page')\n",
    "            page_content = page_texts.get(page_num)\n",
    "            \n",
    "            if not page_content:\n",
    "                continue\n",
    "                \n",
    "            source_node = result.get('source_node')\n",
    "            target_node = result.get('target_node')\n",
    "            \n",
    "            sentences = sent_tokenize(page_content)\n",
    "            for i, sent in enumerate(sentences):\n",
    "                if source_node and source_node in sent and target_node and target_node in sent:\n",
    "                    start = max(0, i - n_sentences)\n",
    "                    end = min(len(sentences), i + n_sentences + 1)\n",
    "                    context_snippet = \" \".join(sentences[start:end])\n",
    "                    context_parts.append(f\"... {context_snippet} ... (출처: Page {page_num})\")\n",
    "                    break\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        return context, pages\n",
    "\n",
    "    # def _build_llm_prompt(self, question: str, context: str, pages: List[str]) -> str:\n",
    "    def _build_llm_prompt(self, question: str, context: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant who answers questions based on the provided context.\n",
    "        You MUST cite the source page number for every piece of information you use.\n",
    "\n",
    "        **Instructions:**\n",
    "        1. Answer the user's question clearly and concisely using ONLY the provided context and knowledge graph information.\n",
    "        2. For every statement, you MUST provide the source page number in parentheses, like this: (Page XX).\n",
    "        3. If a single piece of information is supported by multiple pages, cite all of them: (Page X, Y, Z).\n",
    "        4. If no context is available, state that you are answering based on the graph structure alone.\n",
    "\n",
    "        **Example of a GOOD answer:**\n",
    "        The ductus arteriosus degenerates into the ligamentum arteriosum after birth(page 360). This is a normal physiological change that happens post-delivery(page 361).\n",
    "\n",
    "        **Example of a BAD answer:** -> (This is a bad answer because it lacks the mandatory citation)\n",
    "        The ductus arteriosus becomes the ligamentum arteriosum.\n",
    "\n",
    "        ---\n",
    "        **Context:**\n",
    "        {context}\n",
    "        ---\n",
    "        **Question:**\n",
    "        {question}\n",
    "        ---\n",
    "        **Answer:**\n",
    "        \"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def _call_llm_generate(self, prompt: str) -> str:\n",
    "        if self.llm_loader:\n",
    "            if hasattr(self.llm_loader, \"tokenizer\") and hasattr(self.llm_loader, \"model\"):\n",
    "                tokenizer = self.llm_loader.tokenizer\n",
    "                model = self.llm_loader.model\n",
    "\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    num_beams=3,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_ids = output[0][input_ids.shape[-1]:]\n",
    "                raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                return raw_answer\n",
    "            else:\n",
    "                raw_answer = self.llm_loader.generate(prompt)\n",
    "                return raw_answer\n",
    "        else:\n",
    "            print(\"generation_loader가 로드되지 않음\")\n",
    "            return \"LLM 로더가 설정되지 않았습니다.\"\n",
    "\n",
    "    def generate_response(self, question: str) -> Tuple[str, str]:\n",
    "        entities, relations = self._extract_entities_relations(question)\n",
    "        if not entities and not relations:\n",
    "            return \"질문에서 유효한 엔티티나 릴레이션을 추출할 수 없습니다.\", \"\"\n",
    "        \n",
    "        search_results = self._search_knowledge_graph(entities, relations)\n",
    "        if not search_results:\n",
    "            return \"관련 정보를 지식 그래프에서 찾을 수 없습니다.\", \"\"\n",
    "            \n",
    "        context, pages = self._retrieve_context_from_md(search_results)\n",
    "        if not context:\n",
    "            no_context_message = f\"관련 정보를 {', '.join(pages) if pages else 'N/A'}에서 발견할 수 없음\"\n",
    "            return no_context_message, \"\"\n",
    "        \n",
    "        prompt = self._build_llm_prompt(question, context)\n",
    "        \n",
    "        answer = self._call_llm_generate(prompt)\n",
    "        return answer, context\n",
    "\n",
    "def save_results_to_file(question: str, answer: str, context: str, output_dir: str, file_index: int):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%H%M%S_%f\")\n",
    "    file_name = f\"result_{file_index}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"[질문]\\n{question}\\n\\n\")\n",
    "        f.write(f\"[근거]\\n{context}\\n\\n\")\n",
    "        f.write(f\"[답변]\\n{answer}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qa_system = QASystem(\n",
    "        graphml_path=\"./data/knowledge_graph/knowledge_graph.graphml\",\n",
    "        md_path=\"./data/split_file/anatomy/\"\n",
    "    )\n",
    "    \n",
    "    qa_system.llm_loader = generation_loader\n",
    "    \n",
    "    questions = [\n",
    "        ############## 1_Embryology.md\n",
    "        \"What are the two essential components of a higher organism cell as defined in the text?\", # 7페이지\n",
    "        \"Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\", # 7페이지\n",
    "        \"What is the primary role of the yolk-sac in the embryo's early development?\", # 20페이지\n",
    "        \"How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\", # 19페이지\n",
    "        \"What significant developments occur in a human embryo during the Second Week?\", # 33페이지\n",
    "        \"What are the key characteristics of the human embryo by the end of the Third Week?\", # 33페이지\n",
    "        \n",
    "        ############## 2_Osteology.md\n",
    "        \"What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\", # 38페이지\n",
    "        \"How is each vertebral body formed from primitive segments during development?\", # 38페이지\n",
    "        \"What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\", # 88페이지\n",
    "        \"Describe the sphenoidal rostrum and its articulation.\",# 88\n",
    "        \"What is the tibia, and where is it located in the human leg?\", # 158\n",
    "        \"Describe the superior articular surface of the tibia's upper extremity.\", # 158\n",
    "\n",
    "        ############## 3_Syndesmology.md\n",
    "        \"What are joints or articulations, and how are immovable joints characterized?\", # 174\n",
    "        \"How does the articular lamella differ from ordinary bone tissue?\", # 174\n",
    "        \"Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\", # 207\n",
    "        \"List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\", # 207\n",
    "        \"What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\", # 236\n",
    "        \"How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\", # 236\n",
    "\n",
    "        ############## 4_Myology.md\n",
    "        \"How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\", # 250\n",
    "        \"Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\", # 250\n",
    "        \"What is the triangular ligament and where is it located?\", # 290\n",
    "        \"What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\", # 290\n",
    "        \"Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\", # 322\n",
    "        \"What is the Peronæus tertius, and where is it inserted?\", # 322\n",
    "\n",
    "        ############## 5_Angiology.md\n",
    "        \"What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\", # 334\n",
    "        \"Describe the composition and variations of the external coat (tunica adventitia) in arteries.\", # 334\n",
    "        \"How do the Vitelline Veins develop into parts of the portal and hepatic veins?\", # 345\n",
    "        \"What happens to the Umbilical Veins during embryonic development and after birth?\", # 345\n",
    "        \"What are the three phases of a cardiac cycle and what happens during each?\", # 358\n",
    "        \"What are the main peculiarities observed in the fetal heart's vascular system?\" # 359\n",
    "    ]   \n",
    "\n",
    "    today = datetime.now()\n",
    "    folder_name = f\"{today.month}월{today.day}일\"\n",
    "    output_dir = os.path.join(\"./result\", \"knowledge_graph\", folder_name)\n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"질문: {q}\")\n",
    "        response, context = qa_system.generate_response(q)\n",
    "        print(f\"답변: {response}\\n\")\n",
    "        save_results_to_file(q, response, context, output_dir, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce28cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./data/split_file/anatomy/1_Embryology.md...\n",
      "Extraction for 1_Embryology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/1_Embryology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/2_Osteology.md...\n",
      "Extraction for 2_Osteology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/2_Osteology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/3_Syndesmology.md...\n",
      "Extraction for 3_Syndesmology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/3_Syndesmology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/4_Myology.md...\n",
      "Extraction for 4_Myology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/4_Myology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/5_Angiology.md...\n",
      "Extraction for 5_Angiology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/5_Angiology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/9_Neurology.md...\n",
      "Extraction for 9_Neurology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/9_Neurology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/11_Splanchnology.md...\n",
      "Extraction for 11_Splanchnology.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/11_Splanchnology_extracted.txt\n",
      "Processing ./data/split_file/anatomy/6_The_Arteries.md...\n",
      "Extraction for 6_The_Arteries.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/6_The_Arteries_extracted.txt\n",
      "Processing ./data/split_file/anatomy/7_The_Veins.md...\n",
      "Extraction for 7_The_Veins.md completed. Results saved to ./data/extracted_results/LLMGraphTransformer/7_The_Veins_extracted.txt\n",
      "Processing ./data/split_file/anatomy/8_The_Lymphatic_System.md...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from langchain.schema import Generation, LLMResult\n",
    "# from langchain.llms.base import LLM\n",
    "# from model_loader.config import generation_loader\n",
    "# from langchain_core.documents import Document\n",
    "# from model_loader.local_loader import LocalModelLoader\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# def extract_entities_and_relations(input_dir, output_dir, pages_to_process=5):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     graph_generator = LLMGraphTransformer(llm=CustomLLM(generation_loader=generation_loader), strict_mode=False)\n",
    "\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if filename.endswith(\".md\"):\n",
    "#             filepath = os.path.join(input_dir, filename)\n",
    "#             print(f\"Processing {filepath}...\")\n",
    "#             with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 content = f.read()\n",
    "\n",
    "#             pages = content.split('\\n\\n') \n",
    "            \n",
    "#             extracted_text = []\n",
    "#             for i, page_content in enumerate(pages[:pages_to_process]):\n",
    "#                 if not page_content.strip():\n",
    "#                     continue\n",
    "\n",
    "#                 try:\n",
    "#                     documents = [Document(page_content=page_content)]\n",
    "                    \n",
    "#                     graph_documents = graph_generator.convert_to_graph_documents(documents)\n",
    "\n",
    "#                     for graph_document in graph_documents:\n",
    "#                         extracted_text.append(f\"--- Page {i+1} ---\")\n",
    "#                         extracted_text.append(\"Entities:\")\n",
    "#                         for entity in graph_document.nodes:\n",
    "#                             extracted_text.append(f\"  - {entity.type}: {entity.id}\")\n",
    "                        \n",
    "#                         extracted_text.append(\"\\nRelationships:\")\n",
    "#                         for relationship in graph_document.relationships:\n",
    "#                             extracted_text.append(f\"  - ({relationship.source.id})-[{relationship.type}]->({relationship.target.id})\")\n",
    "#                         extracted_text.append(\"\\n\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing page {i+1} of {filename}: {e}\")\n",
    "#                     extracted_text.append(f\"--- Error processing Page {i+1}: {e} ---\")\n",
    "                \n",
    "#             output_filename = os.path.splitext(filename)[0] + \"_extracted.txt\"\n",
    "#             output_filepath = os.path.join(output_dir, output_filename)\n",
    "#             with open(output_filepath, \"w\", encoding=\"utf-8\") as out_f:\n",
    "#                 out_f.write(\"\\n\".join(extracted_text))\n",
    "#             print(f\"Extraction for {filename} completed. Results saved to {output_filepath}\")\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     generation_loader: object\n",
    "#     is_local_model: bool = False\n",
    "#     llm_model: object = None\n",
    "#     llm_tokenizer: object = None\n",
    "\n",
    "#     def __init__(self, generation_loader, **kwargs):\n",
    "#         super().__init__(generation_loader=generation_loader, **kwargs) \n",
    "#         self.generation_loader = generation_loader\n",
    "#         self.is_local_model = isinstance(self.generation_loader, LocalModelLoader)\n",
    "#         if self.is_local_model:\n",
    "#             self.llm_model = self.generation_loader.model\n",
    "#             self.llm_tokenizer = self.generation_loader.tokenizer\n",
    "\n",
    "#     def _call(self, prompt: str, stop=None) -> str:\n",
    "#         if self.is_local_model:\n",
    "#             inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n",
    "#             max_new_tokens = 512\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.llm_model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens = max_new_tokens,\n",
    "#                     num_return_sequences = 1,\n",
    "#                     do_sample = True,\n",
    "#                     temperature = 0.4,\n",
    "#                     top_p = 0.9,\n",
    "#                     repetition_penalty = 1.2,\n",
    "#                     eos_token_id = self.llm_tokenizer.eos_token_id\n",
    "#                 )\n",
    "#             generated_text = self.llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "#         else:\n",
    "#             generated_text = self.generation_loader.generate(prompt)\n",
    "#         return generated_text\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom_generation_loader_llm\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_directory = \"./data/split_file/anatomy\"\n",
    "#     output_directory = \"./data/extracted_results/LLMGraphTransformer\"\n",
    "#     extract_entities_and_relations(input_directory, output_directory, pages_to_process=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY_NODE_TYPES : \n",
      "['ADJECTIVE', 'ALIAS', 'ANATOMICAL_FORAMEN', 'ANATOMICAL_GROUP', 'ANATOMICAL_LANDMARK', 'ANATOMICAL_PROCESS', 'ANATOMICALREGION', 'ANATOMICALSPACE', 'ANATOMICAL_SURFACE', 'ANATOMICAL_TERM', 'ANATOMICALSTRUCTURE', 'ANATOMY', 'ANIMAL', 'APONEUROSIS', 'ARTIFACT', 'AXIS', 'BIOLOGICAL_MATERIAL', 'BIOLOGICAL_PROCESS', 'BIOLOGICALENTITY', 'BIOLOGICALPROCESS', 'BODYFLUID', 'BONE', 'BONE_AXIS', 'BONE_DESCRIPTION', 'BONE_SYSTEM', 'BONE_TISSUE', 'BONE_TYPE', 'BORDER', 'BOUNDARY', 'BRAIN_STRUCTURE', 'BRAINSTEM', 'CANAL', 'CAPSULE', 'CARTILAGE', 'CELL', 'CELL_TYPE', 'CELLSTRUCTURE', 'CELLTYPE', 'CHARACTERISTIC', 'CHEMICAL', 'CIRCLE', 'CIRCULATION', 'CIRCULATORY_SYSTEM', 'CREST', 'CURVE', 'DATA', 'DATE', 'DEPRESSION', 'DESCRIPTION', 'DIRECTION', 'DISK', 'DIVERTICULUM', 'DUCT', 'DYE', 'ELEMENT', 'EMINENCE', 'ENVIRONMENT', 'EVENT', 'EXTREMITY', 'FASCIA', 'FIBER', 'FIELD_OF_STUDY', 'FLUID', 'FORAMEN', 'FORMATION', 'GENDER', 'GEOMETRICPROPERTY', 'GLAND', 'GROOVE', 'GROUP', 'INFORMATION', 'JOINT', 'JUNCTION', 'LAW', 'LAYER', 'LIFE_STAGE', 'LIGAMENT', 'LINE', 'MANNER', 'MATERIALPROPERTY', 'MEATUS', 'MECHANISM', 'MEDICALCONDITION', 'MEMBRANE', 'MOLECULE', 'MUSCLE', 'MUSCLE_ACTION', 'NERVE', 'NERVOUS_SYSTEM', 'NETWORK', 'NODE', 'NOMENCLATURE', 'NOTCH', 'OBSERVATION', 'ORGAN', 'ORGAN_SYSTEM', 'ORGANELLE', 'ORGANISM', 'ORIGIN', 'PATH', 'PERSON', 'PHYSICAL_CONTACT', 'PHYSICALACTION', 'PHYSICALPROPERTY', 'PHYSICALSUPPORT', 'PLANE', 'PLATE', 'POINT', 'PROCESS', 'PROFESSION', 'PROPERTY', 'PROTEIN', 'PUBLICATION', 'QUANTITY', 'RIDGE', 'SECTION', 'SEPTUM', 'SHAPE', 'SINUS', 'SKIN', 'SPATIALARRANGEMENT', 'SPINE', 'STATE', 'SUBSTANCE', 'SURFACE', 'SYSTEM', 'TENDENCY', 'TENDON', 'THEORY', 'TISSUE', 'TOOL', 'TOOTH', 'TOPIC', 'TUBEROSITY', 'VALVE', 'VESICLE', 'VESSEL', 'WORK', 'ANATOMY', 'BODY_PART', 'LIGAMENT', 'NERVE']\n",
      "entity 개수 : 142\n",
      "MY_RELATIONSHIP_TYPES : \n",
      "['ABDUCT', 'ABSORB', 'ACCOMPLISH', 'ACCUMULATE', 'ACTION', 'ADAPT', 'ADMIT', 'AFFECT', 'ALLOW', 'ANTAGONIZE', 'APPEAR', 'ARCHES_OVER', 'ARCH_FORWARD', 'ARE', 'ATTACH', 'BRING', 'CHARACTERIZE', 'ESTABLISH', 'EXPAND', 'FORM', 'FUSE', 'MAKE', 'OCCUPY', 'ORIFICE', 'OSSIFY', 'REPLACE', 'ARISE', 'ARRANGE', 'ARTICULATE', 'ASSIST', 'ASSUME', 'ATTACHMENT', 'AUTHORED', 'AUTHOR_OF', 'SACROCOCCYGEAL_LIGAMENT_POSTERIOR', 'BEAR', 'BEGIN', 'BELIEVE', 'BELOW', 'BEND', 'BIND', 'BLEND', 'BOUND', 'BREAK', 'BRIDGE', 'DO', 'CAPABLE_OF', 'CARRY', 'CAUSE', 'CHANGE', 'CHECK', 'CLOSE', 'CLOTH', 'COME', 'COMMENCE', 'COMMUNICATE', 'COMPLETE', 'COMPOSE', 'COMPRESS', 'CONNECT', 'CONSTITUTE', 'CONTINUE', 'CONTRACT', 'CONTRIBUTE', 'CONVERGE', 'CONVERT', 'CORRESPOND', 'COVER', 'CREATE', 'CROSS', 'CURVE', 'CUT', 'SACROCOCCYGEAL_LIGAMENT_LATERAL', 'PUBIC_ARCH_FORMATION', 'SUPERIOR_PUBIC_LIGAMENT', 'DECREASE', 'DEEPEN', 'DEGENERATE', 'DEPOSIT', 'DERIVE_FROM', 'DESCEND', 'DESCRIBE', 'DETERMINE', 'DEVELOPE', 'DIFFER', 'DIMINISH', 'DIRECT', 'DISTRIBUTE', 'DIVIDE', 'CONTRIBUTE', 'DRAIN', 'DRAW', 'SACROCOCCYGEAL_LIGAMENT_ANTERIOR', 'EFFECT', 'ELONGATE', 'EMBRACE', 'ENCIRCLE', 'ENCLOSE', 'END', 'ENSHEATHED_BY', 'ENTER', 'ENVELOP', 'EVERT', 'EXAMINE', 'EXCLUDE', 'EXHIBIT', 'EXIST', 'EXPAND', 'EXTEND', 'FAIL', 'FILL', 'FIT', 'FIX', 'FLARE', 'FLOW', 'FOLLOW', 'ATTACH', 'PROTECT', 'FUNCTION', 'FUSE', 'GIVE', 'GLIDE', 'GROOVE', 'GROUP', 'GROW', 'GUIDE', 'HARMONIZE', 'HAS', 'HAS_ABNORMALITY', 'HAS_ADAPTATION', 'HAS_AGE', 'HAS_ALIAS', 'HAS_ANGLE', 'HAS_APPEARANCE', 'HAS_ARCH', 'HAS_ARRANGEMENT', 'HAS_ARTICULATION', 'HAS_ATTACHMENT', 'HAS_BASE', 'HAS_BONE', 'HAS_CENTER', 'HAS_CIRCUMFERENCE', 'HAS_CONDITION', 'HAS_COUNT', 'HAS_DATE', 'HAS_DEPRESSION', 'HAS_DIAMETER', 'HAS_DISLOCATION', 'HAS_ELASTICITY', 'HAS_ENDS', 'HAS_ESTIMATED_AGE', 'HAS_EXAMPLE', 'HAS_EXCEPTION', 'HAS_FACET', 'HAS_FACETS', 'HAS_FEATURE', 'HAS_FIBERS', 'HAS_FLOOR_OF', 'HAS_FORCE', 'HAS_FUNCTION', 'HAS_FUNCTION_OF', 'HAS_GROOVE', 'HAS_GROWTH', 'HAS_IMPRESSION', 'HAS_INSERTION', 'HAS_JOINT', 'HAS_LAYER', 'HAS_LENGTH', 'HAS_LIGAMENT', 'HAS_LIMITED', 'HAS_LINE', 'HAS_LINING', 'HAS_LOCATION', 'HAS_MAXIMUM', 'HAS_NUCLEI', 'HAS_NUMBER', 'HAS_NUMBER_OF_BONES', 'HAS_ORIGIN', 'HAS_ORIGIN_ON', 'HAS_PLAN', 'HAS_POSITION', 'HAS_PRIMARY_CENTER', 'HAS_PROCESS', 'HAS_PROPERTY', 'HAS_PULL', 'HAS_QUANTITY', 'HAS_RAMIFICATIONS', 'HAS_RESISTANCE', 'HAS_SCALE', 'HAS_SECONDARY_CENTER', 'HAS_SECTION', 'HAS_SEGMENTATION', 'HAS_SEGMENTS', 'HAS_SHAPE', 'HAS_SHEATH', 'HAS_SIMILAR_ARRANGEMENT_TO', 'HAS_SIZE', 'HAS_SMOOTHNESS', 'HAS_SPACES', 'HAS_STAGE', 'HAS_SURFACE', 'HAS_THICKNESS', 'HAS_TYPE', 'HAS_VARIETY', 'HAS_VOLUME', 'HAVE', 'HOLD', 'IMBED', 'IMPLANT', 'IMPORTANT', 'INCREASE', 'INDENT', 'INDICATE', 'INHERIT', 'INSERT', 'INTERPOSE', 'INTERRUPT', 'INTERVENE', 'INVADE', 'INVEST', 'INVOLVE', 'CONTACT', 'IS', 'ACCOMPANY', 'ACCURATE', 'BE_KNOWN_AS', 'APPLY', 'PROLONG', 'BEHIND', 'BLEND', 'BUILD', 'CALL', 'CARRY', 'CHECK', 'BE_CLOSER_TO', 'COMPLETE', 'COMPLICATE', 'CONCAVE', 'BE_CONDITION', 'CONTINUE', 'CONVERT', 'CONVEY', 'COVER', 'DEPRESS', 'DEVELOP', 'DIRECT', 'DIVIDE', 'DRAW', 'EFFECT', 'ELEVATE', 'ENCLOSE', 'ENLARGE', 'BE_EXCEPTION', 'EXPAND', 'EXPEL', 'FILL', 'FIX', 'BE_FOR', 'FORM', 'BE_FREE_FROM', 'FUSE', 'BE_HOMOLOGUE_OF', 'BE_IN', 'INCLUDE', 'INCREASE', 'INDICATE', 'INFRONT_OF', 'BE_INTERMEDIATE', 'INVOLVE', 'LARGER_THAN', 'BE_LIABLE_TO', 'LIMIT', 'LINE', 'LODGE', 'BE_MADE_OF', 'BE_MORE_EXTENSIVE_BETWEEN', 'NARROW', 'CONNECTED_TO', 'NOT_COVERED_BY', 'NOT_RECOGNIZE', 'BE_OF_SIZE', 'PERFORATE', 'PIERCE', 'PLACE', 'BE_POINT_OF', 'BE_POSSIBLE_BY', 'PRESENT_AS', 'PRESS', 'PREVENT', 'PUBLISH', 'RAISE', 'BE_RARE_IN', 'REDUCE', 'REGARD', 'RELAX', 'TENSE', 'RETURN', 'ROOF', 'SEGMENT', 'SHOW', 'BE_SIMILAR_TO', 'BE_SMALLER_THAN', 'STRAIGHTEN', 'STRETCH', 'SUBDIVIDE', 'TERM', 'BE_THICKEST_ALONG', 'TILT', 'BE_TRACE_OF', 'VESTIGIAL', 'MOVE_TRUNK_FORWARD', 'LACK', 'LEAVE', 'LIE', 'LIMIT', 'LINE', 'LIVE_FROM', 'LOCATE', 'LODGE', 'LOOK', 'LOSE', 'LIE_BETWEEN', 'MAGNIFY_AT', 'MAINTAIN', 'MAKE', 'MARK', 'UTILIZE_FOR', 'CONSTITUTE', 'LIE_ON', 'OCCUR_IN', 'MEET_WITH', 'MEET', 'FUSE', 'MENTION', 'MIGRATE_TO', 'MISTRANSLATE_AS', 'MIX', 'MODIFY_TO', 'MOST_COMMON_IN', 'MOVE', 'NOT_PRESENT_IN', 'NUMBER', 'OBLITERATE', 'OCCUPY', 'OCCUR', 'OPEN', 'ORIGIN', 'OSSIFY_FROM', 'OVERCOME', 'OVERLAP', 'OWE', 'PARALLEL_TO', 'INSERT', 'PART_OF', 'PASS', 'PASSAGE', 'PERFORM', 'PERMIT', 'PERSIST', 'PLACE_AT', 'POINT_OUT', 'POSSESS', 'PRECEDE', 'PRESENT', 'PROJECT', 'PROLONG_FROM', 'PROPOSE', 'PROTECT', 'PROTRUDE', 'PROVIDE', 'PUBLISH_IN', 'PUMP_THROUGH', 'RADIATE_FROM', 'RAISE', 'RANGE_FROM', 'REACHE', 'RECEIVE', 'REFLECT', 'REGARD', 'REGULATE', 'RELEASE', 'REPRESENT', 'RESEMBLE', 'RESIST', 'REST_ON', 'RETAIN', 'RETARD', 'RETRACT', 'RETURN_TO', 'RETURN', 'REVOLVE_UPON', 'RISE_FROM', 'ROLL_UPON', 'ROTATE', 'RUDIMENT_OF', 'RUN', 'SEPARATE', 'SHRIVEL', 'SIMILAR_TO', 'SITUATE', 'SPREAD_OVER', 'SPRING_FROM', 'STAIN_WITH', 'START_AT', 'STATE', 'STEADY', 'STRENGTHEN', 'STRETCH', 'SUFFICE_TO_RETAIN', 'SUPPLY', 'SUPPORT', 'SURROUND', 'SUSPEND_FROM', 'SYNCHRONIZE_WITH', 'TAKE_FROM', 'TEND_TO', 'THICKEST_AT', 'TRACE', 'TRANSFER', 'TRANSFORM', 'TRANSMIT', 'TURN_AROUND', 'TYPE_OF', 'UNDERGO', 'UNION_TAKE_PLACE', 'UNITE', 'USE', 'CLOSE_ABOUT', 'VARY_IN', 'VISUALIZE_IN', 'ACCOMPANY', 'AFFPRD_SURFACE_FOR', 'BE_KNOWN_AS', 'ANTAGONIST_OF', 'APPEAR_IN', 'CLOSE_AT', 'COMPOSED', 'DISTRIBUTE', 'BE_FOUND_IN', 'ARISE_FROM', 'ARRANGE', 'ATTACHMENT', 'AUTHOR', 'SEPARATED', 'BEHIND', 'ACT_ON', 'BLEND', 'BLOOD_FROM', 'BRANCH_OF', 'SEPARATE_FROM', 'CAVITY_OF', 'COLLECT', 'COMMUNICATE_WITH', 'COMPLETE', 'COMPOSE', 'COMPRISE', 'CONSTITUTE', 'CONTAIN', 'CONTINUOUS_WITH', 'CONVERGE_TO', 'CONVERT_TO', 'CROSS', 'DEFICIENT_IN', 'DESCEND', 'DESCRIBE_AS', 'DISAPPEAR_UP_TO', 'DISTRIBUTED_TO', 'DRAW', 'ENCIRCLE', 'ENCLOSE', 'END_IN', 'OPEN_INTO', 'ENLARGED', 'ENTER', 'ENTRANCE', 'ESTABLISH', 'EXIST_IN_MIDDLE_OF', 'EXIT', 'EXPANSION_FROM_TENDON', 'FALL_ON', 'FILL', 'FIRST_INDICATION_OF', 'FIX', 'FIND_IN', 'FUSE', 'FUTURE', 'GIVE', 'GROOVED_FOR', 'HAVE', 'HIDE', 'HOLD', 'INDICATE', 'INNERVATE', 'INSERTION', 'INTEGRAL_PART_OF', 'INTERPOSE_BETWEEN', 'INTERRUPT', 'INTERVENE', 'BLEND', 'INVOLVE', 'CONTINUOUS_WITH', 'BE_IN', 'LOCATE', 'COMPOSE', 'NOT_RETURN_TO', 'CONVEY_BY', 'CONVEY_TO', 'SITUATE_BETWEEN', 'JUNCTION_OF', 'LACK', 'LIE', 'LINE', 'LOCATION', 'LODGE', 'LOOSENESS', 'LOWER_BORDER_OF', 'MARK', 'MEASURE', 'MIGRATE_OVER', 'MOVEMENT', 'OCCUPY', 'ONE_FOR', 'OPEN', 'OPPONENT', 'ORIGIN', 'PASS_FORWARD_TO', 'PASSAGE', 'PERFORM', 'PERMIT', 'PLUG', 'PRESENT', 'EXERT_ON', 'PRODUCE', 'PROJECT', 'PROLONGATION_UPWARD_OF', 'PULL', 'PURPOSE', 'PUSH', 'RECEIVE', 'REGULATION', 'RELATION', 'REPRESENT', 'RESIST', 'RETURN_BLOOD_TO', 'RISE', 'ROOF', 'SEPARATE', 'SERVE', 'SITUATE', 'COMPARE_SIZE', 'BE_STRONGEST', 'BE_MOST_DISTINCTSTRUCTURE', 'SUBDIVIDE', 'PREVENT', 'SUPPORT', 'PROTECT', 'SURMOUNT', 'TERMINATE', 'THICKEST_IN', 'THIN', 'TRANSMIT', 'TRAVERSE', 'TRUNK_OF', 'UNDER_SURFACE_OF', 'UNDERGO', 'UNITE', 'VARY', 'VISIBLE']\n",
      "relation 개수 : 568\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "MY_NODE_TYPES = ['Adjective', 'Alias', 'Anatomical Foramen', 'Anatomical Group', 'Anatomical Landmark', 'Anatomical Process', 'AnatomicalRegion', 'AnatomicalSpace', 'Anatomical Surface', 'Anatomical Term', 'AnatomicalStructure', 'Anatomy', 'Animal', 'Aponeurosis', 'Artifact', 'Axis', 'Biological Material', 'Biological Process', 'BiologicalEntity', 'BiologicalProcess', 'BodyFluid', 'Bone', 'Bone Axis', 'Bone Description', 'Bone System', 'Bone Tissue', 'Bone Type', 'Border', 'Boundary', 'Brain Structure', 'Brainstem', 'Canal', 'Capsule', 'Cartilage', 'Cell', 'Cell Type', 'CellStructure', 'CellType', 'Characteristic', 'Chemical', 'Circle', 'Circulation', 'Circulatory System', 'Crest', 'Curve', 'Data', 'Date', 'Depression', 'Description', 'Direction', 'Disk', 'Diverticulum', 'Duct', 'Dye', 'Element', 'Eminence', 'Environment', 'Event', 'Extremity', 'Fascia', 'Fiber', 'Field of Study', 'Fluid', 'Foramen', 'Formation', 'Gender', 'GeometricProperty', 'Gland', 'Groove', 'Group', 'Information', 'Joint', 'Junction', 'Law', 'Layer', 'Life Stage', 'Ligament', 'Line', 'Manner', 'MaterialProperty', 'Meatus', 'Mechanism', 'MedicalCondition', 'Membrane', 'Molecule', 'Muscle', 'Muscle Action', 'Nerve', 'Nervous System', 'Network', 'Node', 'Nomenclature', 'Notch', 'Observation', 'Organ', 'Organ System', 'Organelle', 'Organism', 'Origin', 'Path', 'Person', 'Physical Contact', 'PhysicalAction', 'PhysicalProperty', 'PhysicalSupport', 'Plane', 'Plate', 'Point', 'Process', 'Profession', 'Property', 'Protein', 'Publication', 'Quantity', 'Ridge', 'Section', 'Septum', 'Shape', 'Sinus', 'Skin', 'SpatialArrangement', 'Spine', 'State', 'Substance', 'Surface', 'System', 'Tendency', 'Tendon', 'Theory', 'Tissue', 'Tool', 'Tooth', 'Topic', 'Tuberosity', 'Valve', 'Vesicle', 'Vessel', 'Work', 'anatomy', 'body_part', 'ligament', 'nerve']\n",
    "\n",
    "MY_RELATIONSHIP_TYPES = ['ABDUCT', 'ABSORB', 'ACCOMPLISH', 'ACCUMULATE', 'ACTION', 'ADAPT', 'ADMIT', 'AFFECT', 'ALLOW', 'ANTAGONIZE', 'APPEAR', 'ARCHES_OVER', 'ARCH_FORWARD', 'ARE', 'ATTACH', 'BRING', 'CHARACTERIZE', 'ESTABLISH', 'EXPAND', 'FORM', 'FUSE', 'MAKE', 'OCCUPY', 'ORIFICE', 'OSSIFY', 'REPLACE', 'ARISE', 'ARRANGE', 'ARTICULATE', 'ASSIST', 'ASSUME', 'ATTACHMENT', 'AUTHORED', 'AUTHOR_OF', 'SACROCOCCYGEAL_LIGAMENT_POSTERIOR', 'BEAR', 'BEGIN', 'BELIEVE', 'BELOW', 'BEND', 'BIND', 'BLEND', 'BOUND', 'BREAK', 'BRIDGE', 'DO', 'CAPABLE_OF', 'CARRY', 'CAUSE', 'CHANGE', 'CHECK', 'CLOSE', 'CLOTH', 'COME', 'COMMENCE', 'COMMUNICATE', 'COMPLETE', 'COMPOSE', 'COMPRESS', 'CONNECT', 'CONSTITUTE', 'CONTINUE', 'CONTRACT', 'CONTRIBUTE', 'CONVERGE', 'CONVERT', 'CORRESPOND', 'COVER', 'CREATE', 'CROSS', 'CURVE', 'CUT', 'SACROCOCCYGEAL_LIGAMENT_LATERAL', 'PUBIC_ARCH_FORMATION', 'SUPERIOR_PUBIC_LIGAMENT', 'DECREASE', 'DEEPEN', 'DEGENERATE', 'DEPOSIT', 'DERIVE_FROM', 'DESCEND', 'DESCRIBE', 'DETERMINE', 'DEVELOPE', 'DIFFER', 'DIMINISH', 'DIRECT', 'DISTRIBUTE', 'DIVIDE', 'CONTRIBUTE', 'DRAIN', 'DRAW', 'SACROCOCCYGEAL_LIGAMENT_ANTERIOR', 'EFFECT', 'ELONGATE', 'EMBRACE', 'ENCIRCLE', 'ENCLOSE', 'END', 'ENSHEATHED_BY', 'ENTER', 'ENVELOP', 'EVERT', 'EXAMINE', 'EXCLUDE', 'EXHIBIT', 'EXIST', 'EXPAND', 'EXTEND', 'FAIL', 'FILL', 'FIT', 'FIX', 'FLARE', 'FLOW', 'FOLLOW', 'ATTACH', 'PROTECT', 'FUNCTION', 'FUSE', 'GIVE', 'GLIDE', 'GROOVE', 'GROUP', 'GROW', 'GUIDE', 'HARMONIZE', 'HAS', 'HAS_ABNORMALITY', 'HAS_ADAPTATION', 'HAS_AGE', 'HAS_ALIAS', 'HAS_ANGLE', 'HAS_APPEARANCE', 'HAS_ARCH', 'HAS_ARRANGEMENT', 'HAS_ARTICULATION', 'HAS_ATTACHMENT', 'HAS_BASE', 'HAS_BONE', 'HAS_CENTER', 'HAS_CIRCUMFERENCE', 'HAS_CONDITION', 'HAS_COUNT', 'HAS_DATE', 'HAS_DEPRESSION', 'HAS_DIAMETER', 'HAS_DISLOCATION', 'HAS_ELASTICITY', 'HAS_ENDS', 'HAS_ESTIMATED_AGE', 'HAS_EXAMPLE', 'HAS_EXCEPTION', 'HAS_FACET', 'HAS_FACETS', 'HAS_FEATURE', 'HAS_FIBERS', 'HAS_FLOOR_OF', 'HAS_FORCE', 'HAS_FUNCTION', 'HAS_FUNCTION_OF', 'HAS_GROOVE', 'HAS_GROWTH', 'HAS_IMPRESSION', 'HAS_INSERTION', 'HAS_JOINT', 'HAS_LAYER', 'HAS_LENGTH', 'HAS_LIGAMENT', 'HAS_LIMITED', 'HAS_LINE', 'HAS_LINING', 'HAS_LOCATION', 'HAS_MAXIMUM', 'HAS_NUCLEI', 'HAS_NUMBER', 'HAS_NUMBER_OF_BONES', 'HAS_ORIGIN', 'HAS_ORIGIN_ON', 'HAS_PLAN', 'HAS_POSITION', 'HAS_PRIMARY_CENTER', 'HAS_PROCESS', 'HAS_PROPERTY', 'HAS_PULL', 'HAS_QUANTITY', 'HAS_RAMIFICATIONS', 'HAS_RESISTANCE', 'HAS_SCALE', 'HAS_SECONDARY_CENTER', 'HAS_SECTION', 'HAS_SEGMENTATION', 'HAS_SEGMENTS', 'HAS_SHAPE', 'HAS_SHEATH', 'HAS_SIMILAR_ARRANGEMENT_TO', 'HAS_SIZE', 'HAS_SMOOTHNESS', 'HAS_SPACES', 'HAS_STAGE', 'HAS_SURFACE', 'HAS_THICKNESS', 'HAS_TYPE', 'HAS_VARIETY', 'HAS_VOLUME', 'HAVE', 'HOLD', 'IMBED', 'IMPLANT', 'IMPORTANT', 'INCREASE', 'INDENT', 'INDICATE', 'INHERIT', 'INSERT', 'INTERPOSE', 'INTERRUPT', 'INTERVENE', 'INVADE', 'INVEST', 'INVOLVE', 'CONTACT', 'IS', 'ACCOMPANY', 'ACCURATE', 'BE_KNOWN_AS', 'APPLY', 'PROLONG', 'BEHIND', 'BLEND', 'BUILD', 'CALL', 'CARRY', 'CHECK', 'BE_CLOSER_TO', 'COMPLETE', 'COMPLICATE', 'CONCAVE', 'BE_CONDITION', 'CONTINUE', 'CONVERT', 'CONVEY', 'COVER', 'DEPRESS', 'DEVELOP', 'DIRECT', 'DIVIDE', 'DRAW', 'EFFECT', 'ELEVATE', 'ENCLOSE', 'ENLARGE', 'BE_EXCEPTION', 'EXPAND', 'EXPEL', 'FILL', 'FIX', 'BE_FOR', 'FORM', 'BE_FREE_FROM', 'FUSE', 'BE_HOMOLOGUE_OF', 'BE_IN', 'INCLUDE', 'INCREASE', 'INDICATE', 'INFRONT_OF', 'BE_INTERMEDIATE', 'INVOLVE', 'LARGER_THAN', 'BE_LIABLE_TO', 'LIMIT', 'LINE', 'LODGE', 'BE_MADE_OF', 'BE_MORE_EXTENSIVE_BETWEEN', 'NARROW', 'CONNECTED_TO', 'NOT_COVERED_BY', 'NOT_RECOGNIZE', 'BE_OF_SIZE', 'PERFORATE', 'PIERCE', 'PLACE', 'BE_POINT_OF', 'BE_POSSIBLE_BY', 'PRESENT_AS', 'PRESS', 'PREVENT', 'PUBLISH', 'RAISE', 'BE_RARE_IN', 'REDUCE', 'REGARD', 'RELAX', 'TENSE', 'RETURN', 'ROOF', 'SEGMENT', 'SHOW', 'BE_SIMILAR_TO', 'BE_SMALLER_THAN', 'STRAIGHTEN', 'STRETCH', 'SUBDIVIDE', 'TERM', 'BE_THICKEST_ALONG', 'TILT', 'BE_TRACE_OF', 'VESTIGIAL', 'MOVE_TRUNK_FORWARD', 'LACK', 'LEAVE', 'LIE', 'LIMIT', 'LINE', 'LIVE_FROM', 'LOCATE', 'LODGE', 'LOOK', 'LOSE', 'LIE_BETWEEN', 'MAGNIFY_AT', 'MAINTAIN', 'MAKE', 'MARK', 'UTILIZE_FOR', 'CONSTITUTE', 'LIE_ON', 'OCCUR_IN', 'MEET_WITH', 'MEET', 'FUSE', 'MENTION', 'MIGRATE_TO', 'MISTRANSLATE_AS', 'MIX', 'MODIFY_TO', 'MOST_COMMON_IN', 'MOVE', 'NOT_PRESENT_IN', 'NUMBER', 'OBLITERATE', 'OCCUPY', 'OCCUR', 'OPEN', 'ORIGIN', 'OSSIFY_FROM', 'OVERCOME', 'OVERLAP', 'OWE', 'PARALLEL_TO', 'INSERT', 'PART_OF', 'PASS', 'PASSAGE', 'PERFORM', 'PERMIT', 'PERSIST', 'PLACE_AT', 'POINT_OUT', 'POSSESS', 'PRECEDE', 'PRESENT', 'PROJECT', 'PROLONG_FROM', 'PROPOSE', 'PROTECT', 'PROTRUDE', 'PROVIDE', 'PUBLISH_IN', 'PUMP_THROUGH', 'RADIATE_FROM', 'RAISE', 'RANGE_FROM', 'REACHE', 'RECEIVE', 'REFLECT', 'REGARD', 'REGULATE', 'RELEASE', 'REPRESENT', 'RESEMBLE', 'RESIST', 'REST_ON', 'RETAIN', 'RETARD', 'RETRACT', 'RETURN_TO', 'RETURN', 'REVOLVE_UPON', 'RISE_FROM', 'ROLL_UPON', 'ROTATE', 'RUDIMENT_OF', 'RUN', 'SEPARATE', 'SHRIVEL', 'SIMILAR_TO', 'SITUATE', 'SPREAD_OVER', 'SPRING_FROM', 'STAIN_WITH', 'START_AT', 'STATE', 'STEADY', 'STRENGTHEN', 'STRETCH', 'SUFFICE_TO_RETAIN', 'SUPPLY', 'SUPPORT', 'SURROUND', 'SUSPEND_FROM', 'SYNCHRONIZE_WITH', 'TAKE_FROM', 'TEND_TO', 'THICKEST_AT', 'TRACE', 'TRANSFER', 'TRANSFORM', 'TRANSMIT', 'TURN_AROUND', 'TYPE_OF', 'UNDERGO', 'UNION_TAKE_PLACE', 'UNITE', 'USE', 'CLOSE_ABOUT', 'VARY_IN', 'VISUALIZE_IN', 'ACCOMPANY', 'AFFPRD_SURFACE_FOR', 'be known as', 'antagonist of', 'appear in', 'close at', 'composed', 'distribute', 'be found in', 'arise from', 'arrange', 'attachment', 'author', 'separated', 'behind', 'act on', 'blend', 'blood from', 'branch of', 'separate from', 'cavity of', 'collect', 'communicate with', 'complete', 'compose', 'comprise', 'constitute', 'contain', 'continuous with', 'converge to', 'convert_to', 'cross', 'deficient in', 'descend', 'describe as', 'disappear up to', 'distributed to', 'draw', 'encircle', 'enclose', 'end in', 'open into', 'enlarged', 'enter', 'entrance', 'establish', 'exist in middle of', 'exit', 'expansion from tendon', 'fall on', 'fill', 'first indication of', 'fix', 'find in', 'fuse', 'future', 'give', 'grooved for', 'have', 'hide', 'hold', 'indicate', 'innervate', 'insertion', 'integral part of', 'interpose between', 'interrupt', 'intervene', 'blend', 'involve', 'continuous with', 'be in', 'locate', 'compose', 'not return to', 'convey by', 'convey to', 'situate between', 'junction of', 'lack', 'lie', 'line', 'location', 'lodge', 'looseness', 'lower border of', 'mark', 'measure', 'migrate over', 'movement', 'occupy', 'one for', 'open', 'opponent', 'origin', 'pass forward to', 'passage', 'perform', 'permit', 'plug', 'present', 'exert on', 'produce', 'project', 'prolongation upward of', 'pull', 'purpose', 'push', 'receive', 'regulation', 'relation', 'represent', 'resist', 'return blood to', 'rise', 'roof', 'separate', 'serve', 'situate', 'compare size', 'be strongest', 'be most distinct' 'structure', 'subdivide', 'prevent', 'support', 'protect', 'surmount', 'terminate', 'thickest in', 'thin', 'transmit', 'traverse', 'trunk of', 'under surface of', 'undergo', 'unite', 'vary', 'visible']\n",
    "\n",
    "output_entity = [s.upper().replace(' ', '_') for s in MY_NODE_TYPES]\n",
    "output_relation = [s.upper().replace(' ', '_') for s in MY_RELATIONSHIP_TYPES]\n",
    "print(\"MY_NODE_TYPES : \")\n",
    "print(output_entity)\n",
    "print(f\"entity 개수 : {len(output_entity)}\")\n",
    "\n",
    "print(\"MY_RELATIONSHIP_TYPES : \")\n",
    "print(output_relation)\n",
    "print(f\"relation 개수 : {len(output_relation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9a2838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 대상 폴더: ./data/split_file/anatomy/\n",
      "분석된 .md 파일 개수: 12개\n",
      "------------------------------\n",
      "전체 문단 개수: 9083개\n",
      "전체 문단 글자 수 합계: 3665997자\n",
      "모든 파일의 문단 평균 길이: 403.61자\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_overall_average_paragraph_length(directory_path: str) -> None:\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"오류: '{directory_path}' 디렉터리를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    md_files = glob.glob(os.path.join(directory_path, '*.md'))\n",
    "\n",
    "    if not md_files:\n",
    "        print(f\"오류: '{directory_path}' 디렉터리에서 .md 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    total_paragraph_length = 0\n",
    "    total_paragraph_count = 0\n",
    "\n",
    "    for file_path in md_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            paragraphs = content.split('\\n\\n')\n",
    "            \n",
    "            for p in paragraphs:\n",
    "                stripped_p = p.strip()\n",
    "                if stripped_p:\n",
    "                    total_paragraph_length += len(stripped_p)\n",
    "                    total_paragraph_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"'{file_path}' 파일을 처리하는 중 오류 발생: {e}\")\n",
    "\n",
    "    if total_paragraph_count == 0:\n",
    "        print(\"분석할 문단이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    average_length = total_paragraph_length / total_paragraph_count\n",
    "\n",
    "    print(f\"분석 대상 폴더: {directory_path}\")\n",
    "    print(f\"분석된 .md 파일 개수: {len(md_files)}개\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"전체 문단 개수: {total_paragraph_count}개\")\n",
    "    print(f\"전체 문단 글자 수 합계: {total_paragraph_length}자\")\n",
    "    print(f\"모든 파일의 문단 평균 길이: {average_length:.2f}자\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    anatomy_folder_path = './data/split_file/anatomy/'\n",
    "    calculate_overall_average_paragraph_length(anatomy_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 대상 폴더: ./data/split_file/anatomy/\n",
      "----------------------------------------\n",
      "📄 파일: 1_Embryology.md      | 최대 문단 길이: 3755자\n",
      "📄 파일: 2_Osteology.md       | 최대 문단 길이: 5589자\n",
      "📄 파일: 3_Syndesmology.md    | 최대 문단 길이: 3140자\n",
      "📄 파일: 4_Myology.md         | 최대 문단 길이: 3162자\n",
      "📄 파일: 5_Angiology.md       | 최대 문단 길이: 2970자\n",
      "📄 파일: 9_Neurology.md       | 최대 문단 길이: 2476자\n",
      "📄 파일: 11_Splanchnology.md  | 최대 문단 길이: 4685자\n",
      "📄 파일: 6_The_Arteries.md    | 최대 문단 길이: 2412자\n",
      "📄 파일: 7_The_Veins.md       | 최대 문단 길이: 1745자\n",
      "📄 파일: 8_The_Lymphatic_System.md | 최대 문단 길이: 2908자\n",
      "📄 파일: 10_The_Organs_of_the_Senses_and_the_Common_Integument.md | 최대 문단 길이: 2612자\n",
      "📄 파일: 12_Surface_Anatomy_and_Surface_Markings.md | 최대 문단 길이: 3148자\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def find_max_paragraph_length_per_file(directory_path: str) -> None:\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"오류: '{directory_path}' 디렉터리를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    md_files = glob.glob(os.path.join(directory_path, '*.md'))\n",
    "\n",
    "    if not md_files:\n",
    "        print(f\"오류: '{directory_path}' 디렉터리에서 .md 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"분석 대상 폴더: {directory_path}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for file_path in md_files:\n",
    "        max_length_in_file = 0\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            paragraphs = content.split('\\n\\n')\n",
    "            \n",
    "            for p in paragraphs:\n",
    "                stripped_p = p.strip()\n",
    "                if stripped_p:\n",
    "                    if len(stripped_p) > max_length_in_file:\n",
    "                        max_length_in_file = len(stripped_p)\n",
    "            \n",
    "            file_name = os.path.basename(file_path)\n",
    "            print(f\"📄 파일: {file_name:<20} | 최대 문단 길이: {max_length_in_file}자\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"'{file_path}' 파일을 처리하는 중 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    anatomy_folder_path = './data/split_file/anatomy/'\n",
    "    find_max_paragraph_length_per_file(anatomy_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/HarryPotterQA 폴더 내의 QA쌍 개수 : 28888\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def count(directory_path) :\n",
    "    total_qa_count = 0\n",
    "\n",
    "    if not os.path.isdir(directory_path) :\n",
    "        print(f\"{directory_path}를 찾을 수 없습니다\")\n",
    "        return 0\n",
    "    \n",
    "    for filename in os.listdir(directory_path) :\n",
    "        if filename.endswith(\".json\") :\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try :\n",
    "                with open(file_path, 'r', encoding=\"utf-8\") as f :\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list) :\n",
    "                        total_qa_count += len(data)\n",
    "                    elif isinstance(data, dict) :\n",
    "                        total_qa_count += 1\n",
    "            except json.JSONDecodeError :\n",
    "                print(f\"{file_path} 파일의 형식이 잘못됨\")\n",
    "            except Exception as e :\n",
    "                print(f\"{file_path} 파일을 처리하는 중 오류 : {e}\")\n",
    "\n",
    "    return total_qa_count\n",
    "\n",
    "target_directory = \"./data/HarryPotterQA\"\n",
    "\n",
    "total_pairs = count(target_directory)\n",
    "\n",
    "if total_pairs > 0 :\n",
    "    print(f\"{target_directory} 폴더 내의 QA쌍 개수 : {total_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08521a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
