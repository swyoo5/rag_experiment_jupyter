{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from model_loader.config import embedding_loader, generation_loader\n",
    "from chroma_db import ChromaVectorDB\n",
    "\n",
    "class HybridSearcher:\n",
    "    def __init__(self, embedding_model, chunk_size=200, chunk_overlap=50, \n",
    "                 persist_directory=\"./chroma_db\", collection_name=\"Gray\"):\n",
    "        \"\"\"\n",
    "        하이브리드 검색 클래스 초기화\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: 임베딩 모델\n",
    "            chunk_size (int): 청크 크기\n",
    "            chunk_overlap (int): 청크 간 중복 크기\n",
    "            persist_directory (str): 벡터 DB 저장 경로\n",
    "            collection_name (str): 벡터 DB 컬렉션 이름\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.chunks = None\n",
    "        self.chunk_metadata = None\n",
    "        self.bm25_index = None\n",
    "        self.vector_index = None\n",
    "        \n",
    "        # ChromaDB 벡터 DB 클래스 초기화 - 동일한 임베딩 모델 사용\n",
    "        self.vector_db = ChromaVectorDB(\n",
    "            embedding_model=self.embedding_model,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        # 검색 결과 비교용 플래그\n",
    "        self.debug_mode = False\n",
    "        \n",
    "        # 프롬프트 로드\n",
    "        self.complexity_prompt = self._load_prompt(\"prompts/en/complex/complex_prompt.txt\")\n",
    "        self.decompose_prompt = self._load_prompt(\"prompts/en/decompose/decompose_prompt.txt\")\n",
    "\n",
    "    def _load_prompt(self, file_path):\n",
    "        \"\"\"프롬프트 파일 로드\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"프롬프트 파일 로드 중 오류 발생: {e}\")\n",
    "            # 기본 프롬프트 반환\n",
    "            if \"complex\" in file_path:\n",
    "                return \"질문을 분석하여 복합적인 질문인지 판단하세요. 복합적인 질문은 여러 하위 질문으로 분해할 수 있습니다. '예' 또는 '아니오'로만 답변하세요.\"\n",
    "            elif \"decompose\" in file_path:\n",
    "                return \"다음 복합적인 질문을 여러 개의 간단한 하위 질문으로 분해하세요. JSON 형식으로 하위 질문 목록을 반환하세요.\"\n",
    "\n",
    "    def load_document(self, file_path):\n",
    "        \"\"\"문서 로드 및 청크 분할\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 문서를 청크로 분할\n",
    "        self.chunks, self.chunk_metadata = self._split_into_chunks_with_metadata(content)\n",
    "        \n",
    "        # BM25 인덱스 생성\n",
    "        tokenized_chunks = [self._simple_tokenize(chunk) for chunk in self.chunks]\n",
    "        self.bm25_index = BM25Okapi(tokenized_chunks)\n",
    "        \n",
    "        # 벡터 임베딩 생성 및 인메모리 인덱스 구축\n",
    "        self.vector_index = self.embedding_model.encode(self.chunks)\n",
    "        \n",
    "        # ChromaDB에 문서 추가\n",
    "        # 문서 ID 생성\n",
    "        doc_name = os.path.basename(file_path)\n",
    "        doc_ids = [f\"{doc_name}_{i}\" for i in range(len(self.chunks))]\n",
    "        \n",
    "        # 메타데이터에 페이지 정보 포함\n",
    "        metadatas = [{\"page\": meta[\"page\"], \"source\": doc_name, \"chunk_index\": i} \n",
    "                    for i, meta in enumerate(self.chunk_metadata)]\n",
    "        \n",
    "        # 벡터 DB에 문서 추가\n",
    "        self.vector_db.add_documents(\n",
    "            documents=self.chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=doc_ids\n",
    "        )\n",
    "        \n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def search(self, query, top_n=5, alpha=0.5, use_vector_db=True):\n",
    "        \"\"\"\n",
    "        하이브리드 검색 수행\n",
    "        \n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            top_n (int): 반환할 결과 수\n",
    "            alpha (float): BM25와 벡터 검색 결과 결합 비율 (0에 가까울수록 벡터 검색 중시)\n",
    "            use_vector_db (bool): ChromaDB 벡터 DB 사용 여부\n",
    "        \n",
    "        Returns:\n",
    "            list: 검색 결과 리스트\n",
    "        \"\"\"\n",
    "        if self.chunks is None or self.bm25_index is None:\n",
    "            raise ValueError(\"문서가 로드되지 않았습니다. load_document()를 먼저 호출하세요.\")\n",
    "        \n",
    "        if not isinstance(query, str) :\n",
    "            query = str(query)\n",
    "            \n",
    "        # BM25 검색 수행\n",
    "        bm25_scores = self.bm25_index.get_scores(self._simple_tokenize(query))\n",
    "        \n",
    "        # 두 검색 방식 모두 수행 (디버그 모드)\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        memory_scores = cosine_similarity([query_embedding], self.vector_index)[0]\n",
    "        \n",
    "        if use_vector_db:\n",
    "            # ChromaDB를 사용한 벡터 검색\n",
    "            db_results = self.vector_db.search(query, top_n=len(self.chunks))\n",
    "            \n",
    "            # 결과를 벡터 점수로 변환\n",
    "            vector_scores = np.zeros(len(self.chunks))\n",
    "            for res in db_results:\n",
    "                # 문서 ID에서 인덱스 추출\n",
    "                chunk_id = res[\"id\"]\n",
    "                if \"_\" in chunk_id:\n",
    "                    try:\n",
    "                        chunk_index = int(chunk_id.split(\"_\")[-1])\n",
    "                        if chunk_index < len(self.chunks):\n",
    "                            # 거리를 유사도로 변환 (코사인 거리는 1 - 코사인 유사도)\n",
    "                            if res[\"distance\"] is not None:\n",
    "                                # ChromaDB가 코사인 거리를 사용하는 경우\n",
    "                                similarity = 1.0 - res[\"distance\"]\n",
    "                                vector_scores[chunk_index] = similarity\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            # 디버그 모드에서 점수 비교\n",
    "            if self.debug_mode:\n",
    "                print(\"\\n벡터 검색 점수 비교:\")\n",
    "                for i in range(min(5, len(self.chunks))):\n",
    "                    print(f\"Chunk {i}: Memory={memory_scores[i]:.4f}, ChromaDB={vector_scores[i]:.4f}, 차이={memory_scores[i]-vector_scores[i]:.4f}\")\n",
    "        else:\n",
    "            # 메모리 내 벡터 검색 수행 (기존 방식)\n",
    "            vector_scores = memory_scores\n",
    "        \n",
    "        # 검색 결과 결합\n",
    "        combined_scores = self._combine_scores(bm25_scores, vector_scores, alpha)\n",
    "        \n",
    "        # 상위 N개 결과 반환\n",
    "        top_indices = np.argsort(combined_scores)[-top_n:][::-1]\n",
    "        results = [\n",
    "            {\n",
    "                \"chunk\": self.chunks[i],\n",
    "                \"score\": combined_scores[i],\n",
    "                \"bm25_score\": bm25_scores[i],\n",
    "                \"vector_score\": vector_scores[i],\n",
    "                \"memory_score\": memory_scores[i],  # 디버깅용\n",
    "                \"index\": i,\n",
    "                \"page\": self.chunk_metadata[i][\"page\"]\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _split_into_chunks_with_metadata(self, text):\n",
    "        \"\"\"텍스트를 청크로 분할하고 페이지 정보를 메타데이터로 유지하는 함수\"\"\"\n",
    "        chunks = []\n",
    "        chunk_metadata = []\n",
    "        \n",
    "        # 페이지 패턴 정규식 (####으로 시작하는 페이지 헤더)\n",
    "        page_pattern = re.compile(r'####\\s*Page (\\d+)')\n",
    "        \n",
    "        # 텍스트를 줄 단위로 처리\n",
    "        lines = text.split('\\n')\n",
    "        current_page = \"unknown\"\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            # 페이지 헤더 확인\n",
    "            page_match = page_pattern.match(line)\n",
    "            \n",
    "            if page_match:\n",
    "                # 새 페이지 시작\n",
    "                # 현재 청크가 있으면 저장\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    chunk_metadata.append({\"page\": current_page})\n",
    "                    current_chunk = \"\"\n",
    "                \n",
    "                # 새 페이지 번호 설정\n",
    "                current_page = page_match.group(1)\n",
    "                continue\n",
    "            \n",
    "            # 현재 청크에 라인 추가\n",
    "            current_chunk += line + \"\\n\"\n",
    "            \n",
    "            # 청크 크기 확인\n",
    "            if len(current_chunk) >= self.chunk_size:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                chunk_metadata.append({\"page\": current_page})\n",
    "                current_chunk = \"\"  # 새 청크 시작\n",
    "        \n",
    "        # 마지막 청크 처리\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "            chunk_metadata.append({\"page\": current_page})\n",
    "        \n",
    "        # 너무 작은 청크 결합 (메타데이터 유지)\n",
    "        i = 0\n",
    "        while i < len(chunks) - 1:\n",
    "            if len(chunks[i]) + len(chunks[i+1]) < self.chunk_size:\n",
    "                # 같은 페이지인 경우에만 결합\n",
    "                if chunk_metadata[i][\"page\"] == chunk_metadata[i+1][\"page\"]:\n",
    "                    chunks[i] = chunks[i] + \"\\n\\n\" + chunks[i+1]\n",
    "                    chunks.pop(i+1)\n",
    "                    chunk_metadata.pop(i+1)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return chunks, chunk_metadata\n",
    "    \n",
    "    def _simple_tokenize(self, text):\n",
    "        \"\"\"텍스트를 간단히 토크나이징하는 함수\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    def _combine_scores(self, bm25_scores, vector_scores, alpha=0.5):\n",
    "        \"\"\"BM25와 벡터 검색 점수를 결합\"\"\"\n",
    "        # 점수 정규화\n",
    "        if np.max(bm25_scores) > 0:\n",
    "            bm25_scores = bm25_scores / np.max(bm25_scores)\n",
    "        if np.max(vector_scores) > 0:\n",
    "            vector_scores = vector_scores / np.max(vector_scores)\n",
    "        \n",
    "        # 가중 평균 계산\n",
    "        combined = alpha * bm25_scores + (1 - alpha) * vector_scores\n",
    "        return combined\n",
    "    \n",
    "    def get_chunks_with_page_info(self, indices=None):\n",
    "        \"\"\"청크와 페이지 정보 반환\"\"\"\n",
    "        if indices is None:\n",
    "            return [(chunk, self.chunk_metadata[i][\"page\"]) for i, chunk in enumerate(self.chunks)]\n",
    "        else:\n",
    "            return [(self.chunks[i], self.chunk_metadata[i][\"page\"]) for i in indices if i < len(self.chunks)]\n",
    "    \n",
    "    # 1. 질문이 복합적인지 확인하는 메서드\n",
    "    def is_complex_question(self, question):\n",
    "        \"\"\"\n",
    "        LLM을 사용하여 질문이 복합적인지 판단하는 메서드\n",
    "        \n",
    "        Args:\n",
    "            question (str): 사용자 질문\n",
    "            \n",
    "        Returns:\n",
    "            bool: 복합적인 질문이면 True, 아니면 False\n",
    "        \"\"\"\n",
    "        prompt = self.complexity_prompt.format(question=question)\n",
    "        response = generation_loader.generate(prompt)\n",
    "        \n",
    "        # '예' 또는 'Yes'가 응답에 포함되어 있으면 복합적인 질문으로 간주\n",
    "        response = response.lower().strip()\n",
    "        print(f\"###############질문이 복잡한가요? : {response}\")\n",
    "        return '예' in response or 'yes' in response\n",
    "    \n",
    "    # 2. 복합적인 질문을 하위 질문으로 분해하는 메서드\n",
    "    def decompose_question(self, complex_question):\n",
    "        \"\"\"\n",
    "        LLM을 사용하여 복합적인 질문을 여러 개의 하위 질문으로 분해하는 메서드\n",
    "        \n",
    "        Args:\n",
    "            complex_question (str): 복합적인 사용자 질문\n",
    "            \n",
    "        Returns:\n",
    "            list: 하위 질문 목록\n",
    "        \"\"\"\n",
    "        prompt = self.decompose_prompt.format(question=complex_question)\n",
    "        response = generation_loader.generate(prompt)\n",
    "\n",
    "        try:\n",
    "            # JSON 형식으로 반환된 하위 질문 파싱\n",
    "            # JSON 블록 추출 (```json과 ```로 감싸져 있을 수 있음)\n",
    "            json_match = re.search(r'```json\\s*(.+?)\\s*```', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                response = json_match.group(1)\n",
    "            \n",
    "            # 중괄호 블록 추출\n",
    "            json_match = re.search(r'(\\{.+?\\})', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                response = json_match.group(1)\n",
    "                \n",
    "            sub_questions_data = json.loads(response)\n",
    "            # 다양한 JSON 형식 처리\n",
    "            if isinstance(sub_questions_data, list):\n",
    "                return sub_questions_data\n",
    "            elif isinstance(sub_questions_data, dict):\n",
    "                if \"questions\" in sub_questions_data:\n",
    "                    return sub_questions_data[\"questions\"]\n",
    "                elif \"subQuestions\" in sub_questions_data:\n",
    "                    return sub_questions_data[\"subQuestions\"]\n",
    "                else:\n",
    "                    # 딕셔너리의 값들을 리스트로 반환\n",
    "                    return list(sub_questions_data.values())\n",
    "                \n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            print(f\"하위 질문 파싱 오류: {e}\")\n",
    "            print(f\"LLM 응답: {response}\")\n",
    "            \n",
    "            # 파싱 실패 시 줄바꿈을 기준으로 질문 추출 시도\n",
    "            questions = []\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and ('?' in line or '질문' in line):\n",
    "                    # 숫자, 점, 괄호 등의 접두어 제거\n",
    "                    cleaned_line = re.sub(r'^[\\d\\.\\)\\-\\s]+', '', line).strip()\n",
    "                    if cleaned_line:\n",
    "                        questions.append(cleaned_line)\n",
    "            \n",
    "            if questions:\n",
    "                return questions\n",
    "            # 단일 질문으로 처리\n",
    "            return [complex_question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef70066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader.config import *\n",
    "from save_utils import *\n",
    "from translate import *\n",
    "\n",
    "class CarManualQA:\n",
    "    def __init__(self, generation_loader, data_folder=\"./data/split_file\", \n",
    "                 prompt_path_ko=\"./prompts/ko/generation/gemma3/generation_prompt2.txt\", \n",
    "                 prompt_path_en=\"./prompts/en/generation/gemma3/generation_prompt2.txt\", \n",
    "                 result_path=\"./result/5월12일/gemma3\",\n",
    "                 use_vector_db=True, \n",
    "                 persist_directory=\"./chroma_db\",\n",
    "                 collection_name=\"Gray\",\n",
    "                 language=\"en\"):\n",
    "        \"\"\"\n",
    "        자동차 매뉴얼 Q&A 시스템 초기화\n",
    "        \n",
    "        Args:\n",
    "            generation_loader: 텍스트 생성 모델 로더\n",
    "            data_folder (str): 분할된 데이터 파일들이 있는 폴더 경로\n",
    "            prompt_path (str): 프롬프트 템플릿 파일 경로\n",
    "            result_path (str): 결과를 저장할 경로\n",
    "            use_vector_db (bool): ChromaDB 벡터 DB 사용 여부\n",
    "            persist_directory (str): 벡터 DB 저장 경로\n",
    "            collection_name (str): 벡터 DB 컬렉션 이름\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.result_path = result_path\n",
    "        self.use_vector_db = use_vector_db\n",
    "        self.language = language\n",
    "\n",
    "        if collection_name == \"Gray\" :\n",
    "            collection_name = f\"Gray_{language}\"\n",
    "        \n",
    "        # 하이브리드 검색기 초기화 - ChromaDB 지원 버전\n",
    "        self.searcher = HybridSearcher(\n",
    "            embedding_model=embedding_loader,\n",
    "            chunk_size=200, \n",
    "            chunk_overlap=50,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        self.loader = generation_loader\n",
    "        self.prompt_path_ko = prompt_path_ko\n",
    "        self.prompt_path_en = prompt_path_en\n",
    "\n",
    "        if language==\"ko\" :\n",
    "            self.prompt_template = self._load_prompt(prompt_path_ko)\n",
    "        else :\n",
    "            self.prompt_template = self._load_prompt(prompt_path_en)\n",
    "        \n",
    "        # 카테고리별 파일 매핑\n",
    "        if language == \"en\" :\n",
    "            self.category_to_file = {\n",
    "                \"16\": \"GrayAnatomy_Formatted.md\"\n",
    "            }\n",
    "        else :\n",
    "            self.category_to_file = {\n",
    "                \"16\": \"full.txt\"\n",
    "            }\n",
    "        \n",
    "        try :\n",
    "            self.translate_en_to_ko = en_to_ko\n",
    "        except ImportError :\n",
    "            print(\"translate.py 모듈을 임포트할 수 없습니다.\")\n",
    "            self.translate_en_to_ko = en_to_ko\n",
    "\n",
    "        # 이미 처리된 파일 추적\n",
    "        self.loaded_files = set()\n",
    "        \n",
    "        # 전체 파일을 벡터 DB에 미리 로드할지 여부\n",
    "        self.preload_all = False\n",
    "        \n",
    "        # 디버그 모드\n",
    "        self.debug_mode = False\n",
    "        \n",
    "    def preload_documents(self):\n",
    "        \"\"\"\n",
    "        모든 카테고리의 파일을 미리 벡터 DB에 로드\n",
    "        \"\"\"\n",
    "        print(\"모든 문서를 벡터 DB에 로드 중...\")\n",
    "        for category, filename in self.category_to_file.items():\n",
    "            file_path = os.path.join(self.data_folder, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"카테고리 {category}: {filename} 로드 중...\")\n",
    "                self._load_document(file_path)\n",
    "                self.loaded_files.add(file_path)\n",
    "            else:\n",
    "                print(f\"[경고] 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        \n",
    "        print(f\"총 {len(self.loaded_files)}개 파일이 벡터 DB에 로드되었습니다.\")\n",
    "        self.preload_all = True\n",
    "\n",
    "    def _load_document(self, file_path):\n",
    "        \"\"\"\n",
    "        문서를 로드하여 검색기에 추가\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): 로드할 파일 경로\n",
    "            \n",
    "        Returns:\n",
    "            int: 로드된 청크 수\n",
    "        \"\"\"\n",
    "        # 이미 로드된 파일이면 건너뛰기\n",
    "        if file_path in self.loaded_files and self.preload_all:\n",
    "            if self.debug_mode:\n",
    "                print(f\"이미 로드된 파일입니다: {file_path}\")\n",
    "            return 0\n",
    "        \n",
    "        # 파일 로드 및 벡터 DB에 추가\n",
    "        num_chunks = self.searcher.load_document(file_path)\n",
    "        \n",
    "        # 파일 추적 목록에 추가\n",
    "        self.loaded_files.add(file_path)\n",
    "        \n",
    "        if self.debug_mode:\n",
    "            print(f\"파일 로드 완료: {file_path} ({num_chunks}개 청크)\")\n",
    "        \n",
    "        return num_chunks\n",
    "\n",
    "    def filter_relevant_content(self, query, search_results, threshold=0.7, top_k=5):\n",
    "        \"\"\"\n",
    "        LLM을 사용하여 검색 결과에서 질문과 관련이 있는 내용을 평가하고 \n",
    "        관련성 점수가 높은 상위 k개 결과만 반환\n",
    "        \n",
    "        Args:\n",
    "            query (str): 사용자 질의\n",
    "            search_results (list): 검색 결과 목록\n",
    "            threshold (float): 관련성 점수 임계값 (0.0~1.0)\n",
    "            top_k (int): 반환할 상위 결과 개수\n",
    "            \n",
    "        Returns:\n",
    "            list: 점수가 높은 상위 k개의 필터링된 관련 정보 목록\n",
    "        \"\"\"\n",
    "        scored_results = []\n",
    "        \n",
    "        if self.debug_mode:\n",
    "            print(f\"검색 결과 {len(search_results)}개에 대한 관련성 필터링 시작\")\n",
    "        \n",
    "        # 각 검색 결과에 대해 관련성 평가\n",
    "        for result in search_results:\n",
    "            # 프롬프트 구성 - 관련성 평가용\n",
    "            relevance_prompt = f\"\"\"\n",
    "    다음은 사용자의 질문입니다:\n",
    "    \"{query}\"\n",
    "    다음은 검색된 텍스트 정보입니다:\n",
    "    \"{result['chunk']}\"\n",
    "    위 텍스트가 사용자 질문에 얼마나 관련이 있는지 평가해주세요.\n",
    "    평가는 다음과 같이 응답해주세요:\n",
    "    1. 관련성 점수: 0.0 ~ 1.0 사이의 숫자 (1.0이 가장 관련성 높음)\n",
    "    2. 이유: 관련성이 높거나 낮은 이유를 간략하게 설명\n",
    "    응답 형식:\n",
    "    {{\n",
    "    \"score\": 0.0~1.0,\n",
    "    \"reason\": \"평가 이유\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "            \n",
    "            relevance_result = self.loader.generate(relevance_prompt)\n",
    "            \n",
    "            try:\n",
    "                # JSON 응답 파싱 (정확한 JSON이 아닐 수 있으므로 예외 처리)\n",
    "                import re\n",
    "                import json\n",
    "                \n",
    "                # JSON 형식 추출 시도\n",
    "                json_match = re.search(r'\\{.*\"score\".*:.*,.*\"reason\".*:.*\\}', relevance_result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    relevance_data = json.loads(json_match.group(0))\n",
    "                    score = float(relevance_data.get(\"score\", 0))\n",
    "                else:\n",
    "                    # 숫자만 추출 시도\n",
    "                    score_match = re.search(r'score\"?\\s*:?\\s*(\\d+\\.\\d+|\\d+)', relevance_result)\n",
    "                    score = float(score_match.group(1)) if score_match else 0.0\n",
    "                \n",
    "                # 점수와 함께 결과 저장 (임계값 이상인 것만)\n",
    "                if score >= threshold:\n",
    "                    if self.debug_mode:\n",
    "                        print(f\"관련성 높음 (점수: {score:.2f}): {result['page']}페이지\")\n",
    "                    # 원본 결과에 점수 정보 추가\n",
    "                    result_with_score = result.copy()\n",
    "                    result_with_score['relevance_score'] = score\n",
    "                    scored_results.append(result_with_score)\n",
    "                else:\n",
    "                    if self.debug_mode:\n",
    "                        print(f\"관련성 낮음 (점수: {score:.2f}): {result['page']}페이지\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # 파싱 실패 시 안전을 위해 낮은 점수로 포함\n",
    "                if self.debug_mode:\n",
    "                    print(f\"관련성 평가 파싱 오류: {str(e)}\")\n",
    "                result_with_score = result.copy()\n",
    "                result_with_score['relevance_score'] = 0.1  # 낮은 기본 점수\n",
    "                scored_results.append(result_with_score)\n",
    "        \n",
    "        # 점수를 기준으로 내림차순 정렬\n",
    "        sorted_results = sorted(scored_results, key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과만 선택\n",
    "        top_results = sorted_results[:top_k]\n",
    "        \n",
    "        if self.debug_mode:\n",
    "            print(f\"필터링 결과: 총 {len(search_results)}개 중 {len(scored_results)}개가 임계값 통과\")\n",
    "            print(f\"상위 {min(top_k, len(top_results))}개 결과만 선택됨\")\n",
    "            for i, res in enumerate(top_results):\n",
    "                print(f\"  {i+1}위: 점수 {res['relevance_score']:.2f} - {res['page']}페이지\")\n",
    "        \n",
    "        # 결과가 없는 경우 원본의 일부라도 반환\n",
    "        if not top_results and search_results:\n",
    "            if self.debug_mode:\n",
    "                print(f\"필터링 결과가 없어 원본 상위 결과 {min(top_k, len(search_results))}개 포함\")\n",
    "            top_results = search_results[:min(top_k, len(search_results))]\n",
    "        \n",
    "        return top_results\n",
    "    \n",
    "    def _format_filtered_results(self, filtered_results):\n",
    "        \"\"\"\n",
    "        필터링된 결과를 문자열로 변환\n",
    "        \n",
    "        Args:\n",
    "            filtered_results (list): 필터링된 검색 결과 리스트\n",
    "            \n",
    "        Returns:\n",
    "            str: 포맷된 문자열\n",
    "        \"\"\"\n",
    "        result_str = \"\"\n",
    "        \n",
    "        for item in filtered_results:\n",
    "            result_str += f\"# 질문: {item['sub_question']}\\n\\n\"\n",
    "            \n",
    "            # results가 이미 점수로 정렬되었다고 가정\n",
    "            for res in item['results']:\n",
    "                page_info = res['page']\n",
    "                score_info = \"\"\n",
    "                if 'relevance_score' in res:\n",
    "                    score_info = f\" (관련성: {res['relevance_score']:.2f})\"\n",
    "                \n",
    "                result_str += f\"## {page_info}페이지{score_info}\\n\"\n",
    "                result_str += f\"{res['chunk']}\\n\\n\"\n",
    "        \n",
    "        return result_str\n",
    "\n",
    "    def generate_response(self, query, category, top_n=5, alpha=0.5, target_language=None):\n",
    "        \"\"\"\n",
    "        질의에 대한 응답 생성\n",
    "        \n",
    "        Args:\n",
    "            query (str): 사용자 질의\n",
    "            category (str): 검색할 카테고리\n",
    "            top_n (int): 검색 결과 수\n",
    "            alpha (float): BM25와 벡터 검색 가중치 (높을수록 BM25 중시)\n",
    "            \n",
    "        Returns:\n",
    "            dict: 생성된 응답 및 메타데이터\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response_language = target_language if target_language else self.language\n",
    "            source_language = self.language\n",
    "\n",
    "            # 파일 로드 확인\n",
    "            filename = self.category_to_file[category]\n",
    "            file_path = os.path.join(self.data_folder, filename)\n",
    "            \n",
    "            # 해당 파일이 존재하지 않는 경우\n",
    "            if not os.path.exists(file_path):\n",
    "                error_msg = f\"[오류] 파일을 찾을 수 없습니다: {file_path}\"\n",
    "                print(error_msg)\n",
    "                return {\"답변\": error_msg, \"후처리\": error_msg}\n",
    "            \n",
    "            # 파일 로드 (이미 로드된 경우 건너뜀)\n",
    "            if not self.preload_all:\n",
    "                self._load_document(file_path)\n",
    "            \n",
    "            # 단계 1: 질문 복잡성 판단\n",
    "            # is_complex = self.searcher.is_complex_question(query)\n",
    "            \n",
    "            # if self.debug_mode:\n",
    "            #     print(f\"질문 복잡성 분석: {'복합적인 질문' if is_complex else '단순 질문'}\")\n",
    "            \n",
    "            # if is_complex:\n",
    "            #     # 복합적인 질문 처리\n",
    "            #     # 단계 2: 질문 분해\n",
    "            #     sub_questions = self.searcher.decompose_question(query)\n",
    "            #     print(f\"하위 질문 분해 결과: {sub_questions}\")\n",
    "                \n",
    "            #     # 모든 하위 질문에 대한 검색 결과를 저장할 리스트\n",
    "            #     all_search_results = []\n",
    "            #     all_filtered_results = []\n",
    "\n",
    "            #     # 단계 3: 각 하위 질문에 대해 검색 수행\n",
    "            #     for i, sub_q in enumerate(sub_questions):\n",
    "            #         if self.debug_mode:\n",
    "            #             print(f\"하위 질문 {i+1} 검색 중: '{sub_q}'\")\n",
    "                    \n",
    "            #         if not isinstance(query, str) :\n",
    "            #             query = str(query)\n",
    "            #         # 하위 질문에 대한 검색 수행\n",
    "            #         search_results = self.searcher.search(\n",
    "            #             query=sub_q, \n",
    "            #             top_n=top_n, \n",
    "            #             alpha=alpha,\n",
    "            #             use_vector_db=self.use_vector_db\n",
    "            #         )\n",
    "            #         filtered_results = self.filter_relevant_content(query, search_results)\n",
    "\n",
    "            #         # 검색 결과 저장\n",
    "            #         all_search_results.append({\n",
    "            #             \"sub_question\": sub_q,\n",
    "            #             \"results\": search_results\n",
    "            #         })\n",
    "\n",
    "            #         all_filtered_results.append({\n",
    "            #             \"sub_question\" : sub_q,\n",
    "            #             \"results\" : filtered_results\n",
    "            #         })\n",
    "\n",
    "            #     # 컨텍스트 구성 - 모든 하위 질문 결과 통합\n",
    "            #     context = \"\"\n",
    "            #     for i, sr in enumerate(all_filtered_results):\n",
    "            #         context += f\"\\n### 하위 질문 {i+1}: {sr['sub_question']}\\n\"\n",
    "                    \n",
    "            #         for _, res in enumerate(sr[\"results\"]):\n",
    "            #             context += f\"#### Page {res['page']}\\n\"\n",
    "            #             context += f\"{res['chunk']}\\n\\n\"\n",
    "                \n",
    "            #     # 프롬프트 구성 - 복합 질문용\n",
    "            #     prompt = self.prompt_template.format(context=context, query=query)\n",
    "                \n",
    "            # else:\n",
    "            #     # 단순 질문 처리 - 기존 방식대로 검색\n",
    "            #     search_results = self.searcher.search(\n",
    "            #         query=query, \n",
    "            #         top_n=top_n, \n",
    "            #         alpha=alpha,\n",
    "            #         use_vector_db=self.use_vector_db\n",
    "            #     )\n",
    "                \n",
    "            #     filtered_results = self.filter_relevant_content(query, search_results)\n",
    "\n",
    "            #     # 컨텍스트 구성 - 단순 질문용\n",
    "            #     context = \"\\n\\n\".join([f\"#### Page {result['page']}\\n{result['chunk']}\" for result in filtered_results])\n",
    "                \n",
    "            #     # 프롬프트 구성 - 단순 질문용\n",
    "            #     prompt = self.prompt_template.format(context=context, query=query)\n",
    "            \n",
    "\n",
    "            search_results = self.searcher.search(\n",
    "                query=query, \n",
    "                top_n=top_n, \n",
    "                alpha=alpha,\n",
    "                use_vector_db=self.use_vector_db\n",
    "            )\n",
    "            \n",
    "            # filtered_results = self.filter_relevant_content(query, search_results)\n",
    "\n",
    "            # 컨텍스트 구성 - 단순 질문용\n",
    "            # context = \"\\n\\n\".join([f\"#### Page {result['page']}\\n{result['chunk']}\" for result in filtered_results])\n",
    "            context = \"\\n\\n\".join([f\"#### Page {result['page']}\\n{result['chunk']}\" for result in search_results])\n",
    "            # 프롬프트 구성 - 단순 질문용\n",
    "            prompt = self.prompt_template.format(context=context, query=query)\n",
    "\n",
    "            # 응답 생성 파트\n",
    "            if hasattr(self.loader, \"tokenizer\"):\n",
    "                # Huggingface 모델 사용\n",
    "                tokenizer = self.loader.tokenizer\n",
    "                model = self.loader.model\n",
    "                \n",
    "                # 인풋 토크나이즈\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "                \n",
    "                # 텍스트 생성\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=400,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=False,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    num_beams=3,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                generated_ids = output[0][input_ids.shape[-1]:]\n",
    "                raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            else:\n",
    "                # OpenAI 등 API 기반 모델 사용 (Ollama 포함)\n",
    "                raw_answer = self.loader.generate(prompt)\n",
    "            \n",
    "            # 응답 후처리\n",
    "            final_answer = self._extract_answer_content(raw_answer)\n",
    "            final_answer = self._remove_chinese_characters(final_answer)\n",
    "            \n",
    "            translated_answer = None\n",
    "            if source_language == \"en\" and response_language == \"ko\" :\n",
    "                if self.translate_en_to_ko :\n",
    "                    try :\n",
    "                        translated_answer = self.translate_en_to_ko(final_answer)\n",
    "                        if self.debug_mode :\n",
    "                            print(\"영어 -> 한국어 번역 완료\")\n",
    "                    except Exception as e :\n",
    "                        print(f\"번역 중 오류 발생 : {e}\")\n",
    "                        translated_answer = f\"[번역 오류] {final_answer}\"\n",
    "                else :\n",
    "                    translated_answer = f\"[번역 모듈 없음] {final_answer}\"\n",
    "            # 결과 준비\n",
    "            response = {\n",
    "                \"답변\": raw_answer,\n",
    "                \"후처리\": final_answer,\n",
    "                \"문서 일부\": context,\n",
    "                \"question_en\": query,\n",
    "                \"answer_en\": raw_answer,\n",
    "                # \"is_complex\": is_complex,\n",
    "                # \"필터링_결과\": all_filtered_results\n",
    "            }\n",
    "            \n",
    "            if translated_answer :\n",
    "                response[\"번역된 답변\"] = translated_answer\n",
    "                \n",
    "            # 복합 질문인 경우 하위 질문 정보 추가\n",
    "            # if is_complex:\n",
    "            #     response[\"sub_questions\"] = sub_questions\n",
    "            #     response[\"sub_search_results\"] = all_search_results\n",
    "            #     response[\"filtered_search_results\"] = all_filtered_results\n",
    "            # else:\n",
    "            #     response[\"검색_결과\"] = search_results\n",
    "            #     response[\"필터링_결과\"] = all_filtered_results\n",
    "            \n",
    "            # 결과 저장\n",
    "            try:\n",
    "                # result_path가 없으면 상위 스코프나 기본값으로 설정\n",
    "                result_path = getattr(self, 'result_path', '../result')\n",
    "                \n",
    "                # 알파값으로 폴더 경로 생성\n",
    "                # alpha_str = f\"{alpha:.1f}\"\n",
    "                # db_type = \"chromadb\" if self.use_vector_db else \"memory\"\n",
    "                # question_type = \"complex\" if is_complex else \"simple\"\n",
    "                # result_path = os.path.join(result_path, f\"{db_type}_{question_type}_alpha_{alpha_str}\")\n",
    "                \n",
    "                # 폴더가 없으면 생성\n",
    "                os.makedirs(result_path, exist_ok=True)\n",
    "                \n",
    "                filtered_results_str = \"\"\n",
    "                filtered_results_str = self._format_filtered_results(response[\"필터링_결과\"])\n",
    "\n",
    "                # 파일 저장\n",
    "                if self.debug_mode:\n",
    "                    print(f\"저장 시도: {result_path}\")\n",
    "                save_response_to_file(\n",
    "                    query=query,\n",
    "                    answer=response[\"답변\"],\n",
    "                    final_answer=response[\"번역된 답변\"],\n",
    "                    context=response[\"문서 일부\"],\n",
    "                    folder=result_path,\n",
    "                    filtered_results=filtered_results_str\n",
    "                )\n",
    "                if self.debug_mode:\n",
    "                    print(f\"저장 완료: {result_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"결과 저장 중 오류 발생: {e}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"[생성 오류] {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\"답변\": error_msg, \"후처리\": error_msg}\n",
    "    \n",
    "    def _load_prompt(self, path):\n",
    "        \"\"\"프롬프트 템플릿 로드\"\"\"\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 프롬프트 로드 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    \n",
    "    def _extract_answer_content(self, text):\n",
    "        \"\"\"응답에서 답변 부분만 추출\"\"\"\n",
    "        pattern = r\"(?:<\\|?|<|)?\\|?answer\\|?(?:\\|?>|>)?(.*?)(?:<\\|?|<|)?\\|?endanswer\\|?(?:\\|?>|>)?\"\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(1).strip() if match else text.strip()\n",
    "    \n",
    "    def _remove_chinese_characters(self, text):\n",
    "        \"\"\"중국어 문자 제거\"\"\"\n",
    "        return re.sub(r'[\\u4E00-\\u9FFF]', '', text)\n",
    "    \n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"벡터 DB 컬렉션 통계 조회\"\"\"\n",
    "        if hasattr(self.searcher, 'vector_db') and hasattr(self.searcher.vector_db, 'get_collection_stats'):\n",
    "            return self.searcher.vector_db.get_collection_stats()\n",
    "        return {\"error\": \"벡터 DB 통계를 조회할 수 없습니다.\"}\n",
    "    \n",
    "    def toggle_vector_db(self, use_vector_db=None):\n",
    "        \"\"\"벡터 DB 사용 여부 전환\"\"\"\n",
    "        if use_vector_db is not None:\n",
    "            self.use_vector_db = use_vector_db\n",
    "        else:\n",
    "            self.use_vector_db = not self.use_vector_db\n",
    "        \n",
    "        print(f\"벡터 DB 사용 여부: {self.use_vector_db}\")\n",
    "        return self.use_vector_db\n",
    "    \n",
    "    def set_debug_mode(self, debug_mode=True):\n",
    "        \"\"\"디버그 모드 설정\"\"\"\n",
    "        self.debug_mode = debug_mode\n",
    "        return self.debug_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5de4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "class ChromaVectorDB:\n",
    "    \"\"\"ChromaDB를 이용한 벡터 DB 관리 클래스\"\"\"\n",
    "    \n",
    "    # def __init__(self, embedding_model, persist_directory=\"./chroma_db\", collection_name=\"Gray\"):\n",
    "    #     \"\"\"\n",
    "    #     ChromaDB 초기화\n",
    "        \n",
    "    #     Args:\n",
    "    #         embedding_model: 임베딩 모델 (SentenceTransformer 또는 호환 래퍼)\n",
    "    #         persist_directory (str): 벡터 DB가 저장될 디렉토리 경로\n",
    "    #         collection_name (str): 컬렉션 이름\n",
    "    #     \"\"\"\n",
    "    #     # 디렉토리 생성 확인\n",
    "    #     os.makedirs(persist_directory, exist_ok=True)\n",
    "        \n",
    "    #     # 임베딩 모델 저장\n",
    "    #     self.embedding_model = embedding_model\n",
    "        \n",
    "    #     # ChromaDB 클라이언트 설정\n",
    "    #     self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "    #     # 사용자 정의 임베딩 함수 생성 - 기존 embedding_model을 활용\n",
    "    #     self.embedding_function = CustomEmbeddingFunction(self.embedding_model)\n",
    "        \n",
    "    #     # 컬렉션 생성 또는 가져오기\n",
    "    #     try:\n",
    "    #         self.collection = self.client.get_collection(\n",
    "    #             name=collection_name,\n",
    "    #             embedding_function=self.embedding_function\n",
    "    #         )\n",
    "    #         print(f\"기존 컬렉션 '{collection_name}'을 로드했습니다.\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"컬렉션 로드 중 오류 발생: {e}\")\n",
    "    #         print(f\"새 컬렉션 '{collection_name}'을 생성합니다.\")\n",
    "    #         self.collection = self.client.create_collection(\n",
    "    #             name=collection_name,\n",
    "    #             embedding_function=self.embedding_function\n",
    "    #         )\n",
    "    def __init__(self, embedding_model, persist_directory=\"./chroma_db\", collection_name=\"Gray\"):\n",
    "        \"\"\"\n",
    "        ChromaDB 초기화\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: 임베딩 모델 (SentenceTransformer 또는 호환 래퍼)\n",
    "            persist_directory (str): 벡터 DB가 저장될 디렉토리 경로\n",
    "            collection_name (str): 컬렉션 이름\n",
    "        \"\"\"\n",
    "        # 디렉토리 생성 확인\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        \n",
    "        # 임베딩 모델 저장\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # 기존 데이터베이스 디렉토리 확인 및 처리\n",
    "        db_path = pathlib.Path(persist_directory)\n",
    "        db_exists = db_path.exists() and any(db_path.iterdir())\n",
    "        \n",
    "        # 사용자 정의 임베딩 함수 생성 - 기존 embedding_model을 활용\n",
    "        self.embedding_function = CustomEmbeddingFunction(self.embedding_model)\n",
    "        \n",
    "        # 만약 데이터베이스가 손상되었거나 호환성 문제가 있다면 다시 생성\n",
    "        try:\n",
    "            self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "            \n",
    "            # 컬렉션 가져오기\n",
    "            try:\n",
    "                self.collection = self.client.get_collection(\n",
    "                    name=collection_name,\n",
    "                    embedding_function=self.embedding_function\n",
    "                )\n",
    "                print(f\"기존 컬렉션 '{collection_name}'을 로드했습니다.\")\n",
    "            except Exception as e:\n",
    "                print(f\"컬렉션 로드 중 오류 발생: {e}\")\n",
    "                \n",
    "                # 오류가 'max_seq_id' 관련 문제인지 확인\n",
    "                if \"max_seq_id\" in str(e) or \"PersistentData\" in str(e):\n",
    "                    print(\"데이터베이스 구조 호환성 문제가 발견되었습니다. 컬렉션을 재생성합니다.\")\n",
    "                    \n",
    "                    # 기존 컬렉션 제거 시도\n",
    "                    try:\n",
    "                        self.client.delete_collection(name=collection_name)\n",
    "                        print(f\"기존 컬렉션 '{collection_name}'을 삭제했습니다.\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # 새 컬렉션 생성\n",
    "                    self.collection = self.client.create_collection(\n",
    "                        name=collection_name,\n",
    "                        embedding_function=self.embedding_function\n",
    "                    )\n",
    "                    print(f\"새 컬렉션 '{collection_name}'을 생성했습니다.\")\n",
    "                else:\n",
    "                    # 다른 종류의 오류면 새 컬렉션 생성\n",
    "                    print(f\"새 컬렉션 '{collection_name}'을 생성합니다.\")\n",
    "                    self.collection = self.client.create_collection(\n",
    "                        name=collection_name,\n",
    "                        embedding_function=self.embedding_function\n",
    "                    )\n",
    "                    \n",
    "        except Exception as outer_e:\n",
    "            print(f\"치명적 오류: ChromaDB 클라이언트 초기화 실패: {outer_e}\")\n",
    "            print(\"백업 솔루션: 기존 데이터베이스를 재설정합니다.\")\n",
    "            \n",
    "            # 데이터베이스 디렉토리가 존재하면 백업 후 재생성\n",
    "            if db_exists:\n",
    "                # 기존 폴더 백업 (이름 변경)\n",
    "                import shutil\n",
    "                import time\n",
    "                \n",
    "                backup_dir = f\"{persist_directory}_backup_{int(time.time())}\"\n",
    "                try:\n",
    "                    shutil.move(persist_directory, backup_dir)\n",
    "                    print(f\"기존 데이터베이스를 {backup_dir}로 백업했습니다.\")\n",
    "                except Exception as move_err:\n",
    "                    print(f\"백업 실패: {move_err}\")\n",
    "                    # 기존 폴더 삭제 시도\n",
    "                    try:\n",
    "                        shutil.rmtree(persist_directory)\n",
    "                        print(f\"기존 데이터베이스 폴더를 삭제했습니다.\")\n",
    "                    except Exception as rm_err:\n",
    "                        print(f\"폴더 삭제 실패: {rm_err}\")\n",
    "            \n",
    "            # 폴더 재생성\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            # 클라이언트와 컬렉션 새로 생성\n",
    "            self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "            print(f\"ChromaDB를 재설정하고 새 컬렉션 '{collection_name}'을 생성했습니다.\")\n",
    "    \n",
    "    def add_documents(self, documents, metadatas=None, ids=None):\n",
    "        \"\"\"\n",
    "        문서를 벡터 DB에 추가\n",
    "        \n",
    "        Args:\n",
    "            documents (list): 문서 텍스트 리스트\n",
    "            metadatas (list, optional): 각 문서에 대한 메타데이터 리스트\n",
    "            ids (list, optional): 각 문서에 대한 고유 ID 리스트\n",
    "        \n",
    "        Returns:\n",
    "            int: 추가된 문서 수\n",
    "        \"\"\"\n",
    "        if ids is None:\n",
    "            # 고유한 ID 생성 (timestamp + index)\n",
    "            import time\n",
    "            timestamp = int(time.time())\n",
    "            ids = [f\"doc_{timestamp}_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        # 메타데이터가 제공되지 않은 경우 빈 딕셔너리 생성\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        # 문서 임베딩 미리 계산 - 디버깅 용도\n",
    "        # embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # 문서 추가 (임베딩은 임베딩 함수가 자동 계산)\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        return len(documents)\n",
    "    \n",
    "    def load_document_file(self, file_path, chunk_size=None):\n",
    "        \"\"\"\n",
    "        파일을 로드하여 벡터 DB에 저장\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): 로드할 파일 경로\n",
    "            chunk_size (int, optional): 청크 분할 크기 (None이면 페이지 단위로 분할)\n",
    "            \n",
    "        Returns:\n",
    "            int: 추가된 청크 수\n",
    "        \"\"\"\n",
    "        # 파일 확장자 확인\n",
    "        # _, ext = os.path.splitext(file_path)\n",
    "        \n",
    "        # 파일 읽기\n",
    "        try:\n",
    "            print(f\"파일 로드 중: {file_path}\")\n",
    "            text_content = pathlib.Path(file_path).read_text(encoding=\"utf-8\")\n",
    "            print(f\"파일 크기: {len(text_content)} 문자\")\n",
    "        except Exception as e:\n",
    "            print(f\"파일 로드 오류: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        # 청크 분할 방식 결정\n",
    "        chunks = []\n",
    "        metadatas = []\n",
    "        \n",
    "        if chunk_size:\n",
    "            # 지정된 청크 크기로 분할\n",
    "            current_pos = 0\n",
    "            while current_pos < len(text_content):\n",
    "                chunk = text_content[current_pos:current_pos + chunk_size]\n",
    "                chunks.append(chunk)\n",
    "                metadatas.append({\"source\": file_path, \"chunk_index\": len(chunks)})\n",
    "                current_pos += chunk_size\n",
    "            \n",
    "            print(f\"크기 기반 분할: {len(chunks)}개 청크 생성\")\n",
    "        else:\n",
    "            # 페이지 단위 분할 (#### Page X 패턴 사용)\n",
    "            page_pattern = r'(####\\s+Page\\s+\\d+\\s*\\n(?:[\\s\\S]*?)(?=####\\s+Page\\s+\\d+\\s*\\n|$))'\n",
    "            page_chunks = re.findall(page_pattern, text_content)\n",
    "            \n",
    "            if page_chunks:\n",
    "                # 페이지 패턴 찾음\n",
    "                for page_chunk in page_chunks:\n",
    "                    # 페이지 번호 추출\n",
    "                    page_match = re.match(r'####\\s+Page\\s+(\\d+)', page_chunk)\n",
    "                    page_num = page_match.group(1) if page_match else \"unknown\"\n",
    "                    \n",
    "                    chunks.append(page_chunk)\n",
    "                    metadatas.append({\"source\": file_path, \"page\": f\"Page {page_num}\"})\n",
    "                \n",
    "                print(f\"페이지 기반 분할: {len(chunks)}개 페이지 찾음\")\n",
    "            else:\n",
    "                # 페이지 패턴 못찾음 - 문단 단위로 분할\n",
    "                paragraphs = re.split(r'\\n\\s*\\n', text_content)\n",
    "                paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "                \n",
    "                for i, para in enumerate(paragraphs):\n",
    "                    chunks.append(para)\n",
    "                    metadatas.append({\"source\": file_path, \"paragraph\": i+1})\n",
    "                \n",
    "                print(f\"문단 기반 분할: {len(chunks)}개 문단 생성\")\n",
    "        \n",
    "        # 청크가 없으면 전체 텍스트를 하나의 청크로 처리\n",
    "        if not chunks:\n",
    "            chunks = [text_content]\n",
    "            metadatas = [{\"source\": file_path, \"full_document\": True}]\n",
    "            print(\"분할 실패: 전체 텍스트를 하나의 청크로 처리\")\n",
    "        \n",
    "        # 벡터 DB에 추가\n",
    "        added_count = self.add_documents(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        print(f\"벡터 DB에 {added_count}개 청크 추가 완료\")\n",
    "        return added_count\n",
    "    \n",
    "    def search(self, query, top_n=5, metadata_filter=None):\n",
    "        \"\"\"\n",
    "        벡터 유사도 기반 검색 수행\n",
    "        \n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            top_n (int): 반환할 결과 수\n",
    "            metadata_filter (dict, optional): 메타데이터 필터링 조건\n",
    "        \n",
    "        Returns:\n",
    "            dict: 검색 결과\n",
    "        \"\"\"\n",
    "        # 진단용 - 쿼리 임베딩 계산\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        print(f\"쿼리 임베딩 shape: {query_embedding.shape}\")\n",
    "        \n",
    "        # ChromaDB를 통한 검색\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_n,\n",
    "            where=metadata_filter,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"]\n",
    "        )\n",
    "        \n",
    "        # 결과 포맷팅\n",
    "        formatted_results = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            formatted_results.append({\n",
    "                \"chunk\": results[\"documents\"][0][i],\n",
    "                \"id\": results[\"ids\"][0][i],\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"distance\": results[\"distances\"][0][i] if \"distances\" in results else None,\n",
    "                # \"embedding\": results[\"embeddings\"][0][i] if \"embeddings\" in results else None\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    # def get_all_document_embeddings(self, ids=None):\n",
    "    #     \"\"\"\n",
    "    #     저장된 모든 문서의 임베딩 조회 (디버깅 용도)\n",
    "    #     \"\"\"\n",
    "    #     result = self.collection.get(ids=ids, include=[\"embeddings\", \"documents\"])\n",
    "    #     return result\n",
    "    \n",
    "    # def get_document_by_id(self, doc_id):\n",
    "    #     \"\"\"ID로 문서 조회\"\"\"\n",
    "    #     result = self.collection.get(ids=[doc_id])\n",
    "    #     if result[\"documents\"]:\n",
    "    #         return {\n",
    "    #             \"chunk\": result[\"documents\"][0],\n",
    "    #             \"metadata\": result[\"metadatas\"][0]\n",
    "    #         }\n",
    "    #     return None\n",
    "    \n",
    "    # def delete_document(self, doc_id):\n",
    "    #     \"\"\"ID로 문서 삭제\"\"\"\n",
    "    #     self.collection.delete(ids=[doc_id])\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        \"\"\"컬렉션 삭제\"\"\"\n",
    "        self.client.delete_collection(self.collection.name)\n",
    "    \n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"컬렉션 통계 조회\"\"\"\n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            \"document_count\": count,\n",
    "            \"collection_name\": self.collection.name\n",
    "        }\n",
    "    \n",
    "class CustomEmbeddingFunction(embedding_functions.EmbeddingFunction):\n",
    "    \"\"\"기존 임베딩 모델을 ChromaDB에서 사용하기 위한 래퍼 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: 임베딩 모델 (encode 메서드 제공)\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        텍스트를 임베딩 벡터로 변환\n",
    "        \n",
    "        Args:\n",
    "            texts: 인코딩할 텍스트 리스트\n",
    "        \n",
    "        Returns:\n",
    "            임베딩 벡터 리스트\n",
    "        \"\"\"\n",
    "        return self.embedding_model.encode(texts).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0acc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_db = ChromaVectorDB(\n",
    "#     embedding_model=embedding_loader,\n",
    "#     persist_directory=\"./chroma_db\",\n",
    "#     collection_name=\"Gray_en\"\n",
    "# )\n",
    "# vector_db.delete_collection()\n",
    "# print(\"기존 컬렉션 삭제 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffced37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader.config import *\n",
    "from chroma_db import *\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "questions = [\n",
    "    \"How do the processes of ossification differ between the skull and the vertebral column, and what are the developmental implications of these differences?\",\n",
    "    \"Compare and contrast the ovum and spermatozoon in terms of their development, maturation process, and contribution to fertilization, explaining how their specialized features facilitate their respective roles.\",\n",
    "    \"How does the structure and development of the vertebral column demonstrate both functional adaptation and evolutionary history, and what role does the notochord play in these processes?\",\n",
    "    \"How do the various stages of embryonic implantation, placental development, and fetal membrane formation collectively establish the maternal-fetal interface, and what are the critical structural and functional relationships that develop during this process?\",\n",
    "    \"What are the primary and secondary curves of the vertebral column, how do they develop chronologically, and what functional advantages do they provide for bipedal locomotion?\",\n",
    "    \"How does the skull's composition of dermal and cartilaginous bones reflect both its developmental origins and its functional requirements, and what are the key differences in the ossification patterns of these two bone types?\",\n",
    "    \"Explain the process of fertilization and early embryonic development, describing how the male and female pronuclei interact to form the segmentation nucleus, and how this leads to the formation of the blastocyst and subsequent implantation.\",\n",
    "    \"How does the development of the thoracic cage from its embryonic origins to its adult form demonstrate both functional adaptation and regional specialization, and what are the key differences between true and false ribs in terms of their structure and functions?\",\n",
    "    \"Compare and contrast the development and structural features of the cervical, thoracic, and lumbar vertebrae, explaining how their specialized characteristics relate to their differing functional roles.\",\n",
    "    \"How does the structure of the placenta facilitate its multiple physiological functions while maintaining separation between maternal and fetal circulations, and what developmental stages are critical to establishing this complex organ?\",\n",
    "    \"How do the primitive embryonic germ layers (ectoderm, mesoderm, and endoderm) contribute to the formation of the skeletal, nervous, and digestive systems, and what key developmental interactions occur between these systems?\",\n",
    "    \"Compare the processes of intramembranous and intracartilaginous ossification, detailing their anatomical sites, cellular mechanisms, and developmental timelines, and explain how these different processes create the various bones of the skeletal system.\",\n",
    "    \"Describe the embryonic development of the branchial region and explain how its derivatives contribute to adult craniofacial structures, detailing the specific fates of each branchial arch and the developmental abnormalities that can occur when this process is disrupted.\",\n",
    "    \"What is the sequence of events that occurs during the maturation of an ovum, and how does this process differ from spermatogenesis in terms of cellular divisions, chromosome reduction, and the number and viability of resulting cells?\",\n",
    "    \"Describe the development of the limbs from their initial appearance as limb buds to their final form, explaining the molecular signals, tissue interactions, and morphogenetic movements involved in this process, and how developmental abnormalities can lead to congenital limb defects.\",\n",
    "    \"Compare the structure and ossification patterns of the vertebral column, sternum, and ribs, explaining how these components develop together to form the thoracic cage, and how variations in their development can lead to congenital abnormalities.\",\n",
    "    \"How does the skull's development from its embryonic origins demonstrate both segmentation patterns similar to the vertebral column and unique craniofacial adaptations, and what are the key differences in ossification between the cranial base, vault, and facial bones?\",\n",
    "    \"Describe the embryonic development of the body cavities, explaining how the coelom divides to form the pericardial, pleural, and peritoneal cavities, and how the diaphragm develops to separate the thoracic and abdominal compartments.\",\n",
    "    \"How do the neural tube and notochord interact during development to establish the central nervous system and axial skeleton, and what are the key signaling molecules and inductive interactions that guide this process?\",\n",
    "    \"Explain the processes of implantation and placentation, detailing the roles of the trophoblast, decidua, and extraembryonic membranes in establishing the maternal-fetal interface, and how abnormalities in these processes can lead to pregnancy complications.\",\n",
    "    \"Compare and contrast the three parts of the axillary artery in terms of their anatomical relations and clinical significance, explaining how these relationships might impact surgical approaches and potential complications during procedures in the axilla.\",\n",
    "    \"Describe the course and major branches of the brachial artery, explaining how its anatomical variations affect the collateral circulation after ligation of different segments of the artery, and discuss the clinical implications of these variations for traumatic injuries and surgical procedures in the upper limb.\",\n",
    "    \"Analyze the arterial supply of the hand, detailing the formation and clinical significance of the superficial and deep palmar arches, their anastomotic patterns, and how these dual systems provide redundancy that affects treatment strategies for hand injuries and vascular disorders.\",\n",
    "    \"Examine the pathway and variations of the radial artery from its origin to the formation of the deep palmar arch, discussing the surgical implications of these variations for procedures such as radial artery harvesting for coronary bypass grafting and the creation of arteriovenous fistulas for hemodialysis access.\",\n",
    "    \"Compare and contrast the anatomical course, relations, and branching patterns of the ulnar and radial arteries in the forearm and hand, analyzing how these differences impact the clinical assessment and management of vascular injuries to these vessels.\",\n",
    "    \"Analyze the anatomical basis of collateral circulation around the elbow joint following ligation or obstruction of the brachial artery, explaining how the network of anastomosing vessels ensures tissue viability and discussing the clinical implications of this vascular arrangement for traumatic injuries and surgical interventions.\",\n",
    "    \"Describe the anatomical boundaries and contents of the axilla, explaining how the neurovascular structures are arranged within this space and discussing the clinical implications of these relationships for procedures such as axillary lymph node dissection, brachial plexus blocks, and approaches to the shoulder joint.\",\n",
    "    \"Explain the origin, course, and branching pattern of the abdominal aorta, describing how its major visceral and parietal branches supply different organ systems, and analyzing the clinical significance of important vascular landmarks and anastomoses for surgical procedures and management of vascular diseases.\",\n",
    "    \"Compare the anatomical relationships and branching patterns of the axillary and brachial arteries, discussing how these relationships provide landmarks for surgical approaches, and analyzing the implications of anatomical variations for diagnostic procedures, treatment of traumatic injuries, and vascular access in these regions.\",\n",
    "    \"Describe the arterial supply to the rectum and anal canal, explaining how the superior, middle, and inferior rectal arteries form a complex anastomotic network, and analyze the clinical significance of this vascular arrangement for colorectal surgical procedures, management of hemorrhoids, and potential consequences of vascular compromise in this region.\",\n",
    "    \"Analyze the vascular supply of the hand through the radial and ulnar arteries, explaining how their terminal branches form the superficial and deep palmar arches, and discuss the clinical implications of these anastomotic patterns for hand injuries, surgical approaches, and the assessment of hand perfusion.\",\n",
    "    \"Describe the origin, course, and major branches of the internal iliac (hypogastric) artery, explaining how its anatomy differs in the fetus compared to the adult, and discuss the clinical significance of these vessels in pelvic surgery, trauma management, and vascular embolization procedures.\",\n",
    "    \"Explain the branching pattern and distribution of the celiac trunk, detailing how its three major divisions (left gastric, splenic, and hepatic arteries) supply the upper abdominal organs, and analyze the clinical significance of their anatomical variations, collateral pathways, and relationships to surrounding structures.\",\n",
    "    \"Compare and contrast the anatomical relationships and distribution patterns of the superior and inferior mesenteric arteries, discussing their embryological significance, anastomotic connections, and the clinical implications of these vascular arrangements for intestinal ischemia, surgical resections, and collateral circulation in occlusive disease.\",\n",
    "    \"Analyze the anatomical relationships and variations of the renal arteries, explaining their importance in surgical approaches to the kidney, renal transplantation, and interventional procedures, while also discussing the clinical significance of accessory renal vessels and their potential role in renovascular hypertension.\",\n",
    "    \"Describe the origin, course, and branches of the external iliac artery, explaining its relationships to surrounding structures and analyzing the clinical implications of these anatomical features for vascular access procedures, traumatic injuries, and surgical approaches to the lower abdomen and pelvis.\",\n",
    "    \"Compare and contrast the arterial supply of the male and female pelvic organs, describing how the internal iliac (hypogastric) artery and its branches distribute blood to these structures, and discuss the clinical significance of these vascular patterns for surgical procedures, hemorrhage control, and interventional radiological techniques.\",\n",
    "    \"Describe the anatomical course and branching pattern of the internal pudendal artery in males and females, explaining how this vessel supplies the perineum and external genitalia, and discuss the clinical significance of its relationships for urogenital and anorectal procedures, traumatic injuries, and erectile dysfunction.\",\n",
    "    \"Explain the anatomical relationships and branching pattern of the thoracic aorta, describing how its branches supply the thoracic walls and viscera, and discuss the clinical significance of these vascular patterns in thoracic surgery, trauma management, and the development of collateral circulation in cases of aortic coarctation.\",\n",
    "    \"Analyze the surgical approaches to the axillary artery and its branches, discussing the potential complications of these procedures, strategies for minimizing vascular injury, and the management of axillary artery trauma in relation to the surrounding neurovascular structures.\",\n",
    "    \"How does the female urethra compare to the male urethra in terms of structure, length, and coats? What specific differences in epithelial lining exist between them?\",\n",
    "    \"What is the developmental journey of the testes from fetal life to adulthood, and what abnormalities can occur during this process? How do these abnormalities affect fertility and health?\",\n",
    "    \"What are the key components of the spermatic cord, and how do they relate to the blood supply, lymphatic drainage, and innervation of the testes? What clinical implications might arise from this anatomical arrangement?\",\n",
    "    \"Describe the structure of the tunica vaginalis and its relationship to the peritoneum. How does it contribute to various testicular pathologies, especially different types of hydrocele, and what distinguishes these conditions from each other?\",\n",
    "    \"Compare and contrast the structure and function of the vesiculae seminales, prostate gland, and bulbourethral glands. How do their secretions contribute to the composition of semen, and what are the clinical implications of diseases affecting these accessory glands?\",\n",
    "    \"How is the penis structured for its dual role in urination and reproduction? What vascular and nervous mechanisms enable erection, and how does this relate to the anatomical organization of the corpora cavernosa and corpus spongiosum?\",\n",
    "    \"What is the anatomical relationship between the ovaries, uterine tubes, and uterus? How does this arrangement facilitate conception, and what pathologies might develop when abnormalities occur in this system?\",\n",
    "    \"How do the structural and functional changes of the uterus vary across a woman's lifecycle, from fetal development through puberty, pregnancy, and menopause? What cyclical changes occur during the menstrual cycle?\",\n",
    "    \"Compare the anatomy of the vagina and its relationship to surrounding structures with that of the male urethra. How do these differences affect susceptibility to infections and other pathological conditions?\",\n",
    "    \"What is the embryological origin and development of the ovaries? How does this relate to the structures found in the adult ovary, and what implications does this have for the development of ovarian pathologies?\",\n",
    "    \"Describe the structure and function of the clitoris and penis as homologous organs. How do they compare in terms of erectile tissue arrangement, vascular supply, innervation, and embryological development? What implications does this have for sexual response and dysfunction?\",\n",
    "    \"How do the vestibular bulbs in females compare anatomically and functionally to the corpus spongiosum/bulb of the penis in males? What is their role in sexual arousal, and how do they relate to the greater vestibular glands and other external genital structures?\",\n",
    "    \"What is the anatomical and functional relationship between the mammary glands and the reproductive system? How do they develop embryologically, and what hormonal changes affect their structure and function throughout a woman's life from puberty through pregnancy, lactation, and menopause?\",\n",
    "    \"How does the structure of the uterine tube facilitate the transport of ova and fertilization? What are the different segments of the tube, their specific functions, and how does ectopic pregnancy develop in relation to tubal anatomy?\",\n",
    "    \"What is the structure and function of the prostate gland, and how does its anatomy relate to common pathologies? How do the different anatomical zones of the prostate correspond to the development of conditions like benign prostatic hyperplasia and prostate cancer?\",\n",
    "    \"Compare and contrast the structure, development, and functions of the thyroid gland with the other major endocrine glands. How does the embryological origin of the thyroid relate to its adult position and function, and what common pathologies affect this gland?\",\n",
    "    \"How do the anatomical features of the female reproductive system contribute to both normal fertility and common infertility issues? Consider the ovaries, fallopian tubes, uterus, cervix, and vagina in your analysis, and explain how structural or functional abnormalities in each can impact conception and pregnancy.\",\n",
    "    \"How does the structure of the scrotum and its layers provide optimal temperature regulation for the testes? What vascular adaptations contribute to this function, and how do pathological conditions like varicocele affect testicular function?\",\n",
    "    \"How do the anatomical and histological structures of the breast change throughout a woman's life, particularly during pregnancy and lactation? What cellular mechanisms enable milk production and ejection, and how does the transition from colostrum to mature milk occur?\",\n",
    "    \"Compare and contrast the structure, embryological origin, and function of the male and female external genitalia. How do homologous structures differ in their adult form, and what hormonal factors influence their development and later function?\"\n",
    "]\n",
    "category_list = [\"16\"] * 60\n",
    "\n",
    "\n",
    "\n",
    "generation_loader = generation_loader\n",
    "# result_base_path = \"../result\"\n",
    "qa_system = CarManualQA(\n",
    "        generation_loader=generation_loader,\n",
    "        data_folder=\"./data\",\n",
    "        prompt_path_en=\"./prompts/en/generation/gemma3/generation_prompt3.txt\",\n",
    "        result_path=\"./result/5월16일/en-gemma3\",\n",
    "        use_vector_db=True,\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        collection_name=\"Gray\",\n",
    "        language=\"en\"\n",
    "    )\n",
    "    \n",
    "for q, c in zip(questions, category_list):\n",
    "    print(f\"\\n새 쿼리: {q}, 카테고리: {c}\")\n",
    "    \n",
    "    \n",
    "    # 검색 결과 가져오기 (alpha 값 전달)\n",
    "    # search_results = test_search_only(q, c, alpha=alpha_formatted)\n",
    "    # context = \"\\n\\n\".join([result[\"chunk\"] for result in search_results])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # 응답 생성 (alpha 값 전달)\n",
    "    chroma_response = qa_system.generate_response(q, c, top_n=5, alpha=0.3, target_language=\"ko\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"추론시간 : {elapsed_time}\")\n",
    "    print(f\"LLM 답변: {chroma_response['후처리']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. The document should be detailed and in-depth.\n",
    "            the document size has be exactly {chunk_size} characters.\"\"\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import *\n",
    "sent = \"The body is the largest part of a vertebra, and is more or less cylindrical in shape. Its upper and lower surfaces are flattened and rough, and give attachment to the intervertebral fibrocartilages, and each presents a rim around its circumference. In front, the body is convex from side to side and concave from above downward. Behind, it is flat from above downward and slightly concave from side to side. Its anterior surface presents a few small apertures, for the passage of nutrient vessels; on the posterior surface is a single large, irregular aperture, or occasionally more than one, for the exit of the basi-vertebral veins from the body of the vertebra.\"\n",
    "print(en_to_ko(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a86cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# safetensors 파일만 다운로드\n",
    "snapshot_download(\n",
    "    repo_id=\"jbochi/madlad400-10b-mt\",\n",
    "    ignore_patterns=[\"*.gguf\"],\n",
    "    local_dir=\"./model/translate/madlad400-10b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# import torch\n",
    "# import time\n",
    "\n",
    "# def load_madlad_model(model_name='jbochi/madlad400-3b-mt'):\n",
    "#     \"\"\"\n",
    "#     MADLAD 모델과 토크나이저 로드\n",
    "    \n",
    "#     Args:\n",
    "#         model_name (str): 사용할 MADLAD 모델 이름\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: (model, tokenizer) - 로드된 모델과 토크나이저\n",
    "#     \"\"\"\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "#     return model, tokenizer\n",
    "\n",
    "# def translate_text(text, source_lang, target_lang, model=None, tokenizer=None):\n",
    "#     \"\"\"\n",
    "#     MADLAD 모델을 사용하여 텍스트 번역\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): 번역할 텍스트\n",
    "#         source_lang (str): 원본 언어 코드 (예: 'ko', 'en')\n",
    "#         target_lang (str): 대상 언어 코드 (예: 'en', 'ko')\n",
    "#         model: 사전 로드된 모델 (없으면 새로 로드)\n",
    "#         tokenizer: 사전 로드된 토크나이저 (없으면 새로 로드)\n",
    "    \n",
    "#     Returns:\n",
    "#         str: 번역된 텍스트\n",
    "#     \"\"\"\n",
    "#     # 모델과 토크나이저가 제공되지 않은 경우 로드\n",
    "#     if model is None or tokenizer is None:\n",
    "#         model, tokenizer = load_madlad_model()\n",
    "    \n",
    "#     # 텍스트 앞에 대상 언어 토큰 추가\n",
    "#     prefix = f\"<2{target_lang}> \"\n",
    "#     input_text = prefix + text\n",
    "    \n",
    "#     # 입력 텍스트를 토큰화\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "#     # 번역 생성\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=input_ids,\n",
    "#         max_length=512,\n",
    "#         num_beams=4,\n",
    "#         length_penalty=0.6\n",
    "#     )\n",
    "    \n",
    "#     # 출력 토큰을 텍스트로 디코딩\n",
    "#     translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return translated_text\n",
    "\n",
    "# def ko_to_en(text, model=None, tokenizer=None):\n",
    "#     \"\"\"\n",
    "#     한국어를 영어로 번역\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): 번역할 한국어 텍스트\n",
    "#         model: 사전 로드된 모델 (없으면 새로 로드)\n",
    "#         tokenizer: 사전 로드된 토크나이저 (없으면 새로 로드)\n",
    "    \n",
    "#     Returns:\n",
    "#         str: 번역된 영어 텍스트\n",
    "#     \"\"\"\n",
    "#     return translate_text(text, 'ko', 'en', model, tokenizer)\n",
    "\n",
    "# def en_to_ko(text, model=None, tokenizer=None):\n",
    "#     \"\"\"\n",
    "#     영어를 한국어로 번역\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): 번역할 영어 텍스트\n",
    "#         model: 사전 로드된 모델 (없으면 새로 로드)\n",
    "#         tokenizer: 사전 로드된 토크나이저 (없으면 새로 로드)\n",
    "    \n",
    "#     Returns:\n",
    "#         str: 번역된 한국어 텍스트\n",
    "#     \"\"\"\n",
    "#     return translate_text(text, 'en', 'ko', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220af276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 평가용 한국어 문장 리스트\n",
    "korean_sentences = [\n",
    "    \"현대 사회에서 인공지능 기술의 급속한 발전은 우리의 일상생활뿐만 아니라 산업 전반에 걸쳐 혁명적인 변화를 가져오고 있으며, 이러한 변화에 적응하지 못하는 기업들은 시장에서 도태될 가능성이 높아지고 있다.\",\n",
    "    \"글로벌 기후 변화로 인한 극단적인 기상 현상이 증가함에 따라, 전 세계 정부와 기관들은 지속 가능한 발전 목표를 달성하기 위해 다양한 환경 정책을 수립하고 국제적 협력을 강화하고 있으나, 여전히 많은 국가들이 경제적 이익을 우선시하는 경향이 있다.\",\n",
    "    \"한국의 전통문화는 오랜 역사를 통해 형성된 독특한 예술 형태와 철학적 개념을 포함하고 있으며, 특히 한식, 한복, 한옥과 같은 문화적 요소들은 현대 사회에서도 그 가치를 인정받아 세계적으로 주목받고 있는 중이다.\",\n",
    "    \"급변하는 디지털 환경 속에서 정보 보안의 중요성은 날로 증가하고 있으며, 개인 정보 유출 사고가 빈번하게 발생함에 따라 기업들은 더욱 강화된 보안 시스템을 구축하고 있지만, 동시에 해커들의 공격 방식도 더욱 교묘해지고 있어 끊임없는 기술적 발전이 요구된다.\",\n",
    "    \"현대 교육 시스템은 학생들의 창의성과 비판적 사고력을 키우는 데 중점을 두고 있으나, 여전히 많은 국가에서는 표준화된 시험과 암기식 학습에 의존하고 있어, 이러한 불균형을 해소하기 위한 교육 개혁의 필요성이 계속해서 제기되고 있다.\",\n",
    "    \"글로벌 경제의 상호연결성이 증가함에 따라 한 국가의 경제적 위기는 빠르게 다른 국가들에게도 영향을 미치게 되었으며, 이로 인해 국제 금융 기관들은 위기 관리 시스템을 강화하고 각국 정부는 경제 안정화 정책을 더욱 중요시하게 되었다.\",\n",
    "    \"현대 의학 기술의 발전으로 평균 수명이 크게 늘어났지만, 동시에 노인 인구 증가로 인한 사회적, 경제적 부담도 증가하고 있어 각국 정부는 노인 복지 정책과 의료 시스템의 효율성 개선에 많은 노력을 기울이고 있다.\",\n",
    "    \"디지털 플랫폼의 확산은 사람들 간의 소통 방식을 근본적으로 변화시켰으며, 소셜 미디어를 통한 정보 공유가 활발해지면서 정보의 신뢰성 문제와 개인 정보 보호에 대한 우려가 함께 증가하고 있어 디지털 리터러시 교육의 중요성이 더욱 강조되고 있다.\",\n",
    "    \"과학기술의 급속한 발전은 의료, 농업, 에너지 등 다양한 분야에서 혁신을 가져왔지만, 동시에 윤리적 문제와 직업 시장의 변화 등 새로운 사회적 도전들을 야기하고 있어, 이러한 변화에 대응하기 위한 사회적 합의와 제도적 준비가 필요하다.\",\n",
    "    \"언어는 단순한 의사소통의 도구를 넘어 문화적 정체성과 세계관을 형성하는 중요한 요소로, 세계화가 진행됨에 따라 많은 소수 언어들이 사라질 위기에 처해있으며, 이는 인류 문화 다양성 보존의 관점에서 중요한 문제로 대두되고 있다.\"\n",
    "]\n",
    "\n",
    "# 번역 평가용 영어 문장 리스트\n",
    "english_sentences = [\n",
    "    \"The rapid advancement of artificial intelligence technology in modern society is bringing revolutionary changes not only to our daily lives but also across industries, and companies that fail to adapt to these changes are increasingly likely to be eliminated from the market.\",\n",
    "    \"As extreme weather phenomena increase due to global climate change, governments and institutions worldwide are establishing various environmental policies and strengthening international cooperation to achieve sustainable development goals, yet many countries still tend to prioritize economic interests.\",\n",
    "    \"Korean traditional culture encompasses unique art forms and philosophical concepts formed throughout its long history, and cultural elements such as Korean cuisine, hanbok (traditional clothing), and hanok (traditional houses) are being recognized globally for their value even in modern society.\",\n",
    "    \"The importance of information security is increasing day by day in the rapidly changing digital environment, and as personal information leakage incidents frequently occur, companies are building enhanced security systems, but at the same time, hackers' attack methods are becoming more sophisticated, requiring continuous technological advancement.\",\n",
    "    \"The modern education system focuses on developing students' creativity and critical thinking skills, but many countries still rely on standardized tests and rote learning, leading to ongoing calls for educational reform to address this imbalance.\",\n",
    "    \"As the interconnectedness of the global economy increases, an economic crisis in one country quickly affects others, leading international financial institutions to strengthen crisis management systems and governments to place greater emphasis on economic stabilization policies.\",\n",
    "    \"While advancements in modern medical technology have significantly extended average life expectancy, the simultaneous increase in the elderly population has increased social and economic burdens, prompting governments to invest heavily in policies for elderly welfare and improving the efficiency of healthcare systems.\",\n",
    "    \"The proliferation of digital platforms has fundamentally changed how people communicate, and as information sharing through social media becomes more active, concerns about information reliability and personal data protection have also increased, emphasizing the importance of digital literacy education.\",\n",
    "    \"The rapid development of science and technology has brought innovation in various fields such as medicine, agriculture, and energy, but it has also created new social challenges such as ethical issues and changes in the job market, requiring social consensus and institutional preparation to respond to these changes.\",\n",
    "    \"Language is more than just a tool for communication; it is an important element that forms cultural identity and worldview, and as globalization progresses, many minority languages are in danger of disappearing, which is emerging as an important issue from the perspective of preserving human cultural diversity.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67383d4c",
   "metadata": {},
   "source": [
    "# jbochi/madlad400-3b-mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2180569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구두점으로 분할된 문장 리스트 : ['The rapid advancement of artificial intelligence technology in modern society is bringing revolutionary changes not only to our daily lives but also across industries, and companies that fail to adapt to these changes are increasingly likely to be eliminated from the market.']\n",
      "구두점으로 분할된 문장 리스트 : ['As extreme weather phenomena increase due to global climate change, governments and institutions worldwide are establishing various environmental policies and strengthening international cooperation to achieve sustainable development goals, yet many countries still tend to prioritize economic interests.']\n",
      "구두점으로 분할된 문장 리스트 : ['Korean traditional culture encompasses unique art forms and philosophical concepts formed throughout its long history, and cultural elements such as Korean cuisine, hanbok (traditional clothing), and hanok (traditional houses) are being recognized globally for their value even in modern society.']\n",
      "구두점으로 분할된 문장 리스트 : [\"The importance of information security is increasing day by day in the rapidly changing digital environment, and as personal information leakage incidents frequently occur, companies are building enhanced security systems, but at the same time, hackers' attack methods are becoming more sophisticated, requiring continuous technological advancement.\"]\n",
      "구두점으로 분할된 문장 리스트 : [\"The modern education system focuses on developing students' creativity and critical thinking skills, but many countries still rely on standardized tests and rote learning, leading to ongoing calls for educational reform to address this imbalance.\"]\n",
      "구두점으로 분할된 문장 리스트 : ['As the interconnectedness of the global economy increases, an economic crisis in one country quickly affects others, leading international financial institutions to strengthen crisis management systems and governments to place greater emphasis on economic stabilization policies.']\n",
      "구두점으로 분할된 문장 리스트 : ['While advancements in modern medical technology have significantly extended average life expectancy, the simultaneous increase in the elderly population has increased social and economic burdens, prompting governments to invest heavily in policies for elderly welfare and improving the efficiency of healthcare systems.']\n",
      "구두점으로 분할된 문장 리스트 : ['The proliferation of digital platforms has fundamentally changed how people communicate, and as information sharing through social media becomes more active, concerns about information reliability and personal data protection have also increased, emphasizing the importance of digital literacy education.']\n",
      "구두점으로 분할된 문장 리스트 : ['The rapid development of science and technology has brought innovation in various fields such as medicine, agriculture, and energy, but it has also created new social challenges such as ethical issues and changes in the job market, requiring social consensus and institutional preparation to respond to these changes.']\n",
      "구두점으로 분할된 문장 리스트 : ['Language is more than just a tool for communication;', ' it is an important element that forms cultural identity and worldview, and as globalization progresses, many minority languages are in danger of disappearing, which is emerging as an important issue from the perspective of preserving human cultural diversity.']\n",
      "영어 -> 한국어 번역 결과\n",
      "구두점으로 분할된 문장 리스트 : ['현대 사회에서 인공 지능 기술의 급속한 발전은 우리의 일상 생활뿐만 아니라 산업 전반에 혁명적인 변화를 가져오고 있으며, 이러한 变化에 적응하지 못하는 기업들은 점점 더 시장에서 제거될 가능성이 높아지고 있습니다.']\n",
      "영->한 1번째 결과 : 현대 사회에서 인공 지능 기술의 급속한 발전은 우리의 일상 생활뿐만 아니라 산업 전반에 혁명을 가져오고 있으며, 이러한 변화에 적응하지 못하는 기업은 점점 더 시장에서 제거될 가능성이 높아지고 있습니다. \n",
      "번역 시간 : 20.734106302261353\n",
      "구두점으로 분할된 문장 리스트 : ['지구 기후 변화로 인해 극한 기상 현상이 증가함에 따라 전 세계 정부와 기관들은 지속 가능한 개발 목표를 달성하기 위해 다양한 환경 정책을 수립하고 국제 협력을 강화하고 있지만, 여전히 많은 국가들이 경제적 이익을 우선시하는 경향이 있습니다.']\n",
      "영->한 2번째 결과 : 지구 기후 변화로 인해 극단적인 기상 현상이 증가함에 따라 전 세계 정부와 기관들은 지속 가능한 개발 목표를 달성하기 위해 다양한 환경 정책을 수립하고 국제 협력을 강화하고 있지만, 여전히 많은 국가들이 경제적 이익을 우선시하는 경향이 있습니다. \n",
      "번역 시간 : 22.975634336471558\n",
      "구두점으로 분할된 문장 리스트 : ['한국의 전통 문화는 오랜 역사를 통해 형성된 독특한 예술 양식과 철학적 개념을 포함하고 있으며, 한국 요리, 한복(전통 의상), 한옥과 같은 문명 요소는 현대 사회에서도 그 가치가 세계적으로 인정되고 있습니다.']\n",
      "영->한 3번째 결과 : 한국의 전통 문화는 오랜 역사를 통해 형성된 독특한 예술 양식과 철학적 개념을 포함하고 있으며, 한국 요리, 한복(전통 의상), 한옥과 같은 문명 요소는 현대 사회에서도 그 가치가 세계적으로 인정되고 있습니다. \n",
      "번역 시간 : 21.18552589416504\n",
      "구두점으로 분할된 문장 리스트 : ['빠르게 변화하는 디지털 환경에서 정보 보안의 중요성은 날이 갈수록 증가하고 있으며, 개인정보 유출 사건이 자주 발생함에 따라 기업들은 향상된 안전 시스템을 구축하고 있습니다.', ' 그러나 동시에 해커의 공격 방법은 지속적인 기술 발전이 필요한 점점 더 정교해지고있습니다.']\n",
      "영->한 4번째 결과 : 빠르게 변화하는 디지털 환경에서 정보 보안의 중요성은 날이 갈수록 높아지고 있으며, 개인정보 유출 사건이 자주 발생하므로 기업들은 향상된 안전 시스템을 구축하고 있습니다. 그러나 동시에 해커의 공격 방법은 지속적인 기술 발전을 필요로 점점 더 정교해지고있다. \n",
      "번역 시간 : 36.89545941352844\n",
      "구두점으로 분할된 문장 리스트 : ['현대 교육 시스템은 학생들의 창의성과 비판적 사고 능력을 발전시키는 데 초점을 맞추고 있지만, 많은 국가들은 여전히 표준화된 시험과 암기에 의존하고 있으며, 이러한 불균형을 해결하기 위해 지속적으로 교양 개혁을 요구하고 있다.']\n",
      "영->한 5번째 결과 : 현대 교육 시스템은 학생들의 창의성과 비판적 사고 능력을 발전시키는 데 초점을 맞추고 있지만, 많은 국가들은 여전히 표준화된 시험과 암기에 의존하고 있으며, 이러한 불균형을 해결하기 위해 지속적으로 교양 개혁을 요구하고 있다. \n",
      "번역 시간 : 22.008360862731934\n",
      "구두점으로 분할된 문장 리스트 : ['세계 경제의 상호 연결성이 증가함에 따라, 한 국가의 경기 위기는 다른 나라에 빠르게 영향을 미치고, 국제 금융 기관은 위험 관리 시스템을 강화하고, 정부는 경상 안정 정책에 더욱 중점을 둔다.']\n",
      "영->한 6번째 결과 : 세계 경제의 상호 연결성이 증가함에 따라, 한 국가의 경기 위기는 다른 나라에 빠르게 영향을 미치고, 국제 금융 기관은 위험 관리 시스템을 강화하고, 정부는 경상 안정 정책에 더욱 중점을 두고 있다. \n",
      "번역 시간 : 21.30028223991394\n",
      "구두점으로 분할된 문장 리스트 : ['현대 의료 기술의 발전은 평균 수명을 상당히 연장시켰지만, 동시에 노인 인구의 증가는 사회적, 경제적 부담을 늘려 정부가 노년 복지 정책에 막대한 투자를 하도록 유도하고 보건 시스템 효율성을 개선했다.']\n",
      "영->한 7번째 결과 : 현대 의료 기술의 발전은 평균 수명을 상당히 연장시켰지만, 동시에 노인 인구의 증가는 사회적, 경제적 부담을 늘려 정부가 노년 복지 정책에 막대한 투자를 하도록 유도하고 보건 시스템 효율성을 개선했다. \n",
      "번역 시간 : 22.137519121170044\n",
      "구두점으로 분할된 문장 리스트 : ['디지털 플랫폼의 확산은 사람들이 소통하는 방식을 근본적으로 변화시켰으며, 소셜 미디어를 통한 정보 공유가 더욱 활성화됨에 따라 정 보의 신뢰성과 개인 데이터 보호에 대한 우려도 증가하였고, 이로 인해 리터러시 교육의 중요성이 강조되었습니다.']\n",
      "영->한 8번째 결과 : 디지털 플랫폼의 확산은 사람들이 소통하는 방식을 근본적으로 변화시켰으며, 소셜 미디어를 통한 정보 공유가 더욱 활성화됨에 따라 뉴스의 신뢰성과 개인 데이터 보호에 대한 우려가 증가하면서 리터러시 교육의 중요성이 강조되었습니다. \n",
      "번역 시간 : 22.63261342048645\n",
      "구두점으로 분할된 문장 리스트 : ['과학과 기술의 급속한 발전은 의학, 농업 및 에너지와 같은 다양한 분야에서 혁신을 가져왔지만 윤리적 문제와 직업 시장 변화 등 새로운 사회적 과제를 창출했으며, 이러한 变化에 대응하기 위해서는 합의와 제도적 준비가 필요하다.']\n",
      "영->한 9번째 결과 : 과학과 기술의 급속한 발전은 의학, 농업 및 에너지와 같은 다양한 분야에서 혁신을 가져왔지만 윤리적 문제와 직업 시장 변화를 포함한 새로운 사회적 과제를 창출했으며, 이들에 대응하기 위해서는 합의와 제도적 준비가 필요하다. \n",
      "번역 시간 : 23.100051403045654\n",
      "구두점으로 분할된 문장 리스트 : ['언어는 단순히 의사소통을 위한 도구가 아닙니다.', ' 언어는 문화적 정체성과 세계관을 형성하는 중요한 요소이며, 국제화가 진행됨에 따라 많은 소수 민족 어들이 사라지는 위험에 처해 있으며 이는 인류문화의 다양성을 보존하는 관점에서 중대한 문제로 부각되고 있다.']\n",
      "영->한 10번째 결과 : 언어는 단순히 의사소통을 위한 도구가 아닙니다. 언어는 문화적 정체성과 세계관을 형성하는 중요한 요소이며, 국제화가 진행됨에 따라 많은 소수 민족어가 사라지는 위험에 처해 있으며 이는 인류문화의 다양성을 보존하는 관점에서 중대한 문제로 부각되고 있다. \n",
      "번역 시간 : 35.899635791778564\n",
      "구두점으로 분할된 문장 리스트 : ['현대 사회에서 인공지능 기술의 급속한 발전은 우리의 일상생활뿐만 아니라 산업 전반에 걸쳐 혁명적인 변화를 가져오고 있으며, 이러한 변화에 적응하지 못하는 기업들은 시장에서 도태될 가능성이 높아지고 있다.']\n",
      "구두점으로 분할된 문장 리스트 : ['글로벌 기후 변화로 인한 극단적인 기상 현상이 증가함에 따라, 전 세계 정부와 기관들은 지속 가능한 발전 목표를 달성하기 위해 다양한 환경 정책을 수립하고 국제적 협력을 강화하고 있으나, 여전히 많은 국가들이 경제적 이익을 우선시하는 경향이 있다.']\n",
      "구두점으로 분할된 문장 리스트 : ['한국의 전통문화는 오랜 역사를 통해 형성된 독특한 예술 형태와 철학적 개념을 포함하고 있으며, 특히 한식, 한복, 한옥과 같은 문화적 요소들은 현대 사회에서도 그 가치를 인정받아 세계적으로 주목받고 있는 중이다.']\n",
      "구두점으로 분할된 문장 리스트 : ['급변하는 디지털 환경 속에서 정보 보안의 중요성은 날로 증가하고 있으며, 개인 정보 유출 사고가 빈번하게 발생함에 따라 기업들은 더욱 강화된 보안 시스템을 구축하고 있지만, 동시에 해커들의 공격 방식도 더욱 교묘해지고 있어 끊임없는 기술적 발전이 요구된다.']\n",
      "구두점으로 분할된 문장 리스트 : ['현대 교육 시스템은 학생들의 창의성과 비판적 사고력을 키우는 데 중점을 두고 있으나, 여전히 많은 국가에서는 표준화된 시험과 암기식 학습에 의존하고 있어, 이러한 불균형을 해소하기 위한 교육 개혁의 필요성이 계속해서 제기되고 있다.']\n",
      "구두점으로 분할된 문장 리스트 : ['글로벌 경제의 상호연결성이 증가함에 따라 한 국가의 경제적 위기는 빠르게 다른 국가들에게도 영향을 미치게 되었으며, 이로 인해 국제 금융 기관들은 위기 관리 시스템을 강화하고 각국 정부는 경제 안정화 정책을 더욱 중요시하게 되었다.']\n",
      "구두점으로 분할된 문장 리스트 : ['현대 의학 기술의 발전으로 평균 수명이 크게 늘어났지만, 동시에 노인 인구 증가로 인한 사회적, 경제적 부담도 증가하고 있어 각국 정부는 노인 복지 정책과 의료 시스템의 효율성 개선에 많은 노력을 기울이고 있다.']\n",
      "구두점으로 분할된 문장 리스트 : ['디지털 플랫폼의 확산은 사람들 간의 소통 방식을 근본적으로 변화시켰으며, 소셜 미디어를 통한 정보 공유가 활발해지면서 정보의 신뢰성 문제와 개인 정보 보호에 대한 우려가 함께 증가하고 있어 디지털 리터러시 교육의 중요성이 더욱 강조되고 있다.']\n",
      "구두점으로 분할된 문장 리스트 : ['과학기술의 급속한 발전은 의료, 농업, 에너지 등 다양한 분야에서 혁신을 가져왔지만, 동시에 윤리적 문제와 직업 시장의 변화 등 새로운 사회적 도전들을 야기하고 있어, 이러한 변화에 대응하기 위한 사회적 합의와 제도적 준비가 필요하다.']\n",
      "구두점으로 분할된 문장 리스트 : ['언어는 단순한 의사소통의 도구를 넘어 문화적 정체성과 세계관을 형성하는 중요한 요소로, 세계화가 진행됨에 따라 많은 소수 언어들이 사라질 위기에 처해있으며, 이는 인류 문화 다양성 보존의 관점에서 중요한 문제로 대두되고 있다.']\n",
      "한국어 -> 영어 번역 결과\n",
      "구두점으로 분할된 문장 리스트 : ['The rapid development of AI technologies in modern society is revolutionizing not only our everyday lives, but also industries, and companies that fail to adapt to these changes are increasingly at risk of falling out of the market.']\n",
      "한->영 1번째 결과 : The rapid development of AI technologies in modern society is revolutionizing not only our everyday lives, but also industries, and companies that fail to adapt to these changes are increasingly at risk of falling out of the market. \n",
      "번역 시간 : 16.90134334564209\n",
      "구두점으로 분할된 문장 리스트 : ['As extreme weather events caused by global climate change increase, governments and institutions around the world are adopting a variety of environmental policies, and strengthening international cooperation, to achieve the Sustainable Development Goals.']\n",
      "한->영 2번째 결과 : As extreme weather events caused by global climate change increase, governments and institutions around the world are adopting a variety of environmental policies, and strengthening international cooperation, to achieve the Sustainable Development Goals. \n",
      "번역 시간 : 17.91715717315674\n",
      "구두점으로 분할된 문장 리스트 : ['Traditional Korean culture includes unique art forms and philosophical concepts that have been formed over a long period of history, and cultural elements such as Korean cuisine, Korean clothing and Korean houses are recognized in modern society and are attracting attention worldwide.']\n",
      "한->영 3번째 결과 : Traditional Korean culture includes unique art forms and philosophical concepts that have been formed over a long period of history, and cultural elements such as Korean cuisine, Korean clothing and Korean houses are recognized in modern society and are attracting attention worldwide. \n",
      "번역 시간 : 18.13147735595703\n",
      "구두점으로 분할된 문장 리스트 : [\"In a rapidly changing digital environment, the importance of information security is increasing every day, and as personal data breach incidents occur more frequently, companies are building more robust security systems, but at the same time, hackers' attack methods are becoming more sophisticated, requiring continuous technological development.\"]\n",
      "한->영 4번째 결과 : In a rapidly changing digital environment, the importance of information security is increasing every day, and as personal data breach incidents occur more frequently, companies are building more robust security systems, but at the same time, hackers' attack methods are becoming more sophisticated, requiring continuous technological development. \n",
      "번역 시간 : 19.777565479278564\n",
      "구두점으로 분할된 문장 리스트 : [\"Modern education systems focus on developing students' creativity and critical thinking, but in many countries they still rely on standardized tests and memorization, and the need for educational reform to address these imbalances continues to be raised.\"]\n",
      "한->영 5번째 결과 : Modern education systems focus on developing students' creativity and critical thinking, but in many countries they still rely on standardized tests and memorization, and the need for educational reform to address these imbalances continues to be raised. \n",
      "번역 시간 : 18.12117886543274\n",
      "구두점으로 분할된 문장 리스트 : ['As the interconnectedness of the global economy increased, economic crises in one country quickly affected others, prompting international financial institutions to strengthen their crisis management systems and national governments to place greater emphasis on economic stabilization policies.']\n",
      "한->영 6번째 결과 : As the interconnectedness of the global economy increased, economic crises in one country quickly affected others, prompting international financial institutions to strengthen their crisis management systems and national governments to place greater emphasis on economic stabilization policies. \n",
      "번역 시간 : 17.99290657043457\n",
      "구두점으로 분할된 문장 리스트 : ['While progress in modern medical technology has significantly increased average life expectancy, at the same time the social and economic burden of an aging population is increasing, and governments are making great efforts to improve elderly welfare policies and the efficiency of health systems.']\n",
      "한->영 7번째 결과 : While progress in modern medical technology has significantly increased average life expectancy, at the same time the social and economic burden of an aging population is increasing, and governments are making great efforts to improve elderly welfare policies and the efficiency of health systems. \n",
      "번역 시간 : 18.788023233413696\n",
      "구두점으로 분할된 문장 리스트 : ['The proliferation of digital platforms has fundamentally changed the way people communicate, and the increased sharing of information through social media has led to increased concerns about the reliability of the information and privacy, which has made digital literacy education even more important.']\n",
      "한->영 8번째 결과 : The proliferation of digital platforms has fundamentally changed the way people communicate, and the increased sharing of information through social media has led to increased concerns about the reliability of the information and privacy, which has made digital literacy education even more important. \n",
      "번역 시간 : 18.232505798339844\n",
      "구두점으로 분할된 문장 리스트 : ['Rapid advances in science and technology have led to innovation in a variety of fields, including medicine, agriculture, and energy, but at the same time have created new social challenges, such as ethical issues and changes in the job market, which require social consensus and institutional preparation to respond to these changes.']\n",
      "한->영 9번째 결과 : Rapid advances in science and technology have led to innovation in a variety of fields, including medicine, agriculture, and energy, but at the same time have created new social challenges, such as ethical issues and changes in the job market, which require social consensus and institutional preparation to respond to these changes. \n",
      "번역 시간 : 19.406304597854614\n",
      "구두점으로 분할된 문장 리스트 : ['Languages are not merely a means of communication, but also an important factor in forming cultural identities and worldviews.', ' As globalization progresses, many minority languages are at risk of disappearing, which poses a major problem for the preservation of human cultural diversity.']\n",
      "한->영 10번째 결과 : Languages are not merely a means of communication, but also an important factor in forming cultural identities and worldviews. As globalization progresses, many minority languages are at risk of disappearing, which poses a major problem for the preservation of human cultural diversity. \n",
      "번역 시간 : 31.437308073043823\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from translate import *\n",
    "\n",
    "en_to_ko_results = [en_to_ko(sent) for sent in english_sentences]\n",
    "print(f\"영어 -> 한국어 번역 결과\")\n",
    "for i, result in enumerate(en_to_ko_results) :\n",
    "    start_time = time.time()\n",
    "    print(f\"영->한 {i+1}번째 결과 : {en_to_ko(result)}\")\n",
    "    end_time = time.time()\n",
    "    trans_time = end_time - start_time\n",
    "    print(f\"번역 시간 : {trans_time}\")\n",
    "\n",
    "ko_to_en_results = [ko_to_en(sent) for sent in korean_sentences]\n",
    "print(f\"한국어 -> 영어 번역 결과\")\n",
    "for i, result in enumerate(ko_to_en_results) :\n",
    "    start_time = time.time()\n",
    "    print(f\"한->영 {i+1}번째 결과 : {ko_to_en(result)}\")\n",
    "    end_time = time.time()\n",
    "    trans_time = end_time - start_time\n",
    "    print(f\"번역 시간 : {trans_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import *\n",
    "\n",
    "sent = \"The body is the largest part of a vertebra, and is more or less cylindrical in shape. Its upper and lower surfaces are flattened and rough, and give attachment to the intervertebral fibrocartilages, and each presents a rim around its circumference. In front, the body is convex from side to side and concave from above downward. Behind, it is flat from above downward and slightly concave from side to side. Its anterior surface presents a few small apertures, for the passage of nutrient vessels; on the posterior surface is a single large, irregular aperture, or occasionally more than one, for the exit of the basi-vertebral veins from the body of the vertebra.\"\n",
    "print(ko_to_en(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657d2951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 전 :  According to page 43, “others, such as the bones of the limbs, are preceded by rods of cartilage. Hence two kinds of ossification are described: the intramembranous and the intracartilaginous.” \n",
      "According to page 39, “Up to a certain stage the development of the skull corresponds with that of the vertebral column.”\n",
      "The document does not contain information about the developmental implications of these differences.\n",
      "구두점으로 분할된 문장 리스트 : ['According to page 43, “others, such as the bones of the limbs, are preceded by rods of cartilage.', 'Hence two kinds of ossification are described: the intramembranous and the intracartilaginous.” \\nAccording to page 39, “Up to a certain stage the development of the skull corresponds with that of the vertebral column.”\\nThe document does not contain information about the developmental implications of these differences.']\n",
      "번역 후 :  43쪽에 따르면, \"사지의 뼈와 같은 다른 것들은 연골의 막대가 앞서 있다. 010년 10월 1일자 뉴욕타임스에 실린 논문에 따르면, “The development of the skull corresponds to that of the vertebral column, but there are some differences in the developmental implications of the intramembranous and the intracartilaginous ossifications.” “AccordingAcAcActAcAcAcAccAcAcAcAcACacAcAc AcAcAc ACacAcAC AcAcACAcAc AccAcAc ACCAcAc\n"
     ]
    }
   ],
   "source": [
    "from translate import *\n",
    "sent = 'According to page 43, “others, such as the bones of the limbs, are preceded by rods of cartilage. Hence two kinds of ossification are described: the intramembranous and the intracartilaginous.” \\nAccording to page 39, “Up to a certain stage the development of the skull corresponds with that of the vertebral column.”\\nThe document does not contain information about the developmental implications of these differences.'\n",
    "print(\"번역 전 : \", sent)\n",
    "result = en_to_ko(sent)\n",
    "print(\"번역 후 : \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd4617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일이 성공적으로 수정되었습니다: ./data/GrayAnatomy_Formatted.md\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 파일 경로\n",
    "file_path = './data/GrayAnatomy_Formatted.md'\n",
    "\n",
    "# 파일 읽기\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 1. 취소선(~~) 제거\n",
    "content = re.sub(r'~~', '', content)\n",
    "\n",
    "# 2. 볼드체(**) 제거 - 단어 자체는 유지하고 표시만 제거\n",
    "content = re.sub(r'\\*\\*', '', content)\n",
    "\n",
    "# 3. 이탤릭체(*) 제거 - 단어 자체는 유지하고 표시만 제거\n",
    "content = re.sub(r'(?<!\\*)\\*(?!\\*)', '', content)\n",
    "\n",
    "content = re.sub(r'F IG', 'FIG', content)\n",
    "# 수정된 내용 저장\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"파일이 성공적으로 수정되었습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f64d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
