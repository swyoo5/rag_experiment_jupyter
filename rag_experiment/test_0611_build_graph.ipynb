{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5838cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy 'en_core_web_sm' 모델이 성공적으로 로드되었습니다.\n",
      "데이터 폴더 존재 여부를 확인합니다...\n",
      "데이터 폴더 './data/split_file/anatomy'가 이미 존재합니다. 기존 파일을 사용합니다.\n",
      "'./data/split_file/anatomy' 폴더에서 Markdown 파일들을 읽고 지식 그래프를 구축합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "지식 그래프 구축: 100%|██████████| 12/12 [04:44<00:00, 23.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지식 그래프 구축 완료. 총 12개의 파일을 처리했습니다.\n",
      "지식 그래프에는 현재 50656개의 노드와 8567713개의 엣지가 있습니다.\n",
      "\n",
      "간단한 시각화를 위해 특정 키워드 주변의 하위 그래프를 생성합니다.\n",
      "시각화할 하위 그래프: 노드 수 4834, 엣지 수 8480058\n",
      "지식 그래프 시각화를 시작합니다...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"SpaCy 'en_core_web_sm' 모델이 성공적으로 로드되었습니다.\")\n",
    "except OSError:\n",
    "    print(\"SpaCy 'en_core_web_sm' 모델을 다운로드합니다...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"SpaCy 'en_core_web_sm' 모델 다운로드 및 로드 완료.\")\n",
    "\n",
    "def extract_entities_and_relations(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    relations = []\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        entities.append(chunk.text.lower())\n",
    "\n",
    "    for i in range(len(doc) - 1):\n",
    "        token1 = doc[i]\n",
    "        token2 = doc[i+1]\n",
    "\n",
    "        if token1.pos_ == \"NOUN\" and token2.pos_ == \"NOUN\":\n",
    "            relations.append((token1.lemma_.lower(), \"has_part\" if token2.dep_ == \"compound\" else \"related_to\", token2.lemma_.lower()))\n",
    "        elif token1.ent_type_ and token2.ent_type_:\n",
    "            relations.append((token1.text.lower(), \"co_occurs_with\", token2.text.lower()))\n",
    "        elif token1.pos_ == \"NOUN\" and token2.dep_ == \"prep\" and i + 2 < len(doc) and doc[i+2].pos_ == \"NOUN\":\n",
    "            relations.append((token1.lemma_.lower(), \"is_\" + token2.text.lower() + \"_of\", doc[i+2].lemma_.lower()))\n",
    "            \n",
    "    extracted_nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            extracted_nouns.append(token.lemma_.lower())\n",
    "\n",
    "    for i in range(len(extracted_nouns)):\n",
    "        for j in range(i + 1, len(extracted_nouns)):\n",
    "            if extracted_nouns[i] != extracted_nouns[j]:\n",
    "                relations.append((extracted_nouns[i], \"co_occurs_in_document\", extracted_nouns[j]))\n",
    "\n",
    "    entities = list(set(entities + extracted_nouns))\n",
    "    relations = list(set(relations))\n",
    "\n",
    "    return entities, relations\n",
    "\n",
    "def clean_markdown(md_text):\n",
    "    text = re.sub(r'^#+\\s*.*$', '', md_text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)\n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'^\\s*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^-{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\*{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'`(.*?)`', r'\\1', text)\n",
    "    text = re.sub(r'\\n\\n+', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def build_knowledge_graph(data_folder_path):\n",
    "    G = nx.DiGraph()\n",
    "    print(f\"'{data_folder_path}' 폴더에서 Markdown 파일들을 읽고 지식 그래프를 구축합니다...\")\n",
    "    \n",
    "    markdown_files = [f for f in os.listdir(data_folder_path) if f.endswith(\".md\")]\n",
    "    for filename in tqdm(markdown_files, desc=\"지식 그래프 구축\"):\n",
    "        filepath = os.path.join(data_folder_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            md_text = f.read()\n",
    "            plain_text = clean_markdown(md_text)\n",
    "            entities, relations = extract_entities_and_relations(plain_text)\n",
    "\n",
    "            for entity in entities:\n",
    "                G.add_node(entity)\n",
    "\n",
    "            for subj, pred, obj in relations:\n",
    "                if subj in G.nodes and obj in G.nodes:\n",
    "                    G.add_edge(subj, obj, relation=pred)\n",
    "                else:\n",
    "                    if subj not in G.nodes:\n",
    "                        G.add_node(subj)\n",
    "                    if obj not in G.nodes:\n",
    "                        G.add_node(obj)\n",
    "                    G.add_edge(subj, obj, relation=pred)\n",
    "    \n",
    "    print(f\"지식 그래프 구축 완료. 총 {len(markdown_files)}개의 파일을 처리했습니다.\")\n",
    "    print(f\"지식 그래프에는 현재 {G.number_of_nodes()}개의 노드와 {G.number_of_edges()}개의 엣지가 있습니다.\")\n",
    "    return G\n",
    "\n",
    "def visualize_graph(graph, title=\"Knowledge Graph\"):\n",
    "    print(\"지식 그래프 시각화를 시작합니다...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pos = nx.spring_layout(graph, k=0.15, iterations=20) \n",
    "\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_color=\"skyblue\", alpha=0.9)\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=8, font_weight=\"bold\")\n",
    "    \n",
    "    nx.draw_networkx_edges(graph, pos, edge_color=\"gray\", arrows=True, alpha=0.6)\n",
    "    edge_labels = nx.get_edge_attributes(graph, 'relation')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=7, verticalalignment='baseline')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"지식 그래프 시각화가 완료되었습니다.\")\n",
    "\n",
    "# --- 간단한 시각화를 위한 추가 함수 ---\n",
    "\n",
    "def get_subgraph_around_keywords(full_graph, keywords, depth=1):\n",
    "    sub_nodes = set()\n",
    "    for keyword in keywords:\n",
    "        if keyword in full_graph:\n",
    "            sub_nodes.add(keyword)\n",
    "            # 특정 깊이까지 이웃 노드 추가\n",
    "            for neighbor in nx.ego_graph(full_graph, keyword, radius=depth).nodes():\n",
    "                sub_nodes.add(neighbor)\n",
    "    \n",
    "    # 선택된 노드로 하위 그래프 생성\n",
    "    subgraph = full_graph.subgraph(sub_nodes)\n",
    "    return subgraph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"./data/split_file/anatomy\"\n",
    "\n",
    "    print(\"데이터 폴더 존재 여부를 확인합니다...\")\n",
    "    if not os.path.exists(data_folder):\n",
    "        print(f\"데이터 폴더 '{data_folder}'가 존재하지 않습니다. 폴더를 생성합니다...\")\n",
    "        os.makedirs(data_folder)\n",
    "        \n",
    "        dummy_files = [\n",
    "            \"1_Embryology.md\", \"2_Osteology.md\", \"3_Syndesmology.md\",\n",
    "            \"4_Myology.md\", \"5_Angiology.md\", \"6_The_Arteries.md\",\n",
    "            \"7_The_Veins.md\", \"8_The_Lymphatic_System.md\", \"9_Neurology.md\",\n",
    "            \"10_The_Organs_of_the_Senses_and_the_Common_Integument.md\",\n",
    "            \"11_Splanchnology.md\", \"12_Surface_Anatomy_and_Surface_Markings.md\"\n",
    "        ]\n",
    "\n",
    "        file_contents = {\n",
    "            \"1_Embryology.md\": \"Embryology is the study of embryos. The human embryo develops after fertilization.\",\n",
    "            \"2_Osteology.md\": \"Osteology is the study of bones. The femur is a long bone in the thigh. Muscles attach to bones.\",\n",
    "            \"3_Syndesmology.md\": \"Syndesmology is the study of joints and ligaments. Ligaments connect bones to other bones.\",\n",
    "            \"4_Myology.md\": \"Myology is the study of muscles. Muscles enable movement. There are different types of muscle tissue.\",\n",
    "            \"5_Angiology.md\": \"Angiology is the study of the circulatory and lymphatic systems. It includes arteries, veins, and lymphatic vessels.\",\n",
    "            \"6_The_Arteries.md\": \"Arteries carry oxygenated blood away from the heart. The aorta is the largest artery.\",\n",
    "            \"7_The_Veins.md\": \"Veins carry deoxygenated blood back to the heart. Valves in veins prevent backflow.\",\n",
    "            \"8_The_Lymphatic_System.md\": \"The lymphatic system is part of the immune system. It includes lymph nodes and lymphatic vessels.\",\n",
    "            \"9_Neurology.md\": \"Neurology is the study of the nervous system. The brain and spinal cord are central nervous system organs.\",\n",
    "            \"10_The_Organs_of_the_Senses_and_the_Common_Integument.md\": \"This section covers sensory organs like eyes and ears, and the integumentary system, which includes skin, hair, and nails.\",\n",
    "            \"11_Splanchnology.md\": \"Splanchnology is the study of internal organs or viscera. It includes organs of the digestive, respiratory, and urogenital systems.\",\n",
    "            \"12_Surface_Anatomy_and_Surface_Markings.md\": \"Surface anatomy studies external features of the body. Surface markings indicate underlying structures.\"\n",
    "        }\n",
    "\n",
    "        print(\"더미 파일을 생성합니다...\")\n",
    "        for filename in tqdm(dummy_files, desc=\"더미 파일 생성\"):\n",
    "            filepath = os.path.join(data_folder, filename)\n",
    "            content = file_contents.get(filename, f\"This is a dummy file for {filename.replace('.md', '').replace('_', ' ')}.\")\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "        print(\"모든 더미 파일 생성이 완료되었습니다.\")\n",
    "    else:\n",
    "        print(f\"데이터 폴더 '{data_folder}'가 이미 존재합니다. 기존 파일을 사용합니다.\")\n",
    "    \n",
    "    full_knowledge_graph = build_knowledge_graph(data_folder)\n",
    "\n",
    "    # --- 간단한 시각화를 위한 코드 추가 ---\n",
    "    print(\"\\n간단한 시각화를 위해 특정 키워드 주변의 하위 그래프를 생성합니다.\")\n",
    "    \n",
    "    # 시각화하고 싶은 키워드를 여기에 입력하세요 (소문자로 입력)\n",
    "    keywords_to_visualize = [\"brain\", \"heart\", \"muscle\", \"bone\", \"artery\"] \n",
    "    \n",
    "    # 키워드 주변의 하위 그래프 생성 (깊이 1로 설정하여 너무 커지지 않도록 함)\n",
    "    sub_graph = get_subgraph_around_keywords(full_knowledge_graph, keywords_to_visualize, depth=1)\n",
    "\n",
    "    print(f\"시각화할 하위 그래프: 노드 수 {sub_graph.number_of_nodes()}, 엣지 수 {sub_graph.number_of_edges()}\")\n",
    "    \n",
    "    if sub_graph.number_of_nodes() > 0:\n",
    "        visualize_graph(sub_graph, title=\"Selected Anatomy Knowledge Subgraph\")\n",
    "    else:\n",
    "        print(\"선택된 키워드와 관련된 노드가 없어 그래프를 시각화할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3fb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
