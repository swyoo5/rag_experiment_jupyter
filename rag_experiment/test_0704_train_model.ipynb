{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a45eeea",
   "metadata": {},
   "source": [
    "# AviationQA 학습 전 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/AviationQA.csv에 json파일 업슴\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'qwen2.5-1.5b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b: 0it [00:00, ?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [03:56<00:00, 16.92s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'deepseek-qwen-bllossom-32b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2\"\n",
    "\n",
    "model_list = [\n",
    "    # \"../model/LLM/gemma-7b\", \n",
    "    # \"../model/LLM/phi-mini-moe\", \n",
    "    \"../model/LLM/qwen2.5-1.5b\", \n",
    "    \"../model/LLM/deepseek-qwen-bllossom-32b\" \n",
    "]\n",
    "\n",
    "data_dir = \"../data/AviationQA.csv\"\n",
    "result_base_dir = \"../result/AviationQA\"\n",
    "\n",
    "MAX_QUESTIONS_PER_FILE = 10\n",
    "\n",
    "common_system_prompt = \"You are a helpful AI assistant.\"\n",
    "\n",
    "os.makedirs(result_base_dir, exist_ok=True)\n",
    "\n",
    "json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "if not json_files :\n",
    "    print(f\"{data_dir}에 json파일 업슴\")\n",
    "    exit()\n",
    "\n",
    "for model_path in model_list :\n",
    "    model_name = os.path.basename(model_path)\n",
    "    model_result_dir = os.path.join(result_base_dir, model_name)\n",
    "    os.makedirs(model_result_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.isdir(model_path) :\n",
    "        print(f\"오류: 로컬 경로 '{model_path}'에 모델 디렉토리가 존재하지 않습니다. 이 모델은 건너뜁니다.\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    pipe = None \n",
    "    try:\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_path,\n",
    "            torch_dtype=\"auto\", \n",
    "            device_map=\"auto\" \n",
    "        )\n",
    "        print(f\"모델 '{model_name}'이 성공적으로 로드되었습니다.\")\n",
    "        \n",
    "        for json_file in tqdm(json_files, desc=f\"Processing {model_name}\"):\n",
    "            file_name = os.path.basename(json_file).replace(\".json\", \"\")\n",
    "            output_file_path = os.path.join(model_result_dir, f\"{file_name}_responses.txt\")\n",
    "            \n",
    "            with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                outfile.write(f\"--- Model: {model_name} ---\\n\")\n",
    "                outfile.write(f\"--- Source Data File: {os.path.basename(json_file)} ---\\n\\n\")\n",
    "\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    qa_data = json.load(f)\n",
    "\n",
    "                for i, item in enumerate(qa_data):\n",
    "                    if i >= MAX_QUESTIONS_PER_FILE:\n",
    "                        break \n",
    "\n",
    "                    question = item.get(\"question\", \"\")\n",
    "                    correct_answer = item.get(\"answer\", \"[정답 답변 없음]\")\n",
    "                    user_content = f\"{common_system_prompt}\\n\\nQuestion: {question}\"\n",
    "\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": user_content}\n",
    "                    ]\n",
    "                    \n",
    "                    formatted_input = pipe.tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        outputs = pipe(\n",
    "                            formatted_input,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_p=0.9,\n",
    "                            eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "                            pad_token_id=pipe.tokenizer.pad_token_id if pipe.tokenizer.pad_token_id is not None else pipe.tokenizer.eos_token_id\n",
    "                        )\n",
    "\n",
    "                        model_response = outputs[0]['generated_text']\n",
    "                        cleaned_response = model_response[len(formatted_input):].strip()\n",
    "\n",
    "                        if not cleaned_response:\n",
    "                            cleaned_response = \"[모델이 답변을 생성하지 못했습니다]\"\n",
    "\n",
    "                    except Exception as gen_e:\n",
    "                        cleaned_response = f\"[답변 생성 중 오류 발생: {gen_e}]\"\n",
    "                        print(f\"경고: '{model_name}' 모델의 '{file_name}' 파일 '{question}' 질문 답변 생성 중 오류: {gen_e}\")\n",
    "\n",
    "                    outfile.write(f\"<질문 {i+1}>\\n\") \n",
    "                    outfile.write(f\"{question}\\n\\n\")\n",
    "                    outfile.write(f\"<답변>\\n\")\n",
    "                    outfile.write(f\"{cleaned_response}\\n\")\n",
    "                    outfile.write(f\"<정답 답변>\\n\")\n",
    "                    outfile.write(f\"{correct_answer}\\n\")\n",
    "\n",
    "                    outfile.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            print(f\"'{os.path.basename(json_file)}' 파일의 {MAX_QUESTIONS_PER_FILE}개 질문에 대한 답변을 저장했습니다: {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{model_name}' 모델을 로드하거나 초기화하는 중 문제가 발생했습니다: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if pipe is not None:\n",
    "            del pipe\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d9158",
   "metadata": {},
   "source": [
    "# AviationQA 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07263e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling, Trainer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b425c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 분할된 파일 ./train_data.json, ./test_data.json를 사용\n",
      "===== Training Start : ../model/LLM/gemma-7b =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d145389c1d64eb6ad7ea6e8176cf185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,275,136 || all params: 8,550,956,032 || trainable%: 0.1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1330703/3787177908.py:180: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoRA 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='8664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  11/8664 00:35 < 9:36:42, 0.25 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "class CustomTrainer(Trainer) :\n",
    "    def compute_loss(self, model, inputs, return_outputs = False, **kwargs) :\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = labels.to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "class QnADataset :\n",
    "    def __init__(self, data_paths, tokenizer, max_length=32786) :\n",
    "        self.data = self.load_data(data_paths)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_data(self, data_paths) :\n",
    "        all_loaded_data = []\n",
    "        for path in data_paths :\n",
    "            with open(path, 'r', encoding=\"utf-8\") as f :\n",
    "                data = json.load(f)\n",
    "                all_loaded_data.extend(data)\n",
    "        return all_loaded_data\n",
    "    \n",
    "    def prepare_input_output(self, item) :\n",
    "        input_text = f\"질문 : {item['question']}\\n문서 : {item['context']}\\n답변 : \"\n",
    "        output_text = item[\"answer\"]\n",
    "\n",
    "        return input_text, output_text\n",
    "    \n",
    "    def tokenize_data(self) :\n",
    "        tokenized_samples = []\n",
    "        for item in self.data :\n",
    "            input_text, output_text = self.prepare_input_output(item)\n",
    "\n",
    "            input_tokens_ids = self.tokenizer(\n",
    "                input_text,\n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            output_tokens_ids = self.tokenizer(\n",
    "                output_text,\n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            full_sequence_ids = input_tokens_ids + output_tokens_ids\n",
    "            labels = [-100] * len(input_tokens_ids) + output_tokens_ids\n",
    "\n",
    "            if self.tokenizer.eos_token_id is not None :\n",
    "                full_sequence_ids.append(self.tokenizer.eos_token_id)\n",
    "                labels.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "            if len(full_sequence_ids) > self.max_length :\n",
    "                full_sequence_ids = full_sequence_ids[:self.max_length]\n",
    "                labels = labels[:self.max_length]\n",
    "\n",
    "            attention_mask = [1] * len(full_sequence_ids)\n",
    "\n",
    "            tokenized_samples.append({\n",
    "                \"input_ids\" : full_sequence_ids,\n",
    "                \"labels\" : labels,\n",
    "                \"attention_mask\" : attention_mask\n",
    "            })\n",
    "\n",
    "        dataset = Dataset.from_list(tokenized_samples)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "def setup_model_and_tokenizer(model_name) :\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quants=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        # device_map={\"\":0},\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side=\"right\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_dora_config() :\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_dora=True\n",
    "    )\n",
    "\n",
    "    return lora_config\n",
    "\n",
    "def prepare_data(data_dir, train_path, test_path) :\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path) :\n",
    "        print(f\"이미 분할된 파일 {train_path}, {test_path}를 사용\")\n",
    "        return\n",
    "    \n",
    "    all_json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "    if not all_json_files :\n",
    "        raise FileNotFoundError(f\"Error : {e}\")\n",
    "    \n",
    "    all_data = []\n",
    "    for path in all_json_files :\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f :\n",
    "            data = json.load(f)\n",
    "            all_data.extend(data)\n",
    "\n",
    "    train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    with open(train_path, 'w', encoding=\"utf-8\") as f :\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    with open(test_path, 'w', encoding=\"utf-8\") as f :\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def main() :\n",
    "    model_list = [\n",
    "        \"../model/LLM/gemma-7b\", \n",
    "        \"../model/LLM/phi-mini-moe\", \n",
    "        \"../model/LLM/qwen2.5-1.5b\", \n",
    "        \"../model/LLM/deepseek-qwen-bllossom-32b\" \n",
    "    ]\n",
    "    \n",
    "    data_dir = \"../data/AviationQA.csv\"\n",
    "    train_data_path = \"./train_data.json\"\n",
    "    test_data_path = \"./test_data.json\"\n",
    "\n",
    "    prepare_data(data_dir, train_data_path, test_data_path)\n",
    "    \n",
    "    for model_name in model_list:\n",
    "        base_model_name_str = os.path.basename(model_name)\n",
    "        output_dir = f\"../model/finetuned-{base_model_name_str}-harrypotter\"\n",
    "\n",
    "        print(f\"===== Training Start : {model_name} =====\")\n",
    "        \n",
    "        model, tokenizer = setup_model_and_tokenizer(model_name)\n",
    "\n",
    "        if tokenizer.pad_token is None :\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        dora_config = setup_dora_config()\n",
    "        model = get_peft_model(model, dora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        dataset_handler = QnADataset([train_data_path], tokenizer)\n",
    "        train_dataset = dataset_handler.tokenize_data()\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=8,\n",
    "            num_train_epochs=3,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"no\",\n",
    "            warmup_steps=100,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        print(\"DoRA 시작\")\n",
    "        trainer.train()\n",
    "\n",
    "        trainer.save_model()\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"DoRA 파인튜닝 완료 : {output_dir}\")\n",
    "\n",
    "        del model, tokenizer, trainer, dataset_handler, train_dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\" : \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6f28d",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Inference Start : ../model/finetuned-gemma-7b-harrypotter ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12efb14b7ae649f9b9d8d55ba722450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '../model/finetuned-gemma-7b-harrypotter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/peft/config.py:257\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     config_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../model/finetuned-gemma-7b-harrypotter'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    100\u001b[39m         torch.cuda.empty_cache()\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     41\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     42\u001b[39m     bnb_4bit_use_double_quants=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     43\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m     bnb_4bit_compute_dtype=torch.float16\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     48\u001b[39m     model_name,\n\u001b[32m     49\u001b[39m     quantization_config=bnb_config,\n\u001b[32m     50\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n\u001b[32m     55\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/peft/peft_model.py:479\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    478\u001b[39m     config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mPeftConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubfolder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_auth_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     ].from_pretrained(model_id, **kwargs)\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[32m    489\u001b[39m     config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/peft/config.py:263\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m         config_file = hf_hub_download(\n\u001b[32m    258\u001b[39m             model_id,\n\u001b[32m    259\u001b[39m             CONFIG_NAME,\n\u001b[32m    260\u001b[39m             **hf_hub_download_kwargs,\n\u001b[32m    261\u001b[39m         )\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    265\u001b[39m loaded_attributes = \u001b[38;5;28mcls\u001b[39m.from_json_file(config_file)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[33m\"\u001b[39m\u001b[33mpeft_type\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Can't find 'adapter_config.json' at '../model/finetuned-gemma-7b-harrypotter'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "def main():\n",
    "    model_list = [\n",
    "        \"../model/LLM/gemma-7b\", \n",
    "        \"../model/LLM/phi-mini-moe\", \n",
    "        \"../model/LLM/qwen2.5-1.5b\", \n",
    "        \"../model/LLM/deepseek-qwen-bllossom-32b\" \n",
    "    ]\n",
    "    test_data_path = \"./test_data.json\"\n",
    "    \n",
    "    if not os.path.exists(test_data_path):\n",
    "        raise FileNotFoundError(f\"테스트 데이터 파일({test_data_path})이 없습니다. train.py를 먼저 실행하세요.\")\n",
    "\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    today = datetime.now()\n",
    "    date_folder_name = f\"{today.month}월{today.day}일\"\n",
    "    result_dir = os.path.join(\"../result\", date_folder_name)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    for model_name in model_list:\n",
    "        base_model_name_str = os.path.basename(model_name)\n",
    "        adapter_path = f\"../model/finetuned-{base_model_name_str}-aviation\"\n",
    "\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"\\n===== 스킵: {adapter_path} 경로에 학습된 모델이 없습니다. =====\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*20} Inference Start : {adapter_path} {'='*20}\")\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quants=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "        \n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        inference_results = []\n",
    "        \n",
    "        for item in test_data:\n",
    "            user_content = f\"질문 : {item['question']}\\n문서 : {item['context']}\"\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            \n",
    "            # 파이프라인의 토크나이저를 사용하여 챗 템플릿 적용\n",
    "            formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # 파이프라인으로 텍스트 생성\n",
    "            outputs = pipe(\n",
    "                formatted_prompt,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "                pad_token_id=pipe.tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            # 생성된 부분만 추출\n",
    "            full_response = outputs[0]['generated_text']\n",
    "            generated_answer = full_response[len(formatted_prompt):].strip()\n",
    "\n",
    "            print(\"\\n---\")\n",
    "            print(f\"질문: {item['question']}\")\n",
    "            print(f\"정답: {item['answer']}\")\n",
    "            print(f\"모델 생성 답변: {generated_answer}\")\n",
    "            print(\"---\")\n",
    "\n",
    "            result_entry = {\n",
    "                \"question\": item['question'],\n",
    "                \"ground_truth_answer\": item['answer'],\n",
    "                \"generated_answer\": generated_answer\n",
    "            }\n",
    "            inference_results.append(result_entry)\n",
    "\n",
    "        file_name = f\"{base_model_name_str}_results.json\"\n",
    "        save_path = os.path.join(result_dir, file_name)\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(inference_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\n추론 결과 저장 완료: {save_path}\")\n",
    "\n",
    "        del model, tokenizer, pipe\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
