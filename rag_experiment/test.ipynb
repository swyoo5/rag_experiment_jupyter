{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90799b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e9e8623c7d4d989c4da5c8c2623d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20646e29bec4b5998a21d1eefbb2c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from search import *\n",
    "from model_loader.config import *\n",
    "from save_utils import *\n",
    "import time\n",
    "\n",
    "class CarManualQA:\n",
    "    def __init__(self, generation_loader, data_folder=\"./data/split_file\", prompt_path=\"./prompts/ko/generation/cogito-qwen/generation_prompt2.txt\", result_path=\"./result\"):\n",
    "        self.data_folder = data_folder\n",
    "        self.result_path = result_path\n",
    "        self.searcher = HybridSearcher(embedding_loader, chunk_size=200, chunk_overlap=50)\n",
    "        self.loader = generation_loader\n",
    "        self.prompt_path = prompt_path\n",
    "        self.prompt_template = self._load_prompt(prompt_path)\n",
    "        self.category_to_file = {\n",
    "            '1': \"1_키.txt\",  # 1~13, 73\n",
    "            '2': \"2_중앙_자동_잠금장치.txt\",  # 14~18\n",
    "            '4': \"3_도어.txt\",  # 19~22\n",
    "            '5': \"4_차량안전.txt\",  # 22~25\n",
    "            '6': \"5_미러.txt\",  # 26~29\n",
    "            '7': \"6_유리창_및_루프.txt\",  # 40~46\n",
    "            '9': \"7_헤드레스트_시트.txt\",  # 47~54, 74~78\n",
    "            '11': \"8_안전벨트.txt\",  # 54~59\n",
    "            '12': \"9_에어백.txt\",  # 60~68\n",
    "            '13': \"10_어린이_시트.txt\",  # 68~72\n",
    "            '14': \"11_운전석.txt\",  # 79~90\n",
    "            '15': \"12_하이패스.txt\"  # 29~40\n",
    "        }\n",
    "\n",
    "    def generate_response(self, query, category, top_n=5, alpha=0.5):\n",
    "        time_logs = {}\n",
    "        total_start = time.time()\n",
    "        \n",
    "        # 단계 1: 문서 로드\n",
    "        t_start = time.time()\n",
    "        filename = self.category_to_file[category]\n",
    "        file_path = os.path.join(self.data_folder, filename)\n",
    "        self.searcher.load_document(file_path)\n",
    "        time_logs['1_문서_로드'] = time.time() - t_start\n",
    "        print(f\"1. 문서 로드: {time_logs['1_문서_로드']:.2f}초\")\n",
    "        \n",
    "        # 단계 2: 검색 실행\n",
    "        t_start = time.time()\n",
    "        search_results = self.searcher.search(query, top_n=top_n, alpha=alpha)\n",
    "        time_logs['2_검색_실행'] = time.time() - t_start\n",
    "        print(f\"2. 검색 실행: {time_logs['2_검색_실행']:.2f}초\")\n",
    "        \n",
    "        # 단계 3: 컨텍스트 준비\n",
    "        t_start = time.time()\n",
    "        context = \"\\n\\n\".join([result[\"chunk\"] for result in search_results])\n",
    "        prompt = self.prompt_template.format(context=context, context_qa=\"\", query=query)\n",
    "        time_logs['3_컨텍스트_준비'] = time.time() - t_start\n",
    "        print(f\"3. 컨텍스트 준비: {time_logs['3_컨텍스트_준비']:.2f}초\")\n",
    "        \n",
    "        # 단계 4: 토크나이징\n",
    "        t_start = time.time()\n",
    "        if hasattr(self.loader, \"tokenizer\"):\n",
    "            tokenizer = self.loader.tokenizer\n",
    "            model = self.loader.model\n",
    "            # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = input_ids.repeat(2, 1).to(model.device)  # 배치 크기 2로 설정\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "            time_logs['4_토크나이징'] = time.time() - t_start\n",
    "            print(f\"4. 토크나이징: {time_logs['4_토크나이징']:.2f}초\")\n",
    "            \n",
    "            # 단계 5: 모델 생성 (가장 시간이 많이 걸릴 것으로 예상)\n",
    "            t_start = time.time()\n",
    "            t_start = time.time()\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2,\n",
    "                early_stopping=True,\n",
    "                num_beams=3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            time_logs['5_모델_생성'] = time.time() - t_start\n",
    "            print(f\"5. 모델 생성: {time_logs['5_모델_생성']:.2f}초\")\n",
    "            \n",
    "            # 단계 6: 디코딩\n",
    "            t_start = time.time()\n",
    "            generated_ids = output[0][input_ids.shape[-1]:]\n",
    "            raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            time_logs['6_디코딩'] = time.time() - t_start\n",
    "            print(f\"6. 디코딩: {time_logs['6_디코딩']:.2f}초\")\n",
    "        else:\n",
    "            # API 모델 사용 시\n",
    "            t_start = time.time()\n",
    "            raw_answer = self.loader.generate(prompt)\n",
    "            time_logs['4_API_생성'] = time.time() - t_start\n",
    "            print(f\"4. API 생성: {time_logs['4_API_생성']:.2f}초\")\n",
    "\n",
    "        # 단계 7: 후처리\n",
    "        t_start = time.time()\n",
    "        final_answer = self._extract_answer_content(raw_answer)\n",
    "        final_answer = self._remove_chinese_characters(final_answer)\n",
    "        time_logs['7_후처리'] = time.time() - t_start\n",
    "        print(f\"7. 후처리: {time_logs['7_후처리']:.2f}초\")\n",
    "        \n",
    "        # 단계 8: 결과 저장\n",
    "        t_start = time.time()\n",
    "        response = {\n",
    "            \"답변\": raw_answer,\n",
    "            \"후처리\": final_answer,\n",
    "            \"문서 일부\": context,\n",
    "            \"question_en\": query,\n",
    "            \"answer_en\": raw_answer,\n",
    "            \"검색_결과\": search_results,\n",
    "            \"시간_로그\": time_logs\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            result_path = getattr(self, 'result_path', '../result')\n",
    "            alpha_str = f\"{alpha:.1f}\"\n",
    "            alpha_folder = os.path.join(result_path, f\"alpha_{alpha_str}\")\n",
    "            os.makedirs(alpha_folder, exist_ok=True)\n",
    "            \n",
    "            save_response_to_file(\n",
    "                query=query,\n",
    "                answer=response[\"답변\"],\n",
    "                final_answer=response[\"후처리\"],\n",
    "                context=response[\"문서 일부\"],\n",
    "                folder=alpha_folder\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"결과 저장 중 오류 발생: {e}\")\n",
    "        \n",
    "        time_logs['8_결과_저장'] = time.time() - t_start\n",
    "        print(f\"8. 결과 저장: {time_logs['8_결과_저장']:.2f}초\")\n",
    "        \n",
    "        # 총 소요 시간\n",
    "        total_time = time.time() - total_start\n",
    "        time_logs['총_소요_시간'] = total_time\n",
    "        print(f\"총 소요 시간: {total_time:.2f}초\")\n",
    "        \n",
    "        # 각 단계별 비율 출력\n",
    "        print(\"\\n각 단계별 시간 비율:\")\n",
    "        for key, value in time_logs.items():\n",
    "            if key != '총_소요_시간':\n",
    "                percentage = (value / total_time) * 100\n",
    "                print(f\"  {key}: {percentage:.1f}%\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _load_prompt(self, path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 프롬프트 로드 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_answer_content(self, text):\n",
    "        pattern = r\"(?:<\\|?|<|)?\\|?answer\\|?(?:\\|?>|>)?(.*?)(?:<\\|?|<|)?\\|?endanswer\\|?(?:\\|?>|>)?\"\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(1).strip() if match else text.strip()\n",
    "    \n",
    "    def _remove_chinese_characters(self, text):\n",
    "        return re.sub(r'[\\u4E00-\\u9FFF]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c86b65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 애가 문 못열게 하는거 어케함?\n",
      "카테고리: 2\n",
      "알파: 0.5\n",
      "--------------------------------------------------\n",
      "1. 문서 로드: 0.11초\n",
      "2. 검색 실행: 0.04초\n",
      "3. 컨텍스트 준비: 0.00초\n",
      "4. 토크나이징: 0.01초\n",
      "5. 모델 생성: 167.49초\n",
      "6. 디코딩: 0.00초\n",
      "7. 후처리: 0.00초\n",
      "8. 결과 저장: 0.00초\n",
      "총 소요 시간: 167.65초\n",
      "\n",
      "각 단계별 시간 비율:\n",
      "  1_문서_로드: 0.1%\n",
      "  2_검색_실행: 0.0%\n",
      "  3_컨텍스트_준비: 0.0%\n",
      "  4_토크나이징: 0.0%\n",
      "  5_모델_생성: 99.9%\n",
      "  6_디코딩: 0.0%\n",
      "  7_후처리: 0.0%\n",
      "  8_결과_저장: 0.0%\n",
      "\n",
      "생성된 답변:\n",
      "참고 이미지 : \n",
      "![어린이 안전 잠금 장치](./image/image_18_1.png)\n",
      "\n",
      "18페이지에 따르면, 뒤쪽 도어의 어린이 안전 잠금 장치를 위쪽으로 올려 잠그면 도어는 실내에서 열 수 없습니다. 기능을 해제하려면 실외 열림 레버로 뒤쪽 도어를 열고 어린이 안전 잠금 장치를 해제 방향으로 내리면 됩니다.\n",
      "\n",
      "주의: 어린이 안전 도어 잠금 장치가 LOCK 위치에 있을 때에는 실내 도어 손잡이를 당기지 마십시오. 실내 도어 손잡이가 손상될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from model_loader.config import generation_loader\n",
    "\n",
    "# CarManualQA 시스템 초기화\n",
    "qa_system = CarManualQA(generation_loader)\n",
    "\n",
    "# 테스트할 질문과 카테고리 선택\n",
    "question = \"애가 문 못열게 하는거 어케함?\"\n",
    "category = '2'\n",
    "alpha = 0.5\n",
    "\n",
    "print(f\"질문: {question}\")\n",
    "print(f\"카테고리: {category}\")\n",
    "print(f\"알파: {alpha}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# generate_response 함수 실행 (내부에서 시간 측정 및 출력됨)\n",
    "result = qa_system.generate_response(question, category, alpha=alpha)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"\\n생성된 답변:\")\n",
    "print(result[\"후처리\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a009f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "_translation_model = None\n",
    "_translation_tokenizer = None\n",
    "\n",
    "MODEL_NAME = \"../model/translate/facebook-m2m100\"\n",
    "\n",
    "def get_translation_model():\n",
    "    global _translation_model, _translation_tokenizer\n",
    "    if _translation_model is None or _translation_tokenizer is None:\n",
    "        _translation_tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
    "        _translation_model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    return _translation_tokenizer, _translation_model\n",
    "\n",
    "def translate_en_to_ko(text):\n",
    "    tokenizer, model = get_translation_model()\n",
    "    tokenizer.src_lang = \"en\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23ffc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from model_loader.config import embedding_loader\n",
    "\n",
    "class HybridSearcher :\n",
    "    def __init__(self, embedding_loader, chunk_size=200, chunk_overlap=50) :\n",
    "        self.embedding_model = embedding_loader\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.chunks = None\n",
    "        self.chunk_metadata = None\n",
    "        self.bm25_index = None\n",
    "        self.vector_index = None\n",
    "\n",
    "    def load_document(self, file_path):\n",
    "        \"\"\"문서 로드 및 청크 분할\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 문서를 청크로 분할\n",
    "        self.chunks, self.chunk_metadata = self._split_into_chunks_with_metadata(content)\n",
    "        \n",
    "        # BM25 인덱스 생성\n",
    "        tokenized_chunks = [self._simple_tokenize(chunk) for chunk in self.chunks]\n",
    "        self.bm25_index = BM25Okapi(tokenized_chunks)\n",
    "        \n",
    "        # 벡터 임베딩 생성 및 인덱스 구축\n",
    "        self.vector_index = self.embedding_model.encode(self.chunks)\n",
    "        \n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def search(self, query, top_n=5, alpha=0.5):\n",
    "        \"\"\"하이브리드 검색 수행\"\"\"\n",
    "        if self.chunks is None or self.bm25_index is None or self.vector_index is None:\n",
    "            raise ValueError(\"문서가 로드되지 않았습니다. load_document()를 먼저 호출하세요.\")\n",
    "        \n",
    "        # BM25 검색 수행\n",
    "        bm25_scores = self.bm25_index.get_scores(self._simple_tokenize(query))\n",
    "        \n",
    "        # 벡터 검색 수행\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        vector_scores = cosine_similarity([query_embedding], self.vector_index)[0]\n",
    "        \n",
    "        # 검색 결과 결합\n",
    "        combined_scores = self._combine_scores(bm25_scores, vector_scores, alpha)\n",
    "        \n",
    "        # 상위 N개 결과 반환\n",
    "        top_indices = np.argsort(combined_scores)[-top_n:][::-1]\n",
    "        results = [\n",
    "            {\n",
    "                \"chunk\": self.chunks[i],\n",
    "                \"score\": combined_scores[i],\n",
    "                \"bm25_score\": bm25_scores[i],\n",
    "                \"vector_score\": vector_scores[i],\n",
    "                \"index\": i,\n",
    "                \"page\" : self.chunk_metadata[i][\"page\"]\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _split_into_chunks_with_metadata(self, text):\n",
    "        \"\"\"텍스트를 청크로 분할하고 페이지 정보를 메타데이터로 유지하는 함수\"\"\"\n",
    "        chunks = []\n",
    "        chunk_metadata = []\n",
    "        \n",
    "        # 페이지 패턴 정규식 (####으로 시작하는 페이지 헤더)\n",
    "        page_pattern = re.compile(r'####\\s*(\\d+)페이지')\n",
    "        \n",
    "        # 텍스트를 줄 단위로 처리\n",
    "        lines = text.split('\\n')\n",
    "        current_page = \"unknown\"\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            # 페이지 헤더 확인\n",
    "            page_match = page_pattern.match(line)\n",
    "            \n",
    "            if page_match:\n",
    "                # 새 페이지 시작\n",
    "                # 현재 청크가 있으면 저장\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    chunk_metadata.append({\"page\": current_page})\n",
    "                    current_chunk = \"\"\n",
    "                \n",
    "                # 새 페이지 번호 설정\n",
    "                current_page = page_match.group(1)\n",
    "                continue\n",
    "            \n",
    "            # 현재 청크에 라인 추가\n",
    "            current_chunk += line + \"\\n\"\n",
    "            \n",
    "            # 청크 크기 확인\n",
    "            if len(current_chunk) >= self.chunk_size:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                chunk_metadata.append({\"page\": current_page})\n",
    "                current_chunk = \"\"  # 새 청크 시작\n",
    "        \n",
    "        # 마지막 청크 처리\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "            chunk_metadata.append({\"page\": current_page})\n",
    "        \n",
    "        # 너무 작은 청크 결합 (메타데이터 유지)\n",
    "        i = 0\n",
    "        while i < len(chunks) - 1:\n",
    "            if len(chunks[i]) + len(chunks[i+1]) < self.chunk_size:\n",
    "                # 같은 페이지인 경우에만 결합\n",
    "                if chunk_metadata[i][\"page\"] == chunk_metadata[i+1][\"page\"]:\n",
    "                    chunks[i] = chunks[i] + \"\\n\\n\" + chunks[i+1]\n",
    "                    chunks.pop(i+1)\n",
    "                    chunk_metadata.pop(i+1)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return chunks, chunk_metadata\n",
    "    \n",
    "    def _simple_tokenize(self, text):\n",
    "        \"\"\"텍스트를 간단히 토크나이징하는 함수\"\"\"\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    def _combine_scores(self, bm25_scores, vector_scores, alpha=0.5):\n",
    "        \"\"\"BM25와 벡터 검색 점수를 결합\"\"\"\n",
    "        # 점수 정규화\n",
    "        if np.max(bm25_scores) > 0:\n",
    "            bm25_scores = bm25_scores / np.max(bm25_scores)\n",
    "        if np.max(vector_scores) > 0:\n",
    "            vector_scores = vector_scores / np.max(vector_scores)\n",
    "        \n",
    "        # 가중 평균 계산\n",
    "        combined = alpha * bm25_scores + (1 - alpha) * vector_scores\n",
    "        return combined\n",
    "    \n",
    "    def get_chunks_with_page_info(self, indices=None) :\n",
    "        if indices is None :\n",
    "            return [(chunk, self.chunk_metadata[i][\"page\"]) for i, chunk in enumerate(self.chunks)]\n",
    "        else :\n",
    "            return [(self.chunks[i], self.chunk_metadata[i][\"page\"]) for i in indices if i < len(self.chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b00c5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader.config import *\n",
    "from save_utils import *\n",
    "\n",
    "class CarManualQA:\n",
    "    def __init__(self, generation_loader, data_folder=\"./data/split_file\", \n",
    "                 ko_prompt_path=\"./prompts/ko/generation/gemma3/generation_prompt.txt\", \n",
    "                 en_prompt_path=\"./prompts/ko/generation/gemma3/generation_prompt.txt\", \n",
    "                 result_path=\"./result/5월9일/gemma3_full\"):\n",
    "        self.data_folder = data_folder\n",
    "        self.result_path = result_path\n",
    "        self.searcher = HybridSearcher(embedding_loader, chunk_size=200, chunk_overlap=50)\n",
    "        self.loader = generation_loader\n",
    "        self.ko_prompt_path = ko_prompt_path\n",
    "        self.en_prompt_path = en_prompt_path\n",
    "        self.ko_prompt_template = self._load_prompt(ko_prompt_path)\n",
    "        self.en_prompt_template = self._load_prompt(en_prompt_path)\n",
    "        self.category_to_file = {\n",
    "            '1': \"1_키.txt\",  # 1~13, 73\n",
    "            '2': \"2_중앙_자동_잠금장치.txt\",  # 14~18\n",
    "            '4': \"3_도어.txt\",  # 19~22\n",
    "            '5': \"4_차량안전.txt\",  # 22~25\n",
    "            '6': \"5_미러.txt\",  # 26~29\n",
    "            '7': \"6_유리창_및_루프.txt\",  # 40~46\n",
    "            '9': \"7_헤드레스트_시트.txt\",  # 47~54, 74~78\n",
    "            '11': \"8_안전벨트.txt\",  # 54~59\n",
    "            '12': \"9_에어백.txt\",  # 60~68\n",
    "            '13': \"10_어린이_시트.txt\",  # 68~72\n",
    "            '14': \"11_운전석.txt\",  # 79~90\n",
    "            '15': \"12_하이패스.txt\",  # 29~40,\n",
    "            \"16\": \"full.txt\"\n",
    "        }\n",
    "\n",
    "    def generate_response(self, query, category, top_n=5, alpha=0.5, language=\"ko\"):\n",
    "        # 검색 파트\n",
    "        filename = self.category_to_file[category]\n",
    "        file_path = os.path.join(self.data_folder, filename)\n",
    "\n",
    "        self.searcher.load_document(file_path)\n",
    "        search_results = self.searcher.search(query, top_n=top_n, alpha=alpha)\n",
    "        \n",
    "        context = \"\\n\\n\".join([f\"#### {result['page']}페이지\\n{result['chunk']}\" for result in search_results])\n",
    "        \n",
    "        # 응답 생성 파트\n",
    "        if language.lower() == \"ko\" :\n",
    "            print(\"번역 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "            prompt_template = self.ko_prompt_template\n",
    "        else :\n",
    "            print(\"번역 OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\")\n",
    "            prompt_template = self.en_prompt_template\n",
    "        \n",
    "        prompt = prompt_template.format(context=context, context_qa=\"\", query=query)\n",
    "\n",
    "        if hasattr(self.loader, \"tokenizer\"):\n",
    "            tokenizer = self.loader.tokenizer\n",
    "            model = self.loader.model\n",
    "\n",
    "            # 인풋 토크나이즈\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "            \n",
    "            # 텍스트 생성\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=400,\n",
    "                temperature=0.3,\n",
    "                do_sample=False,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2,\n",
    "                early_stopping=True,\n",
    "                num_beams=3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            generated_ids = output[0][input_ids.shape[-1]:]\n",
    "            raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        else:\n",
    "            raw_answer = self.loader.generate(prompt)\n",
    "\n",
    "        final_answer = self._extract_answer_content(raw_answer)\n",
    "        final_answer = self._remove_chinese_characters(final_answer)\n",
    "        \n",
    "        translated_answer = None\n",
    "        if language.lower != \"ko\" :\n",
    "            try :\n",
    "                translated_answer = translate_en_to_ko(final_answer)\n",
    "            except Exception as e :\n",
    "                print(f\"번역 중 오류 발생 : {e}\")\n",
    "                translated_answer = f\"[번역 실패] {final_answer}\"\n",
    "\n",
    "        # 결과 준비\n",
    "        response = {\n",
    "            \"답변\": raw_answer,\n",
    "            \"후처리\": final_answer,\n",
    "            \"문서 일부\": context,\n",
    "            \"question_en\": query,\n",
    "            \"answer_en\": raw_answer,\n",
    "            \"검색_결과\": search_results,\n",
    "            \"번역\" : translated_answer if language.lower() != \"ko\" else None\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # result_path가 없으면 상위 스코프나 기본값으로 설정\n",
    "            result_path = getattr(self, 'result_path', '../result')\n",
    "            \n",
    "            # 알파값으로 폴더 경로 생성\n",
    "            alpha_str = f\"{alpha:.1f}\"\n",
    "            alpha_folder = os.path.join(result_path, f\"alpha_{alpha_str}\")\n",
    "            \n",
    "            # 폴더가 없으면 생성\n",
    "            os.makedirs(alpha_folder, exist_ok=True)\n",
    "            \n",
    "            # 파일 저장 시도\n",
    "            print(f\"저장 시도: {alpha_folder}\")  # 디버깅용\n",
    "            save_response_to_file(\n",
    "                query=query,\n",
    "                answer=response[\"답변\"],\n",
    "                final_answer=response[\"후처리\"] if language.lower() == \"ko\" else response[\"번역\"],\n",
    "                context=response[\"문서 일부\"],\n",
    "                folder=alpha_folder\n",
    "            )\n",
    "            print(f\"저장 완료: {alpha_folder}\")  # 디버깅용\n",
    "        except Exception as e:\n",
    "            print(f\"결과 저장 중 오류 발생: {e}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _load_prompt(self, path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 프롬프트 로드 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_answer_content(self, text):\n",
    "        pattern = r\"(?:<\\|?|<|)?\\|?answer\\|?(?:\\|?>|>)?(.*?)(?:<\\|?|<|)?\\|?endanswer\\|?(?:\\|?>|>)?\"\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(1).strip() if match else text.strip()\n",
    "    \n",
    "    def _remove_chinese_characters(self, text):\n",
    "        return re.sub(r'[\\u4E00-\\u9FFF]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02df3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Testing with alpha=0.0 =====\n",
      "\n",
      "새 쿼리: 애가 문 못열게 하는거 어케함?, 카테고리: 16\n",
      "번역 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 중 오류 발생 : Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../model/translate/facebook-m2m100'. Use `repo_type` argument if needed.\n",
      "저장 시도: ./result/5월9일/gemma3_full/alpha_0.0\n",
      "저장 완료: ./result/5월9일/gemma3_full/alpha_0.0\n",
      "추론시간 : 279.2659065723419\n",
      "LLM 답변: [답변]\n",
      "18페이지에 따르면, 뒤쪽 도어의 어린이 안전 잠금 장치를 위쪽으로 올려 잠그면 도어는 실내에서 열 수 없습니다. 기능을 해제하려면 실외 열림 레버로 뒤쪽 도어를 열고 어린이 안전 잠금 장치를 해제 방향으로 내리면 됩니다.\n",
      "\n",
      "[문서]\n",
      "#### 65페이지\n",
      "어린이가 도어에 기대거나 측면 에어백 모듈에 가까이 있지 않도록 하십시오.\n",
      "\n",
      "#### 20페이지\n",
      "참고\n",
      "도어의 키 홈이 결빙되어 열리지 않을 경우에는 살짝 두드리거나 키를 뜨겁게 만든 후 여시기 바랍니다.\n",
      "\n",
      "주의\n",
      "차량을 주차 또는 정차시키고 떠날때에는 모든 도어를 잠그고 키를 소지하십시오. 그렇게 하지 않으면 차량을 도난당할 수 있습니다.\n",
      "\n",
      "경고\n",
      "여름철에 모든 유리창을 닫은 상태에서 어린이, 장애우, 애완동물 등을 차량 안에 두고 절대로 떠나지 마십시오. 차량 실내 온도가 실외보다 더 높게 상승하기 때문에 부상을 입거나 생명을 잃을 수 있습니다.\n",
      "\n",
      "#### 18페이지\n",
      "자동도어 잠금해제\n",
      "차량 충돌 시(감지 센서에 충격 전달시) 도어 잠금을 자동으로 해제시키는기능이 있습니다.\n",
      "단, 도어 잠금 해제와 관련된 기계적인 장치나 배터리에 문제가 있을 때에는 도어가 잠금\n",
      "\n",
      "새 쿼리: 운전석에서 문 다 잠그는거 할 수 있나?, 카테고리: 16\n",
      "번역 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from model_loader.config import generation_loader\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "question_list = [\n",
    "                \"애가 문 못열게 하는거 어케함?\", \n",
    "                \"운전석에서 문 다 잠그는거 할 수 있나?\",\n",
    "                \"차키 배터리 뭐사야돼?\",\n",
    "                \"하이패스에 빨간불 들어오는데 왜이럼?\",\n",
    "                \"시트 너무 낮은데 어떻게 조절함?\",\n",
    "                \"창문 올라가다 마는데 이거 왜이래\",\n",
    "                \"도난방지인가 그거 어떻게 끄냐?\",\n",
    "                \"카드 꽂았는데 하이패스 안되는데 어케함\",\n",
    "                \"열선 버튼 누르면 언제 꺼지냐?\",\n",
    "                \"스마트키 물에 빠졌는데 써도돼?\",\n",
    "                \"하이패스 후불로 하고싶은데 어디다가 말해야대?\",\n",
    "                \"차키 안에 두고 내려서 다른 키로 열었는데 안에 있던 키 작동이 안돼.\",\n",
    "                \"차키 배터리 어따가 버려야되냐? 일반쓰레기인가?\",\n",
    "                \"차 달리다가 자동으로 잠기는거 몇킬로로 달릴때 잠기냐?\",\n",
    "                \"지금 차 친구한테 팔려고 하는데 안에 하이패스는 어케 해야되냐\",\n",
    "                \"하이패스에서 카드를 확인하십시오 라고 나오는데 뭐가문제야\",\n",
    "                \"하이패스 등록하려는데 서류 뭐내야돼?\",\n",
    "                \"뒷자리 애가 창문열고 장난치는데 창문 잠그는방법 없어?\",\n",
    "                \"임산부는 벨트 매야되나? 위험하지않나?\",\n",
    "                \"차 산지 5년 다되가는데 에어백도 점검해야되나?\"\n",
    "                ]\n",
    "category_list = ['16'] * 20\n",
    "alpha_values = np.arange(0.0, 1.1, 0.1)\n",
    "\n",
    "generation_loader = generation_loader\n",
    "result_base_path = \"../result\"\n",
    "qa_system = CarManualQA(generation_loader)\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # 소수점 첫째자리까지 표현하기 위해 포맷팅\n",
    "    alpha_formatted = round(alpha, 1)\n",
    "    print(f\"\\n===== Testing with alpha={alpha_formatted} =====\")\n",
    "    \n",
    "    for q, c in zip(question_list, category_list):\n",
    "        print(f\"\\n새 쿼리: {q}, 카테고리: {c}\")\n",
    "        \n",
    "        \n",
    "        # 검색 결과 가져오기 (alpha 값 전달)\n",
    "        # search_results = test_search_only(q, c, alpha=alpha_formatted)\n",
    "        # context = \"\\n\\n\".join([result[\"chunk\"] for result in search_results])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # 응답 생성 (alpha 값 전달)\n",
    "        result = qa_system.generate_response(q, c, alpha=alpha_formatted, language=\"ko\")\n",
    "        end_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"추론시간 : {elapsed_time}\")\n",
    "        print(f\"LLM 답변: {result['후처리']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f522cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
