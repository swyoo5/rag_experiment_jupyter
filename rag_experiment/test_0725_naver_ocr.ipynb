{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bab68b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preprocessed_pages :\n\u001b[32m     39\u001b[39m     first_page = preprocessed_pages[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     encoding = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboxes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad() :\n\u001b[32m     50\u001b[39m         outputs = model(**encoding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/models/layoutlmv3/processing_layoutlmv3.py:105\u001b[39m, in \u001b[36mLayoutLMv3Processor.__call__\u001b[39m\u001b[34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# verify input\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_processor.apply_ocr \u001b[38;5;129;01mand\u001b[39;00m (boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_processor.apply_ocr \u001b[38;5;129;01mand\u001b[39;00m (word_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    111\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot provide word labels if you initialized the image processor with apply_ocr set to True.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: You cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import os\n",
    "\n",
    "def preprocess_pdf_for_vqa(pdf_path: str) :\n",
    "    if not os.path.exists(pdf_path) :\n",
    "        raise FileNotFoundError(f\"PDF 파일을 찾을 수 없습니다 : {pdf_path}\")\n",
    "    images = convert_from_path(pdf_path)\n",
    "    processed_pages = []\n",
    "\n",
    "    for i, image in enumerate(images) :\n",
    "        ocr_data = pytesseract.image_to_data(image, output_type=Output.DICT)\n",
    "        words, boxes = [], []\n",
    "        for j in range(len(ocr_data[\"text\"])) :\n",
    "            if int(ocr_data[\"conf\"][j]) > 50 and ocr_data[\"text\"][j].strip() != '' :\n",
    "                words.append(ocr_data[\"text\"][j])\n",
    "                x, y, w, h = ocr_data[\"left\"][j], ocr_data[\"top\"][j], ocr_data[\"width\"][j], ocr_data[\"height\"][j]\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "        processed_pages.append({\"image\": image.convert(\"RGB\"), \"words\": words, \"boxes\": boxes})\n",
    "    return processed_pages\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "\n",
    "pdf_file_path = \"./data/GrayAnatomyPDF.pdf\"\n",
    "question = \"what is the title of the book?\"\n",
    "\n",
    "try :\n",
    "    preprocessed_pages = preprocess_pdf_for_vqa(pdf_file_path)\n",
    "except FileNotFoundError as e :\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "if preprocessed_pages :\n",
    "    first_page = preprocessed_pages[0]\n",
    "\n",
    "    encoding = processor(\n",
    "        first_page[\"image\"],\n",
    "        question,\n",
    "        words=first_page[\"words\"],\n",
    "        boxes=first_page[\"boxes\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    print(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d3d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
