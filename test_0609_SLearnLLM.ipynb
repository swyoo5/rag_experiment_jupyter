{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35deaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "# from datasets import Dataset\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cced7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # llm_judge.py\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from model_loader.config import get_model_loader \n",
    "\n",
    "# class LLMJudge:\n",
    "#     def __init__(self, judge_llm_name: str, max_seq_length: int = 512):\n",
    "#         print(f\"판단 모델 로딩 중: {judge_llm_name} (get_model_loader 사용)...\")\n",
    "        \n",
    "#         judge_quantization_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\"\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             judge_loader_instance = get_model_loader(\n",
    "#                 provider=\"local\",\n",
    "#                 model_path=judge_llm_name,\n",
    "#                 quantization_config=judge_quantization_config,\n",
    "#                 torch_dtype=torch.bfloat16\n",
    "#             )\n",
    "#             self.tokenizer = judge_loader_instance.tokenizer\n",
    "#             self.model = judge_loader_instance.model\n",
    "#         except Exception as e:\n",
    "#             print(f\"판단 모델 로더에서 오류 발생: {e}\")\n",
    "#             raise\n",
    "\n",
    "#         if self.tokenizer.pad_token is None:\n",
    "#             self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "#             self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval() \n",
    "#         self.max_seq_length = max_seq_length\n",
    "\n",
    "#     def _create_judge_prompt(self, question: str, context: str, generated_answer: str, true_answer: str) -> str:\n",
    "#         return (\n",
    "#             f\"다음은 질문, 문서, 생성된 답변, 그리고 정답입니다.\\n\"\n",
    "#             f\"생성된 답변이 정답과 의미적으로 동일하거나 정확한지 판단하여 '정답' 또는 '오답'으로만 답변해주세요.\\n\\n\"\n",
    "#             f\"질문: {question}\\n\"\n",
    "#             f\"문서: {context}\\n\"\n",
    "#             f\"생성된 답변: {generated_answer}\\n\"\n",
    "#             f\"정답: {true_answer}\\n\\n\"\n",
    "#             f\"판단: \"\n",
    "#         )\n",
    "\n",
    "#     def is_correct(self, question: str, context: str, generated_answer: str, true_answer: str) -> bool:\n",
    "#         judge_prompt = self._create_judge_prompt(question, context, generated_answer, true_answer)\n",
    "        \n",
    "#         inputs = self.tokenizer(judge_prompt, return_tensors=\"pt\", \n",
    "#                                 max_length=self.max_seq_length, truncation=True).to(self.device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_new_tokens=10, \n",
    "#                 num_beams=1,\n",
    "#                 do_sample=False,\n",
    "#                 temperature=0.0,\n",
    "#                 pad_token_id=self.tokenizer.pad_token_id,\n",
    "#                 eos_token_id=self.tokenizer.eos_token_id\n",
    "#             )\n",
    "        \n",
    "#         judge_result = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], \n",
    "#                                               skip_special_tokens=True).strip().lower()\n",
    "\n",
    "#         if \"오답\" in judge_result or \"틀림\" in judge_result or \"incorrect\" in judge_result:\n",
    "#             return False\n",
    "#         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b16387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # slearn_llm.py\n",
    "# import json\n",
    "# from transformers import TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "# from datasets import Dataset\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import os \n",
    "\n",
    "# from model_loader.config import get_model_loader, generation_loader\n",
    "\n",
    "\n",
    "# def slearn_llm_training(\n",
    "#     model_name: str = \"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "#     data_path: str = \"./data/qna_data.json\",\n",
    "#     output_dir: str = \"./model\",\n",
    "#     max_seq_length: int = 512,\n",
    "#     batch_size: int = 4,\n",
    "#     learning_rate: float = 2e-5,\n",
    "#     num_train_epochs: int = 3,\n",
    "#     eval_steps: int = 500,\n",
    "#     save_steps: int = 500,\n",
    "#     judge_llm_name: str = \"google/gemma-2b\"\n",
    "# ):\n",
    "#     print(f\"학습 모델 로딩 중: {model_name} (model_loader.config.generation_loader 사용)...\")\n",
    "#     tokenizer = generation_loader.tokenizer\n",
    "#     model = generation_loader.model\n",
    "\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#         model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "#     model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "#     llm_judge = LLMJudge(judge_llm_name=judge_llm_name, max_seq_length=max_seq_length)\n",
    "\n",
    "#     print(f\"데이터셋 로딩 중: {data_path}\")\n",
    "#     with open(data_path, 'r', encoding=\"utf-8\") as f:\n",
    "#         qna_data = json.load(f)\n",
    "\n",
    "#     log_dir = os.path.join(output_dir, \"logs\")\n",
    "#     os.makedirs(log_dir, exist_ok=True) \n",
    "#     evaluation_log_path = os.path.join(log_dir, \"evaluation_log.txt\")\n",
    "\n",
    "#     generated_qna_data = []\n",
    "#     model.eval()\n",
    "#     print(\"초기 LLM 답변 생성 중...\")\n",
    "#     with open(evaluation_log_path, 'w', encoding='utf-8') as log_file: \n",
    "#         log_file.write(\"=== SLearnLLM Initial Evaluation Log ===\\n\\n\")\n",
    "        \n",
    "#         for idx, item in enumerate(tqdm(qna_data, desc=\"프리딕션 진행 중\")):\n",
    "#             question = item[\"question\"]\n",
    "#             context = item[\"context\"]\n",
    "#             true_answer = item[\"answer\"]\n",
    "\n",
    "#             input_text_prompt = f\"질문 : {question}\\n문서 : {context}\\n답변 : \" \n",
    "#             inputs = tokenizer(input_text_prompt, return_tensors=\"pt\", max_length=max_seq_length, truncation=True).to(model.device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=128,\n",
    "#                     num_beams=1,\n",
    "#                     do_sample=False,\n",
    "#                     temperature=0.0,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "#             generated_answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "#             is_correct_prediction = llm_judge.is_correct(\n",
    "#                 question=question,\n",
    "#                 context=context,\n",
    "#                 generated_answer=generated_answer,\n",
    "#                 true_answer=true_answer\n",
    "#             )\n",
    "            \n",
    "#             generated_qna_data.append({\n",
    "#                 \"question\": question,\n",
    "#                 \"context\": context,\n",
    "#                 \"true_answer\": true_answer,\n",
    "#                 \"generated_answer\": generated_answer,\n",
    "#                 \"input_text_prompt\": input_text_prompt,\n",
    "#                 \"is_correct_judged\": is_correct_prediction\n",
    "#             })\n",
    "\n",
    "#             log_file.write(f\"--- QA Pair {idx + 1} ---\\n\")\n",
    "#             log_file.write(f\"질문: {question}\\n\")\n",
    "#             log_file.write(f\"문서: {context}\\n\")\n",
    "#             log_file.write(f\"실제 정답: {true_answer}\\n\")\n",
    "#             log_file.write(f\"생성된 답변: {generated_answer}\\n\")\n",
    "#             log_file.write(f\"판단 결과: {'정답' if is_correct_prediction else '오답'}\\n\")\n",
    "#             log_file.write(\"-\" * 30 + \"\\n\\n\")\n",
    "\n",
    "#     print(f\"초기 평가 로그가 '{evaluation_log_path}'에 저장되었습니다.\")\n",
    "\n",
    "#     incorrect_qna_pairs = []\n",
    "#     print(\"판단된 QA 쌍 필터링 중...\")\n",
    "#     for item in tqdm(generated_qna_data, desc=\"필터링 진행 중\"):\n",
    "#         if not item[\"is_correct_judged\"]:\n",
    "#             incorrect_qna_pairs.append({\n",
    "#                 \"question\": item[\"question\"],\n",
    "#                 \"context\": item[\"context\"],\n",
    "#                 \"answer\": item[\"true_answer\"],\n",
    "#                 \"input_text\": item[\"input_text_prompt\"],\n",
    "#                 \"output_text\": item[\"true_answer\"]\n",
    "#             })\n",
    "\n",
    "#     print(f\"총 {len(qna_data)}개 중 {len(incorrect_qna_pairs)}개의 오답 쌍 발견 (LLM 판단 기준).\")\n",
    "#     if not incorrect_qna_pairs:\n",
    "#         print(\"모든 QA 쌍을 정확하게 예측했다고 판단되었습니다. 추가 학습이 필요 없습니다.\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"오답 쌍으로 학습 데이터셋 생성 중...\")\n",
    "#     filtered_data = []\n",
    "#     for item in incorrect_qna_pairs:\n",
    "#         filtered_data.append({\n",
    "#             \"input_text\": item[\"input_text\"],\n",
    "#             \"output_text\": item[\"output_text\"]\n",
    "#         })\n",
    "\n",
    "#     dataset = Dataset.from_list(filtered_data)\n",
    "\n",
    "#     def tokenize_function(examples):\n",
    "#         full_text = [\n",
    "#             f\"{examples['input_text'][i]}{examples['output_text'][i]}{tokenizer.eos_token}\"\n",
    "#             for i in range(len(examples['input_text']))\n",
    "#         ]\n",
    "        \n",
    "#         tokenized_inputs = tokenizer(\n",
    "#             full_text,\n",
    "#             max_length=max_seq_length,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\"\n",
    "#         )\n",
    "\n",
    "#         labels = []\n",
    "#         for i in range(len(examples['input_text'])):\n",
    "#             input_prompt_len = len(tokenizer(examples['input_text'][i], truncation=False)['input_ids'])\n",
    "            \n",
    "#             label = list(tokenized_inputs['input_ids'][i])\n",
    "#             for j in range(input_prompt_len):\n",
    "#                 label[j] = -100\n",
    "#             labels.append(label)\n",
    "        \n",
    "#         tokenized_inputs[\"labels\"] = labels\n",
    "#         return tokenized_inputs\n",
    "\n",
    "#     tokenized_dataset = dataset.map(\n",
    "#         tokenize_function,\n",
    "#         batched=True,\n",
    "#         remove_columns=dataset.column_names\n",
    "#     )\n",
    "\n",
    "#     print(\"필터링된 오답 쌍으로 학습 LLM 재학습 시작...\")\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         learning_rate=learning_rate,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         logging_dir=f\"{output_dir}/logs\",\n",
    "#         logging_steps=100,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=save_steps,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         eval_steps=eval_steps,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"loss\",\n",
    "#         greater_is_better=False,\n",
    "#         fp16=True if torch.cuda.is_available() else False,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=tokenized_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     print(\"학습 완료! 모델 저장 중...\")\n",
    "#     model.save_pretrained(output_dir)\n",
    "#     tokenizer.save_pretrained(output_dir)\n",
    "#     print(f\"학습된 모델이 {output_dir}에 저장되었습니다.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     slearn_llm_training(\n",
    "#         model_name=\"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "#         data_path=\"./data/qna_data.json\",\n",
    "#         output_dir=\"./model/slearn_llm_finetuned_llm_judge_modular\",\n",
    "#         max_seq_length=512,\n",
    "#         batch_size=2,\n",
    "#         learning_rate=2e-5,\n",
    "#         num_train_epochs=3,\n",
    "#         eval_steps=100,\n",
    "#         save_steps=100,\n",
    "#         judge_llm_name=\"google/gemma-2b\" \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     slearn_llm_training(\n",
    "#         model_name=\"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "#         data_path=\"./data/qna_data.json\",\n",
    "#         output_dir=\"./model/slearn_llm_finetuned_llm_judge_modular\",\n",
    "#         max_seq_length=512,\n",
    "#         batch_size=2,\n",
    "#         learning_rate=2e-5,\n",
    "#         num_train_epochs=3,\n",
    "#         eval_steps=100,\n",
    "#         save_steps=100,\n",
    "#         judge_llm_name=\"./model/LLM/deepseek-qwen-bllossom-32b\" \n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a388b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5757bdaaa5440fc9650e485b574954b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0513ca6c7963480bbd5d9d08e97138cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "답변 생성 중: 100%|██████████| 20/20 [02:12<00:00,  6.62s/it]\n",
      "판단 진행 중:   0%|          | 0/20 [00:00<?, ?it/s]/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "판단 진행 중: 100%|██████████| 20/20 [00:12<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 20개의 QA 쌍 중 5개가 오답으로 판단되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5bd83e237e4bde9a92bbc88876f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1016023/1477319817.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have set `args.eval_strategy` to IntervalStrategy.STEPS but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 253\u001b[39m\n\u001b[32m    250\u001b[39m     tokenizer.save_pretrained(output_dir)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mslearn_llm_training_with_llm_judge_modular\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model/LLM/deepseek-qwen-bllossom-32b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/qna_data.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model/LLM/slearn_llm_finetuned_llm_judge_modular\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjudge_llm_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model/LLM/EEVE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mslearn_llm_training_with_llm_judge_modular\u001b[39m\u001b[34m(model_name, data_path, output_dir, max_seq_length, batch_size, learning_rate, num_train_epochs, eval_steps, save_steps, judge_llm_name)\u001b[39m\n\u001b[32m    216\u001b[39m tokenized_dataset = dataset.map(\n\u001b[32m    217\u001b[39m     tokenize_function,\n\u001b[32m    218\u001b[39m     batched=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    219\u001b[39m     remove_columns=dataset.column_names\n\u001b[32m    220\u001b[39m )\n\u001b[32m    222\u001b[39m training_args = TrainingArguments(\n\u001b[32m    223\u001b[39m     output_dir=output_dir,\n\u001b[32m    224\u001b[39m     per_device_train_batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m     gradient_accumulation_steps=\u001b[32m1\u001b[39m,\n\u001b[32m    238\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m trainer.train()\n\u001b[32m    249\u001b[39m model.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/trainer.py:445\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    439\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    440\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWhen using `batch_eval_metrics`, your `compute_metrics` function must take a `compute_result`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    441\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m boolean argument which will be triggered after the last batch of the eval set to signal that the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    442\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m summary statistics should be returned by the function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    443\u001b[39m         )\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.eval_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m args.eval_strategy != \u001b[33m\"\u001b[39m\u001b[33mno\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    446\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have set `args.eval_strategy` to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.eval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but you didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    447\u001b[39m     )\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.save_strategy == SaveStrategy.BEST \u001b[38;5;129;01mor\u001b[39;00m args.load_best_model_at_end:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args.metric_for_best_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: You have set `args.eval_strategy` to IntervalStrategy.STEPS but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "_global_models = {}\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, judge_llm_name: str, max_seq_length: int = 512):\n",
    "        \n",
    "        config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        if judge_llm_name not in _global_models:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(judge_llm_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(judge_llm_name, torch_dtype=torch.bfloat16, quantization_config=config, device_map=\"auto\")\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "            self.model.eval()\n",
    "            \n",
    "            self.device = self.model.device\n",
    "            _global_models[judge_llm_name] = {\"tokenizer\": self.tokenizer, \"model\": self.model, \"device\": self.device}\n",
    "        else:\n",
    "            loaded_data = _global_models[judge_llm_name]\n",
    "            self.tokenizer = loaded_data[\"tokenizer\"]\n",
    "            self.model = loaded_data[\"model\"]\n",
    "            self.device = loaded_data[\"device\"]\n",
    "            \n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _create_judge_prompt(self, question: str, generated_answer: str, true_answer: str) -> str:\n",
    "        return (\n",
    "            f\"다음은 [질문], [생성된 답변], 그리고 [기준 정답]입니다.\\n\"\n",
    "            f\"[기준 정답]을 기준으로 [생성된 답변]이 정답/오답인지 여부를 판단하세요. \\n\"\n",
    "            f\"[생성된 답변]이 **실제로 맞는 답변이더라도 [기준 정답]과 다르면 '오답'으로 판단**해야 합니다.\\n\\n\"\n",
    "            f\"[질문]: {question}\\n\"\n",
    "            f\"[생성된 답변]: {generated_answer}\\n\"\n",
    "            f\"[기준 정답]: {true_answer}\\n\\n\" \n",
    "            f\"판단: \"\n",
    "        )\n",
    "\n",
    "    def is_correct(self, question: str, generated_answer: str, true_answer: str) -> bool:\n",
    "        judge_prompt = self._create_judge_prompt(question, generated_answer, true_answer)\n",
    "        \n",
    "        inputs = self.tokenizer(judge_prompt, return_tensors=\"pt\", \n",
    "                                max_length=self.max_seq_length, truncation=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        judge_result = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], \n",
    "                                             skip_special_tokens=True).strip().lower()\n",
    "\n",
    "        if \"오답\" in judge_result:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "def slearn_llm_training_with_llm_judge_modular(\n",
    "    model_name: str = \"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "    data_path: str = \"./data/qna_data.json\",\n",
    "    output_dir: str = \"./model\",\n",
    "    max_seq_length: int = 512,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 2e-5,\n",
    "    num_train_epochs: int = 3,\n",
    "    eval_steps: int = 500,\n",
    "    save_steps: int = 500,\n",
    "    judge_llm_name: str = \"google/gemma-2b\"\n",
    "):\n",
    "    global _global_models # 전역 변수 사용 명시\n",
    "\n",
    "    config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "    \n",
    "    if model_name not in _global_models:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                     torch_dtype=torch.bfloat16, \n",
    "                                                     device_map=\"auto\",\n",
    "                                                     quantization_config=config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = tokenizer.eos_token_id\n",
    "        _global_models[model_name] = {\"tokenizer\": tokenizer, \"model\": model, \"device\": model.device}\n",
    "    else:\n",
    "        loaded_data = _global_models[model_name]\n",
    "        tokenizer = loaded_data[\"tokenizer\"]\n",
    "        model = loaded_data[\"model\"]\n",
    "    \n",
    "    llm_judge = LLMJudge(judge_llm_name=judge_llm_name, max_seq_length=max_seq_length)\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        qna_data = json.load(f)\n",
    "\n",
    "    generated_qna_data = []\n",
    "    for item in tqdm(qna_data, desc=\"답변 생성 중\"):\n",
    "        question = item['question']\n",
    "        context = item['context']\n",
    "        true_answer = item['answer']\n",
    "\n",
    "        input_text_prompt = f\"질문 : {question}\\n답변 : \"\n",
    "        inputs = tokenizer(input_text_prompt, return_tensors=\"pt\", max_length=max_seq_length, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        generated_qna_data.append({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"input_text_prompt\": input_text_prompt\n",
    "        })\n",
    "\n",
    "    incorrect_qna_pairs = []\n",
    "    \n",
    "    output_results_dir = \"./data/slearn\"\n",
    "    os.makedirs(output_results_dir, exist_ok=True)\n",
    "\n",
    "    for idx, item in enumerate(tqdm(generated_qna_data, desc=\"판단 진행 중\")):\n",
    "        is_correct_prediction = llm_judge.is_correct(\n",
    "            question=item['question'],\n",
    "            # context=item['context'],\n",
    "            generated_answer=item['generated_answer'],\n",
    "            true_answer=item['true_answer']\n",
    "        )\n",
    "        \n",
    "        if not is_correct_prediction:\n",
    "            incorrect_qna_pairs.append({\n",
    "                \"question\": item['question'],\n",
    "                \"context\": item['context'],\n",
    "                \"answer\": item['true_answer'],\n",
    "                \"input_text\": item['input_text_prompt'],\n",
    "                \"output_text\": item['true_answer']\n",
    "            })\n",
    "        \n",
    "        result_status = \"정답\" if is_correct_prediction else \"오답\"\n",
    "        file_name = os.path.join(output_results_dir, f\"slearn_result_{idx+1:05d}.txt\")\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"<질문>: {item['question']}\\n\\n\")\n",
    "            f.write(f\"<문서>: {item['context']}\\n\\n\")\n",
    "            f.write(f\"<생성된 답변>: {item['generated_answer']}\\n\\n\")\n",
    "            f.write(f\"<기준 정답>: {item['true_answer']}\\n\\n\")\n",
    "            f.write(f\"<판단 결과>: {result_status}\\n\")\n",
    "\n",
    "    print(f\"총 {len(qna_data)}개의 QA 쌍 중 {len(incorrect_qna_pairs)}개가 오답으로 판단되었습니다.\")\n",
    "    \n",
    "    if not incorrect_qna_pairs:\n",
    "        return\n",
    "\n",
    "    filtered_data = []\n",
    "    for item in incorrect_qna_pairs:\n",
    "        filtered_data.append({\n",
    "            \"input_text\": item[\"input_text\"],\n",
    "            \"output_text\": item[\"output_text\"]\n",
    "        })\n",
    "\n",
    "    dataset = Dataset.from_list(filtered_data)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        full_text = [\n",
    "            f\"{examples['input_text'][i]}{examples['output_text'][i]}{tokenizer.eos_token}\"\n",
    "            for i in range(len(examples['input_text']))\n",
    "        ]\n",
    "        \n",
    "        tokenized_inputs = tokenizer(\n",
    "            full_text,\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i in range(len(examples['input_text'])):\n",
    "            input_prompt_len = len(tokenizer(examples['input_text'][i], truncation=False)['input_ids'])\n",
    "            \n",
    "            label = list(tokenized_inputs['input_ids'][i])\n",
    "            for j in range(input_prompt_len):\n",
    "                label[j] = -100\n",
    "            labels.append(label)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    slearn_llm_training_with_llm_judge_modular(\n",
    "        model_name=\"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "        data_path=\"./data/qna_data.json\",\n",
    "        output_dir=\"./model/LLM/slearn_llm_finetuned_llm_judge_modular\",\n",
    "        max_seq_length=512,\n",
    "        batch_size=2,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        judge_llm_name=\"./model/LLM/EEVE\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0149e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
