{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c39a73c",
   "metadata": {},
   "source": [
    "find_pages_from_entities\n",
    "retrieve_and_rerank_context\n",
    "generate_response\n",
    "\n",
    "search_knowledge_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a4855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ae504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "WARNING:pykeen.utils:using automatically assigned random_state=705137707\n",
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [423, 827]\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1608170136.\n",
      "INFO:pykeen.pipeline.api:Using device: cuda\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fece03487c48bf8cea0a8964e68d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d57bda1e034507805ef186b4d73960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/827 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.05s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGE 모델 학습 완료\n",
      "질문: What are the two essential components of a higher organism cell as defined in the text?\n",
      "검색할 쿼리 : ['components', 'organism cell', 'text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터DB 검색 결과 : [[0.5573300719261169, 0.6083144545555115, 0.6450870633125305, 0.6469337940216064, 0.6604440808296204], [0.43826135993003845, 0.4938228726387024, 0.6002139449119568, 0.6453126072883606, 0.7087544798851013], [0.6173109412193298, 0.6207958459854126, 0.6389399170875549, 0.6400978565216064, 0.66075199842453]]\n",
      "유사도 필터링 후 엔티티 : {'cells', 'cell'}\n",
      "node_data : {'type': 'Node', 'source_page': '8,13'}\n",
      "cells 노드 데이터 : {'type': 'Node', 'source_page': '8,13'}\n",
      "node_data : {'type': 'Biological Structure', 'source_page': '6'}\n",
      "cell 노드 데이터 : {'type': 'Biological Structure', 'source_page': '6'}\n",
      "찾은 페이지 : {'8', '13', '6'}\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "#############최종 context : ['... In the higher organisms a cell may be defined as “a nucleated mass of protoplasm of microscopic size.” Its\\ntwo essentials, therefore, are: a soft jelly-like material, similar to that found in the ovum, and usually styled\\ncytoplasm, and a small spherical body imbedded in it, and termed a nucleus. Some of the unicellular\\nprotozoa contain no nuclei but granular particles which, like true nuclei, stain with basic dyes. ... (출처: Page 6)', '... The other\\nconstituents of the ovum, viz., its limiting membrane and the denser spot contained in the nucleus, called the\\nnucleolus, are not essential to the type cell, and in fact many cells exist without them. ... (출처: Page 6)', '... The mitochondria are the most constant type of granule and vary in form from granules to rods and threads. Their function is unknown. Some of the granules are proteid in nature and probably essential constituents;\\nothers are fat, glycogen, or pigment granules, and are regarded as adventitious material taken in from\\nwithout, and hence are styled cell-inclusions or paraplasm. When, however, cells have been “fixed” by\\nreagents a fibrillar or granular appearance can often be made out under a high power of the microscope. ... (출처: Page 6)', '... The inner cell-mass remains in contact, however, with the\\ntrophoblast at one pole of the ovum; this is named the embryonic pole, since it indicates the situation where\\nthe future embryo will be developed. The cells of the trophoblast become differentiated into two strata: an\\nouter, termed the syncytiu m or syncytiotrophoblast, so named because it consists of a layer of protoplasm\\nstudded with nuclei, but showing no evidence of subdivision into cells; and an inner layer, the\\ncytotrophoblast or layer of Langhans, in which the cell outlines are defined. ... (출처: Page 13)', '... In appearance and structure the ovum (Fig. 3) differs little from an ordinary cell, but distinctive names have\\nbeen applied to its several parts; thus, the cell substance is known as the yolk or oöplasm, the nucleus as\\nthe germinal vesicle, and the nucleolus as the germinal spot. The ovum is enclosed within a thick,\\ntransparent envelope, the zona striata or zona pellucida, adhering to the outer surface of which are\\nseveral layers of cells, derived from those of the follicle and collectively constituting the corona radiata. ... (출처: Page 8)']\n",
      "답변: The two essential components of a higher organism cell are cytoplasm and a nucleus (Page 6). The cytoplasm is a soft, jelly-like material, and the nucleus is a small spherical body embedded within it (Page 6).\n",
      "\n",
      "질문: Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\n",
      "검색할 쿼리 : ['phases', 'cell division', 'karyokinesis', 'text']\n",
      "벡터DB 검색 결과 : [[0.6316498517990112, 0.7094032168388367, 0.7379952073097229, 0.76524817943573, 0.7754310369491577], [0.43526989221572876, 0.4362708032131195, 0.6406149864196777, 0.6665421724319458, 0.666660487651825], [0.7066031694412231, 0.7864946126937866, 0.8075331449508667, 0.8330833911895752, 0.8342280983924866], [0.6173109412193298, 0.6207959055900574, 0.6389399766921997, 0.640097975730896, 0.6607521176338196]]\n",
      "유사도 필터링 후 엔티티 : {'cells', 'cell'}\n",
      "node_data : {'type': 'Node', 'source_page': '8,13'}\n",
      "cells 노드 데이터 : {'type': 'Node', 'source_page': '8,13'}\n",
      "node_data : {'type': 'Biological Structure', 'source_page': '6'}\n",
      "cell 노드 데이터 : {'type': 'Biological Structure', 'source_page': '6'}\n",
      "찾은 페이지 : {'8', '13', '6'}\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "#############최종 context : ['... 5 The\\nprocess is repeated again and again, so that the two cells are succeeded by four, eight, sixteen, thirty-two,\\nand so on, with the result that a mass of cells is found within the zona striata, and to this mass the term\\nmorula is applied (Fig. 9). The segmentation of the mammalian ovum may not take place in the regular\\nsequence of two, four, eight, etc., since one of the two first formed cells may subdivide more rapidly than the\\nother, giving rise to a three-or a five-cell stage. ... (출처: Page 13)', '... Having regard to the main purpose of this work, it is impossible, in the space available in this section, to\\ndescribe fully, or illustrate adequately, all the phenomena which occur in the different stages of the\\ndevelopment of the human body. Only the principal facts are given, and the student is referred for further\\ndetails to one or other of the text-books 1 on human embryology.\\n## 1. The Animal Cell ... (출처: Page 6)', '... PREVIOUS NEXT ... (출처: Page 8)', '... Thus, the fertilized ovum undergoes repeated segmentation into a number of cells which at first closely\\nresemble one another, but are, sooner or later, differentiated into two groups: (1) somatic cells, the function\\nof which is to build up the various tissues of the body; and (2) germinal cells, which become imbedded in\\nthe sexual glands—the ovaries in the female and the testes in the male—and are destined for the\\nperpetuation of the species. ... (출처: Page 6)', '... The ova are developed from the primitive germ cells which are imbedded in the substance of the ovaries.\\nEach primitive germ cell gives rise, by repeated divisions, to a number of smaller cells termed oögonia, from\\nwhich the ova or primary oöcytes are developed. ... (출처: Page 8)']\n",
      "답변: The provided text does not describe the four main phases of indirect cell division (karyokinesis). It discusses the segmentation of the mammalian ovum into a morula (Page 13), the differentiation of cells into somatic and germinal cells (Page 6), and the development of ova from oögonia (Page 8). Therefore, I cannot answer your question using the given context.\n",
      "\n",
      "질문: What is the primary role of the yolk-sac in the embryo's early development?\n",
      "검색할 쿼리 : ['yolk-sac', 'embryo', 'development']\n",
      "벡터DB 검색 결과 : [[7.343734995113316e-13, 0.4126078486442566, 0.5213832855224609, 0.6891249418258667, 0.8072799444198608], [5.22499903586604e-13, 0.2637534737586975, 0.3119407296180725, 0.3463129997253418, 0.41198474168777466], [3.9522467872206424e-13, 0.5575692057609558, 0.5986218452453613, 0.6342304348945618, 0.6909395456314087]]\n",
      "유사도 필터링 후 엔티티 : {'human embryos', 'embryo', 'formation of embryo', 'human embryo', 'yolk', 'development', 'body of embryo', 'yolk-sac'}\n",
      "node_data : {'type': 'Organism', 'source_page': '348'}\n",
      "human embryos 노드 데이터 : {'type': 'Organism', 'source_page': '348'}\n",
      "node_data : {'type': 'Organ', 'source_page': '19'}\n",
      "embryo 노드 데이터 : {'type': 'Organ', 'source_page': '19'}\n",
      "node_data : {'type': 'Process', 'source_page': '13'}\n",
      "formation of embryo 노드 데이터 : {'type': 'Process', 'source_page': '13'}\n",
      "node_data : {'type': 'Organism', 'source_page': '34'}\n",
      "human embryo 노드 데이터 : {'type': 'Organism', 'source_page': '34'}\n",
      "node_data : {'type': 'Tissue', 'source_page': '12,21'}\n",
      "yolk 노드 데이터 : {'type': 'Tissue', 'source_page': '12,21'}\n",
      "node_data : {'type': 'Process', 'source_page': '59,348'}\n",
      "development 노드 데이터 : {'type': 'Process', 'source_page': '59,348'}\n",
      "node_data : {'type': 'Body Part', 'source_page': '22'}\n",
      "body of embryo 노드 데이터 : {'type': 'Body Part', 'source_page': '22'}\n",
      "node_data : {'type': 'AnatomicalStructure', 'source_page': '13,19,20,21,33,35,340,345'}\n",
      "yolk-sac 노드 데이터 : {'type': 'AnatomicalStructure', 'source_page': '13,19,20,21,33,35,340,345'}\n",
      "찾은 페이지 : {'19', '21', '345', '33', '348', '59', '12', '34', '20', '340', '13', '35', '22'}\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "#############최종 context : ['... The yolk-sac (Figs. 22 and 2 3 ) is situated on the ventral aspect of the embryo; it is lined by entoderm, outside\\nof which is a layer of mesoderm. It is filled with fluid, the vitelline fluid, which possibly may be utilized for the\\nnourishment of the embryo during the earlier stages of its existence. Blood is conveyed to the wall of the sac\\nby the primitive aortæ, and after circulating through a wide-meshed capillary plexus, is returned by the\\nvitelline veins to the tubular heart of the embryo. ... (출처: Page 20)', '... As already stated, the cells of\\nthe trophoblast do not contribute to the formation of the embryo proper; they form the ectoderm of the chorion\\nand play an important part in the development of the placenta. On the deep surface of the inner cell-mass a\\nlayer of flattened cells, the entoderm, is differentiated and quickly assumes the form of a small sac, the\\nyolk-sac. Spaces appear between the remaining cells of the mass (Fig. 11), and by the enlargement and\\ncoalescence of these spaces a cavity, termed the amniotic cavity (Fig. 12), is gradually developed. ... (출처: Page 13)', '... This constitutes the vitelline circulation, and by means of it\\nnutritive material is absorbed from the yolk-sac and conveyed to the embryo. At the end of the fourth week\\nthe yolk-sac presents the appearance of a small pear-shaped vesicle (umbilical vesicle) opening into the\\ndigestive tube by a long narrow tube, the vitelline duct. The vesicle can be seen in the after-birth as a small,\\nsomewhat oval-shaped body whose diameter varies from 1 mm. to 5 mm. ; it is situated between the amnion\\nand the chorion and may lie on or at a varying distance from the placenta. ... (출처: Page 20)', '... The embryo increases rapidly in size, but the circumference of the embryonic disk, or line of meeting of the\\nembryonic and amniotic parts of the ectoderm, is of relatively slow growth and gradually comes to form a\\nconstriction between the embryo and the greater part of the yolk-sac. By means of this constriction, which\\ncorresponds to the future umbilicus, a small part of the yolk-sac is enclosed within the embryo and\\nconstitutes the primitive digestive tube. ... (출처: Page 19)', '... The two Umbilical Vein s fuse early to form a single trunk in the body-stalk, but remain separate within the\\nembryo and pass forward to the sinus venosus in the side walls of the body. Like the vitelline veins, their\\ndirect connection with the sinus venosus becomes interrupted by the developing liver, and thus at this stage\\nthe whole of the blood from the yolk-sac and placenta passes through the substance of the liver before it\\nreaches the heart. ... (출처: Page 345)']\n",
      "답변: The yolk-sac possibly may be utilized for the nourishment of the embryo during the earlier stages of its existence (Page 20). Additionally, nutritive material is absorbed from the yolk-sac and conveyed to the embryo via the vitelline circulation (Page 20, 345).\n",
      "\n",
      "질문: How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\n",
      "검색할 쿼리 : ['embryo', 'yolk-sac', 'part of the yolk-sac', 'formation']\n",
      "벡터DB 검색 결과 : [[8.392440930572731e-13, 0.2637534737586975, 0.3119407296180725, 0.34631288051605225, 0.41198471188545227], [7.916165010828091e-13, 0.41260766983032227, 0.5213832855224609, 0.6891250014305115, 0.8072800636291504], [0.31941092014312744, 0.4270641505718231, 0.5513827800750732, 0.746644139289856, 0.750124454498291], [0.510432243347168, 0.5972580313682556, 0.6122175455093384, 0.6208958625793457, 0.6667767763137817]]\n",
      "유사도 필터링 후 엔티티 : {'formation of embryo', 'human embryos', 'embryo', 'human embryo', 'yolk', 'entoderm of the yolk-sac', 'body of embryo', 'yolk-sac'}\n",
      "node_data : {'type': 'Process', 'source_page': '13'}\n",
      "formation of embryo 노드 데이터 : {'type': 'Process', 'source_page': '13'}\n",
      "node_data : {'type': 'Organism', 'source_page': '348'}\n",
      "human embryos 노드 데이터 : {'type': 'Organism', 'source_page': '348'}\n",
      "node_data : {'type': 'Organ', 'source_page': '19'}\n",
      "embryo 노드 데이터 : {'type': 'Organ', 'source_page': '19'}\n",
      "node_data : {'type': 'Organism', 'source_page': '34'}\n",
      "human embryo 노드 데이터 : {'type': 'Organism', 'source_page': '34'}\n",
      "node_data : {'type': 'Tissue', 'source_page': '12,21'}\n",
      "yolk 노드 데이터 : {'type': 'Tissue', 'source_page': '12,21'}\n",
      "node_data : {'type': 'Tissue', 'source_page': '17'}\n",
      "entoderm of the yolk-sac 노드 데이터 : {'type': 'Tissue', 'source_page': '17'}\n",
      "node_data : {'type': 'Body Part', 'source_page': '22'}\n",
      "body of embryo 노드 데이터 : {'type': 'Body Part', 'source_page': '22'}\n",
      "node_data : {'type': 'AnatomicalStructure', 'source_page': '13,19,20,21,33,35,340,345'}\n",
      "yolk-sac 노드 데이터 : {'type': 'AnatomicalStructure', 'source_page': '13,19,20,21,33,35,340,345'}\n",
      "찾은 페이지 : {'19', '21', '345', '33', '348', '17', '12', '34', '20', '340', '13', '35', '22'}\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "#############최종 context : ['... The embryo increases rapidly in size, but the circumference of the embryonic disk, or line of meeting of the\\nembryonic and amniotic parts of the ectoderm, is of relatively slow growth and gradually comes to form a\\nconstriction between the embryo and the greater part of the yolk-sac. By means of this constriction, which\\ncorresponds to the future umbilicus, a small part of the yolk-sac is enclosed within the embryo and\\nconstitutes the primitive digestive tube. ... (출처: Page 19)', '... The yolk-sac (Figs. 22 and 2 3 ) is situated on the ventral aspect of the embryo; it is lined by entoderm, outside\\nof which is a layer of mesoderm. It is filled with fluid, the vitelline fluid, which possibly may be utilized for the\\nnourishment of the embryo during the earlier stages of its existence. Blood is conveyed to the wall of the sac\\nby the primitive aortæ, and after circulating through a wide-meshed capillary plexus, is returned by the\\nvitelline veins to the tubular heart of the embryo. ... (출처: Page 20)', '... The two Umbilical Vein s fuse early to form a single trunk in the body-stalk, but remain separate within the\\nembryo and pass forward to the sinus venosus in the side walls of the body. Like the vitelline veins, their\\ndirect connection with the sinus venosus becomes interrupted by the developing liver, and thus at this stage\\nthe whole of the blood from the yolk-sac and placenta passes through the substance of the liver before it\\nreaches the heart. ... (출처: Page 345)', '... Second Week. —By the end of this week the ovum has increased considerably in size, and the majority of its\\nvilli are vascularized. The embryo has assumed a definite form, and its cephalic and caudal extremities are\\neasily distinguished. The neural folds are partly united. The embryo is more completely separated from the\\nyolk-sac, and the paraxial mesoderm is being divided into the primitive segments (Fig. 58). ... (출처: Page 33)', '... The caudal end of the embryo is at first connected to the chorion by a band of mesoderm called the\\nbody-stalk, but with the formation of the caudal fold the body-stalk assumes a ventral position; a diverticulum\\nof the yolk-sac extends into the tail fold and is termed the hind-gut. Between the fore-gut and the hind-gut\\nthere exists for a time a wide opening into the yolk-sac, but the latter is gradually reduced to a small\\npear-shaped sac (sometimes termed the umbilical vesicl e ), and the channel of communication is at the\\nsame time narrowed and elongated to form a tube called the vitelline duct. ... (출처: Page 19)']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 672\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(questions):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m질문: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     response, context = \u001b[43mqa_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m답변: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    674\u001b[39m     save_results_to_file(q, response, context, output_dir, i + \u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 602\u001b[39m, in \u001b[36mQASystem.generate_response\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m    598\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(final_context_parts)\n\u001b[32m    600\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m._build_llm_prompt(question, context)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_llm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer, context\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 519\u001b[39m, in \u001b[36mQASystem._call_llm_generate\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m raw_answer\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m         raw_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m raw_answer\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/NAS/home/projects/ml_module/assist_auto/model_loader/ollama_loader.py:20\u001b[39m, in \u001b[36mOllamaModelLoader.generate\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     13\u001b[39m response = requests.post(\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434/api/generate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     json={\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model_path, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt},\n\u001b[32m     16\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m full_output = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1216\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1221\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/urllib3/response.py:1138\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1139\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import nltk\n",
    "# import json\n",
    "# import torch\n",
    "# import chromadb\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import nltk.downloader\n",
    "# from datetime import datetime\n",
    "# from pykeen.models import ComplEx\n",
    "# from model_loader.config import *\n",
    "# from pykeen.pipeline import pipeline\n",
    "# from transformers import AutoTokenizer\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from pykeen.triples import TriplesFactory\n",
    "# from chromadb.utils import embedding_functions\n",
    "# from pykeen.optimizers import AdamW as PyKeenAdamW\n",
    "# from typing import List, Dict, Any, Tuple, Optional, Set\n",
    "# from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "# class CustomEmbeddingFunction(embedding_functions.EmbeddingFunction):\n",
    "#     def __init__(self, embedding_model):\n",
    "#         self.embedding_model = embedding_model\n",
    "    \n",
    "#     def __call__(self, texts):\n",
    "#         return self.embedding_model.encode(texts).tolist()\n",
    "    \n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"punkt_tab\")\n",
    "\n",
    "# generation_loader = generation_loader\n",
    "\n",
    "# class QASystem:\n",
    "#     def __init__(self, \n",
    "#                  graphml_path: str, \n",
    "#                  md_path: str,\n",
    "#                  vector_db_path: str = \"./chroma_db_split\", \n",
    "#                  similarity_threshold: float = 0.5,\n",
    "#                  chunk_token_threshold: int = 250,\n",
    "#                  embedding_model_path: str = \"./model/embedding/bge-m3\"):\n",
    "#         self.graphml_path = graphml_path\n",
    "#         self.md_path = md_path\n",
    "#         self.similarity_threshold = similarity_threshold\n",
    "#         self.chunk_token_threshold = chunk_token_threshold\n",
    "#         self.llm_loader = None\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "#         self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "#         self.embedding_model = SentenceTransformer(embedding_model_path)\n",
    "#         self.custom_embedding_function = CustomEmbeddingFunction(self.embedding_model)\n",
    "\n",
    "#         self.graph = nx.read_graphml(graphml_path)\n",
    "#         self.node_name_map = {node.lower(): node for node in self.graph.nodes()}\n",
    "#         self.client = chromadb.PersistentClient(path=vector_db_path)\n",
    "        \n",
    "#         self.entity_collection = self.client.get_or_create_collection(name=\"entities_split\", embedding_function=self.custom_embedding_function)\n",
    "#         self.relation_collection = self.client.get_or_create_collection(name=\"relations_split\", embedding_function=self.custom_embedding_function)\n",
    "#         self.chunk_collection = self.client.get_or_create_collection(name=\"chunks\", embedding_function=self.custom_embedding_function)\n",
    "#         self.entity_relation_extraction_prompt_template = \"\"\"\n",
    "#             Extract entities and their relations from the following sentence.\n",
    "\n",
    "#             **Entities** should be **unique nouns or concepts**, extracted as **noun phrases** whenever possible. Identify **concrete objects or concepts** rather than complex activities or phenomena as entities.\n",
    "\n",
    "#             **Relations** should clearly describe the connection between two entities, preferring **reusable predicate verbs** for a knowledge graph. Use **concise verbs** or clear, hyphenated forms like **'part_of' or 'includes'**.\n",
    "\n",
    "#             Output the result **only in the following JSON format**, with no other explanations or text:\n",
    "\n",
    "#             ```json\n",
    "#             {{\n",
    "#                 \"entities\": [\n",
    "#                     {{\"name\": \"Entity1\", \"type\": \"Type (e.g., Organ, System, Substance, Function, Disease)\"}},\n",
    "#                     {{\"name\": \"Entity2\", \"type\": \"Type\"}}\n",
    "#                 ],\n",
    "#                 \"relations\": [\n",
    "#                     {{\"head\": \"Entity1\", \"relation\": \"Relation_Type (e.g., part_of, causes)\", \"tail\": \"Entity2\"}},\n",
    "#                     {{\"head\": \"Entity3\", \"relation\": \"generates\", \"tail\": \"Entity4\"}}\n",
    "#                 ]\n",
    "#             }}\n",
    "\n",
    "#             sentence : \"{text_to_analyze}\"\n",
    "#             JSON result :\n",
    "#         \"\"\"\n",
    "\n",
    "#         self._initialize_vector_db()\n",
    "#         # self._initialize_chunk_db()\n",
    "#         self.kge_model, self.triples_factory = self._train_kge_model()\n",
    "\n",
    "#     def _preprocess_text(self, text: str) -> str:\n",
    "#         return text.upper().replace(' ', '_')\n",
    "\n",
    "#     def _create_chunks_from_text(self, text: str, page_num: str) -> List[Dict[str, Any]] :\n",
    "#         chunks = []\n",
    "#         paragraphs = re.split(\"\\n\\n+\", text)\n",
    "#         for para in paragraphs :\n",
    "#             para = para.strip()\n",
    "#             if not para :\n",
    "#                 continue\n",
    "\n",
    "#             para_tokens = self.tokenizer.tokenize(para)\n",
    "\n",
    "#             if len(para_tokens) <= self.chunk_token_threshold :\n",
    "#                 chunks.append({\"document\": para, \"metadata\": {\"source_page\": page_num}})\n",
    "#             else :\n",
    "#                 sentences = sent_tokenize(para)\n",
    "#                 current_chunk_sentences = []\n",
    "#                 current_chunk_tokens = 0\n",
    "\n",
    "#                 for sentence in sentences :\n",
    "#                     sentence_tokens = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "#                     if current_chunk_tokens + len(sentence_tokens) > self.chunk_token_threshold and current_chunk_sentences :\n",
    "#                         chunk_text = \" \".join(current_chunk_sentences)\n",
    "#                         chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "#                         current_chunk_sentences = [sentence]\n",
    "#                         current_chunk_tokens = len(sentence_tokens)\n",
    "#                     else :\n",
    "#                         current_chunk_sentences.append(sentence)\n",
    "#                         current_chunk_tokens += len(sentence_tokens)\n",
    "\n",
    "#                 if current_chunk_sentences :\n",
    "#                     chunk_text = \" \".join(current_chunk_sentences)\n",
    "#                     chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "\n",
    "#         return chunks\n",
    "\n",
    "#     def _initialize_chunk_db(self) :\n",
    "#         if self.chunk_collection.count() > 0 :\n",
    "#             print(\"DB가 이미 초기화되어있음\")\n",
    "#             return\n",
    "        \n",
    "#         print(\"청크 DB 초기화 시작\")\n",
    "#         all_chunks = []\n",
    "#         all_md_files = [f for f in os.listdir(self.md_path) if f.endswith(\".md\")]\n",
    "\n",
    "#         for md_file in all_md_files :\n",
    "#             with open(os.path.join(self.md_path, md_file), 'r', encoding=\"utf-8\") as f :\n",
    "#                 content = f.read()\n",
    "\n",
    "#             page_matches = re.finditer(r\"####\\s+Page\\s+(\\d+)\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "#             for match in page_matches :\n",
    "#                 page_num = match.group(1).strip()\n",
    "#                 page_content = match.group(2).strip()\n",
    "#                 if page_content :\n",
    "#                     chunks = self._create_chunks_from_text(page_content, page_num)\n",
    "#                     all_chunks.extend(chunks)\n",
    "\n",
    "#         if all_chunks :\n",
    "#             documents = [chunk[\"document\"] for chunk in all_chunks]\n",
    "#             metadatas = [chunk[\"metadata\"] for chunk in all_chunks]\n",
    "\n",
    "#             ids = [f\"chunk_{i}_{datetime.now().timestamp()}\" for i in range(len(documents))]\n",
    "#             self.chunk_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "#         print(f\"청크DB 초기화 완료. {self.chunk_collection.count()}개의 청크 추가\")\n",
    "\n",
    "#     def _initialize_vector_db(self):\n",
    "#         if self.entity_collection.count() == 0:\n",
    "#             nodes_to_add = []\n",
    "#             unique_nodes = set()\n",
    "#             for node, data in self.graph.nodes(data=True):\n",
    "#                 processed_node = self._preprocess_text(node)\n",
    "#                 if processed_node not in unique_nodes :\n",
    "#                     metadata = {k: str(v) for k, v in data.items()}\n",
    "#                     metadata['original_name'] = node\n",
    "#                     nodes_to_add.append({'id': processed_node, 'document': node, 'metadata': metadata})\n",
    "#                     unique_nodes.add(processed_node)\n",
    "            \n",
    "#             if nodes_to_add:\n",
    "#                 ids = [item['id'] for item in nodes_to_add]\n",
    "#                 documents = [item['document'] for item in nodes_to_add]\n",
    "#                 metadatas = [item['metadata'] for item in nodes_to_add]\n",
    "#                 self.entity_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "#         if self.relation_collection.count() == 0:\n",
    "#             edges_to_add = []\n",
    "#             unique_processed_relations = set()\n",
    "#             for u, v, data in self.graph.edges(data=True):\n",
    "#                 relation_type = data.get('type')\n",
    "#                 if relation_type:\n",
    "#                     processed_relation = self._preprocess_text(relation_type)\n",
    "#                     if processed_relation not in unique_processed_relations :\n",
    "#                         metadata = {'original_name': relation_type}\n",
    "#                         edges_to_add.append({'id': processed_relation, 'document': processed_relation, 'metadata': metadata})\n",
    "#                         unique_processed_relations.add(processed_relation)\n",
    "\n",
    "#             if edges_to_add:\n",
    "#                 ids = [item['id'] for item in edges_to_add]\n",
    "#                 documents = [item['document'] for item in edges_to_add]\n",
    "#                 metadatas = [item['metadata'] for item in edges_to_add]\n",
    "#                 self.relation_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    \n",
    "#     def _train_kge_model(self) :\n",
    "#         triples = []\n",
    "#         for u, v, data in self.graph.edges(data=True) :\n",
    "#             relation_type = data.get(\"type\")\n",
    "#             if relation_type and isinstance(relation_type, str):\n",
    "#                 triples.append((str(u), str(relation_type), str(v)))\n",
    "\n",
    "#         if not triples :\n",
    "#             print(\"KGE 모델 학습을 위한 트리플이 없음\")\n",
    "#             return None, None\n",
    "#         # print(f\"생성된 트리플 : {len(triples)}\")\n",
    "#         # if len(triples) > 0 :\n",
    "#         #     print(f\"첫 5개 : {triples[:5]}\")\n",
    "\n",
    "#         triples_array = np.array(triples)\n",
    "#         # print(f\"Numpy 배열의 형태 : {triples_array.shape}\")\n",
    "\n",
    "#         training_triples_factory = TriplesFactory.from_labeled_triples(\n",
    "#             triples=triples_array,\n",
    "#             create_inverse_triples=True\n",
    "#         )\n",
    "#         # print(f\"TriplesFactory 생성 완료. 엔티티 수 : {training_triples_factory.num_entities}, 관계 수 : {training_triples_factory.num_relations}\")\n",
    "\n",
    "#         training_set, testing_set = training_triples_factory.split()\n",
    "\n",
    "#         result = pipeline(\n",
    "#             training=training_set,\n",
    "#             testing=testing_set,\n",
    "#             model=ComplEx,\n",
    "#             optimizer=PyKeenAdamW,\n",
    "#             training_kwargs=dict(num_epochs=100, batch_size=256, use_tqdm_batch=False),\n",
    "#             optimizer_kwargs=dict(lr=0.01),\n",
    "#             device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         )\n",
    "\n",
    "#         print(\"KGE 모델 학습 완료\")\n",
    "#         return result.model, training_triples_factory\n",
    "    \n",
    "#     def _get_kge_embedding(self, entity_name: str) -> Optional[torch.Tensor] :\n",
    "#         if self.kge_model is None or self.triples_factory is None :\n",
    "#             return None\n",
    "        \n",
    "#         if entity_name in self.triples_factory.entity_to_id :\n",
    "#             entity_id = self.triples_factory.entity_to_id[entity_name]\n",
    "#             return self.kge_model.entity_representations[0](torch.tensor([entity_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "#         return None\n",
    "    \n",
    "#     def _get_kge_relation_embedding(self, relation_name: str) -> Optional[torch.Tensor] :\n",
    "#         if self.kge_model is None or self.triples_factory is None :\n",
    "#             return None\n",
    "        \n",
    "#         if relation_name in self.triples_factory.relation_to_id :\n",
    "#             relation_id = self.triples_factory.relation_to_id[relation_name]\n",
    "#             return self.kge_model.relation_representations[0](torch.tensor([relation_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "#         return None\n",
    "\n",
    "#     def _extract_entities_relations(self, question) :\n",
    "#         prompt = self.entity_relation_extraction_prompt_template.format(text_to_analyze=question)\n",
    "#         raw_llm_output = self._call_llm_generate(prompt)\n",
    "\n",
    "#         try :\n",
    "#             json_start = raw_llm_output.find(\"{\")\n",
    "#             json_end = raw_llm_output.rfind(\"}\") + 1\n",
    "#             if json_start != -1 and json_end != -1 and json_end > json_start :\n",
    "#                 json_str = raw_llm_output[json_start:json_end]\n",
    "#                 extracted_data = json.loads(json_str)\n",
    "#                 return extracted_data.get(\"entities\", []), extracted_data.get(\"relations\", [])\n",
    "#             else :\n",
    "#                 print(f\"LLM 답변에서 유효한 JSON 형태를 찾을 수 없음 : {raw_llm_output}\")\n",
    "#                 return [], []\n",
    "            \n",
    "#         except json.JSONDecodeError as e :\n",
    "#             print(f\"개체 추출 과정에서 JSON 디코딩 오류 발생: {e}\")\n",
    "#             print(f\"오류 발생 원문: {raw_llm_output}\")\n",
    "#             return [], []\n",
    "    \n",
    "#     def _find_pages_from_entities(self, entities: List[Dict[str, Any]]) -> Set[str]:\n",
    "#         \"\"\"\n",
    "#         1. 엔티티 컬렉션에서 유사 엔티티 검색\n",
    "#         2. GraphML에서 해당 엔티티 노드를 찾아 페이지 정보 추출\n",
    "#         \"\"\"\n",
    "#         query_texts = [e['name'] for e in entities if 'name' in e]\n",
    "#         print(f\"검색할 쿼리 : {query_texts}\")\n",
    "#         if not query_texts:\n",
    "#             return set()\n",
    "\n",
    "#         # 1. 벡터 DB에서 유사 엔티티 검색\n",
    "#         entity_results = self.entity_collection.query(\n",
    "#             query_texts=query_texts,\n",
    "#             n_results=5,\n",
    "#             include=[\"metadatas\", \"distances\"]\n",
    "#         )\n",
    "#         print(f\"벡터DB 검색 결과 : {entity_results.get('distances')}\")\n",
    "#         # print(f\"쿼리 : {query_texts}\")\n",
    "#         # print(f\"검색 결과 : {entity_results}\")\n",
    "\n",
    "#         similar_entity_names = set()\n",
    "#         if entity_results.get('distances'):\n",
    "#             for i, dists in enumerate(entity_results['distances']):\n",
    "#                 for j, dist in enumerate(dists):\n",
    "#                     if dist <= self.similarity_threshold:\n",
    "#                         meta = entity_results['metadatas'][i][j]\n",
    "#                         similar_entity_names.add(meta['original_name'])\n",
    "        \n",
    "#         print(f\"유사도 필터링 후 엔티티 : {similar_entity_names}\")\n",
    "#         # 2. 그래프에서 엔티티를 찾아 페이지 번호 추출\n",
    "#         page_numbers = set()\n",
    "#         # 그래프 노드 키는 'd1'에 페이지 정보가 있음\n",
    "#         for entity_name in similar_entity_names:\n",
    "#             lower_entity_name = entity_name.lower()\n",
    "#             if lower_entity_name in self.node_name_map :\n",
    "#                 original_case_node_name = self.node_name_map[lower_entity_name]\n",
    "#                 node_data = self.graph.nodes[original_case_node_name]\n",
    "#                 print(f\"node_data : {node_data}\")\n",
    "#                 pages_str = node_data.get(\"source_page\")\n",
    "\n",
    "#                 print(f\"{entity_name} 노드 데이터 : {node_data}\")\n",
    "#                 if pages_str :\n",
    "#                     for page in pages_str.split(',') :\n",
    "#                         if page.strip() :\n",
    "#                             page_numbers.add(page.strip())\n",
    "#             else :\n",
    "#                 print(f\"{entity_name}을 그래프에서 찾을 수 없음\")\n",
    "#         return page_numbers\n",
    "    \n",
    "#     # def _search_knowledge_graph(self, entities: List[str]) -> List[Dict[str, Any]]:\n",
    "#     #     if not entities:\n",
    "#     #         return []\n",
    "\n",
    "#     #     # 엔티티 리스트\n",
    "#     #     query_entity_names = [e['name'] for e in entities if 'name' in e]\n",
    "#     #     if not query_entity_names:\n",
    "#     #         return []\n",
    "\n",
    "#     #     kge_similar_entities = set()\n",
    "\n",
    "#     #     for entity_name in query_entity_names :\n",
    "#     #         kge_embedding = self._get_kge_embedding(entity_name)\n",
    "#     #         if kge_embedding is not None :\n",
    "#     #             if entity_name in self.triples_factory.entity_to_id :\n",
    "#     #                 kge_similar_entities.add(entity_name)\n",
    "\n",
    "#     #     chroma_entity_results = self.entity_collection.query(\n",
    "#     #         query_texts = query_entity_names,\n",
    "#     #         n_results=5,\n",
    "#     #         include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "#     #     )\n",
    "\n",
    "#     #     if chroma_entity_results.get(\"distances\") :\n",
    "#     #         for i, dists in enumerate(chroma_entity_results) :\n",
    "#     #             for j, dist in enumerate(dists) :\n",
    "#     #                 if dist <= self.similarity_threshold :\n",
    "#     #                     meta = chroma_entity_results[\"metadatas\"][i][j]\n",
    "#     #                     kge_similar_entities.add(meta[\"original_name\"])\n",
    "\n",
    "#     #     found_results = []\n",
    "#     #     seen_triples = set()\n",
    "\n",
    "#     #     for u, v, data in self.graph.edges(data=True) :\n",
    "#     #         if u in kge_similar_entities or v in kge_similar_entities :\n",
    "#     #             relation_type = data.get(\"type\")\n",
    "#     #             if relation_type :\n",
    "#     #                 identifier = (u, v, relation_type)\n",
    "#     #                 if identifier not in seen_triples :\n",
    "#     #                     result = data.copy()\n",
    "#     #                     result[\"source_node\"] = u\n",
    "#     #                     result[\"target_node\"] = v\n",
    "#     #                     result[\"type\"] = relation_type\n",
    "#     #                     found_results.append(result)\n",
    "#     #                     seen_triples.add(identifier)\n",
    "\n",
    "#     #     return found_results\n",
    "\n",
    "#     def _retrieve_and_rerank_context(self, question: str, page_numbers: Set[str], top_k_rerank: int = 5) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"\n",
    "#         3. 페이지 번호로 MD 파일 검색 및 내용 추출\n",
    "#         4. 추출된 내용을 규칙에 따라 청킹\n",
    "#         5. Cross-encoder로 재정렬\n",
    "#         \"\"\"\n",
    "#         if not page_numbers:\n",
    "#             return []\n",
    "\n",
    "#         # 3. 모든 MD 파일 내용을 읽어 하나의 문자열로 합침\n",
    "#         all_md_content = \"\"\n",
    "#         for md_file in os.listdir(self.md_path):\n",
    "#             if md_file.endswith(\".md\"):\n",
    "#                 with open(os.path.join(self.md_path, md_file), 'r', encoding='utf-8') as f:\n",
    "#                     all_md_content += f.read() + \"\\n\\n\"\n",
    "\n",
    "#         # 페이지 번호에 해당하는 내용 추출 및 청킹\n",
    "#         candidate_chunks = []\n",
    "#         for page_num in page_numbers:\n",
    "#             # 정규식을 사용하여 '#### Page X' 형식의 섹션 찾기\n",
    "#             pattern = re.compile(rf\"####\\s+Page\\s+{re.escape(page_num)}\\b(.*?)(?=####\\s+Page|\\Z)\", re.S)\n",
    "#             match = pattern.search(all_md_content)\n",
    "            \n",
    "#             if match:\n",
    "#                 page_content = match.group(1).strip()\n",
    "#                 # 4. 새로운 규칙에 따라 텍스트 청킹\n",
    "#                 chunks = self._create_chunks_from_text(page_content, page_num)\n",
    "#                 candidate_chunks.extend(chunks)\n",
    "        \n",
    "#         if not candidate_chunks:\n",
    "#             return []\n",
    "            \n",
    "#         # 5. Cross-encoder를 사용하여 재정렬\n",
    "#         rerank_pairs = [(question, chunk['document']) for chunk in candidate_chunks]\n",
    "#         if not rerank_pairs:\n",
    "#             return []\n",
    "\n",
    "#         scores = self.reranker.predict(rerank_pairs)\n",
    "\n",
    "#         reranked_results = []\n",
    "#         for score, chunk in zip(scores, candidate_chunks):\n",
    "#             chunk[\"rerank_score\"] = score\n",
    "#             reranked_results.append(chunk)\n",
    "\n",
    "#         reranked_results.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "#         return reranked_results[:top_k_rerank]\n",
    "\n",
    "\n",
    "#     # def _retrieve_and_rerank_context(self, question: str, search_results: List[Dict[str, Any]], top_k_retrieval: int = 20, top_k_rerank: int = 5) -> List[Dict[str, Any]]:\n",
    "#     #     mentioned_entities = set()\n",
    "#     #     for res in search_results: # 검색된 결과의 양 끝 노드 추가\n",
    "#     #         mentioned_entities.add(res.get('source_node'))\n",
    "#     #         mentioned_entities.add(res.get('target_node'))\n",
    "        \n",
    "#     #     mentioned_entities = {e for e in mentioned_entities if e}\n",
    "\n",
    "#     #     if not mentioned_entities:\n",
    "#     #         return []\n",
    "\n",
    "#     #     query_text = \" \".join(list(mentioned_entities))\n",
    "        \n",
    "#     #     results = self.chunk_collection.query(\n",
    "#     #         query_texts=[query_text],\n",
    "#     #         n_results=top_k_retrieval,\n",
    "#     #         include=[\"documents\", \"metadatas\"]\n",
    "#     #     )\n",
    "        \n",
    "#     #     candidate_chunks = []\n",
    "#     #     seen_chunks = set()\n",
    "#     #     if results['documents'] and results['documents'][0]:\n",
    "#     #         for i in range(len(results['documents'][0])):\n",
    "#     #             doc = results['documents'][0][i]\n",
    "#     #             if doc not in seen_chunks:\n",
    "#     #                 candidate_chunks.append({\n",
    "#     #                     \"document\": doc,\n",
    "#     #                     \"metadata\": results['metadatas'][0][i]\n",
    "#     #                 })\n",
    "#     #                 seen_chunks.add(doc)\n",
    "\n",
    "#     #     if not candidate_chunks:\n",
    "#     #         return []\n",
    "\n",
    "#     #     rerank_pairs = [(question, chunk['document']) for chunk in candidate_chunks]\n",
    "#     #     if not rerank_pairs:\n",
    "#     #         return []\n",
    "\n",
    "#     #     scores = self.reranker.predict(rerank_pairs)\n",
    "\n",
    "#     #     reranked_results = []\n",
    "#     #     for score, chunk in zip(scores, candidate_chunks) :\n",
    "#     #         chunk[\"rerank_score\"] = score\n",
    "#     #         reranked_results.append(chunk)\n",
    "\n",
    "#     #     reranked_results.sort(key=lambda x : x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "#     #     return reranked_results[:top_k_rerank]\n",
    "    \n",
    "#     # def _build_llm_prompt(self, question: str, context: str, pages: List[str]) -> str:\n",
    "#     def _build_llm_prompt(self, question: str, context: str) -> str:\n",
    "#         prompt = f\"\"\"\n",
    "#         You are a helpful assistant who answers questions based on the provided context.\n",
    "#         You MUST cite the source page number for every piece of information you use.\n",
    "\n",
    "#         **Instructions:**\n",
    "#         1. Answer the user's question clearly and concisely using ONLY the provided context and knowledge graph information.\n",
    "#         2. For every statement, you MUST provide the source page number in parentheses, like this: (Page XX).\n",
    "#         3. If a single piece of information is supported by multiple pages, cite all of them: (Page X, Y, Z).\n",
    "#         4. If no context is available, state that you are answering based on the graph structure alone.\n",
    "\n",
    "#         **Example of a GOOD answer:**\n",
    "#         The ductus arteriosus degenerates into the ligamentum arteriosum after birth(page 360). This is a normal physiological change that happens post-delivery(page 361).\n",
    "\n",
    "#         **Example of a BAD answer:** -> (This is a bad answer because it lacks the mandatory citation)\n",
    "#         The ductus arteriosus becomes the ligamentum arteriosum.\n",
    "\n",
    "#         ---\n",
    "#         **Context:**\n",
    "#         {context}\n",
    "#         ---\n",
    "#         **Question:**\n",
    "#         {question}\n",
    "#         ---\n",
    "#         **Answer:**\n",
    "#         \"\"\"\n",
    "#         return prompt.strip()\n",
    "    \n",
    "#     def _call_llm_generate(self, prompt: str) -> str:\n",
    "#         if self.llm_loader:\n",
    "#             if hasattr(self.llm_loader, \"tokenizer\") and hasattr(self.llm_loader, \"model\"):\n",
    "#                 tokenizer = self.llm_loader.tokenizer\n",
    "#                 model = self.llm_loader.model\n",
    "\n",
    "#                 input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "#                 attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "#                 output = model.generate(\n",
    "#                     input_ids=input_ids,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     max_new_tokens=500,\n",
    "#                     temperature=0.0,\n",
    "#                     do_sample=False,\n",
    "#                     top_p=0.85,\n",
    "#                     repetition_penalty=1.2,\n",
    "#                     early_stopping=True,\n",
    "#                     num_beams=3,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "#                 generated_ids = output[0][input_ids.shape[-1]:]\n",
    "#                 raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "#                 return raw_answer\n",
    "#             else:\n",
    "#                 raw_answer = self.llm_loader.generate(prompt)\n",
    "#                 return raw_answer\n",
    "#         else:\n",
    "#             print(\"generation_loader가 로드되지 않음\")\n",
    "#             return \"LLM 로더가 설정되지 않았습니다.\"\n",
    "\n",
    "#     def generate_response(self, question: str) -> Tuple[str, str]:\n",
    "#         # 질문에서 엔티티/개체 추출\n",
    "#         entities, relations = self._extract_entities_relations(question)\n",
    "#         if not entities:\n",
    "#             return \"질문에서 유효한 엔티티나 릴레이션을 추출할 수 없습니다.\", \"\"\n",
    "        \n",
    "#         page_numbers = self._find_pages_from_entities(entities)\n",
    "#         print(f\"찾은 페이지 : {page_numbers}\")\n",
    "#         if not page_numbers :\n",
    "#             return \"페이지를 찾을 수 없음\", \"\"\n",
    "        \n",
    "#         # 지식그래프에서 발견된 엔티티로 청크 검색(유사도) & 재정렬\n",
    "#         reranked_chunks = self._retrieve_and_rerank_context(question, page_numbers)\n",
    "#         if not reranked_chunks:\n",
    "#             return \"관련 페이지는 찾았으나, 질문과 직접적으로 연관된 문맥이 없음\", \"\"\n",
    "        \n",
    "#         final_context_parts = []\n",
    "#         current_len = 0\n",
    "\n",
    "#         if hasattr(self.llm_loader, 'tokenizer') and self.llm_loader.tokenizer is not None:\n",
    "#             print(\"LLM 로더의 특정 토크나이저를 사용하여 길이를 계산합니다.\")\n",
    "#             llm_tokenizer = self.llm_loader.tokenizer\n",
    "#             max_len = getattr(llm_tokenizer, 'model_max_length', 512) - 150\n",
    "            \n",
    "#             base_prompt = self._build_llm_prompt(question, \"\")\n",
    "#             base_prompt_len = len(llm_tokenizer.tokenize(base_prompt))\n",
    "#             current_len += base_prompt_len\n",
    "\n",
    "#             for chunk in reranked_chunks:\n",
    "#                 page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "#                 context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "#                 chunk_token_len = len(llm_tokenizer.tokenize(context_snippet))\n",
    "                \n",
    "#                 if current_len + chunk_token_len <= max_len:\n",
    "#                     final_context_parts.append(context_snippet)\n",
    "#                     current_len += chunk_token_len\n",
    "#                 else:\n",
    "#                     break\n",
    "#         else:\n",
    "#             print(\"범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\")\n",
    "#             proxy_tokenizer = self.tokenizer  \n",
    "#             max_len = 2048 - 500 \n",
    "\n",
    "#             for chunk in reranked_chunks:\n",
    "#                 page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "#                 context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "#                 chunk_token_len = len(proxy_tokenizer.tokenize(context_snippet))\n",
    "                \n",
    "#                 if current_len + chunk_token_len <= max_len:\n",
    "#                     final_context_parts.append(context_snippet)\n",
    "#                     current_len += chunk_token_len\n",
    "#                 else:\n",
    "#                     break\n",
    "#         print(f\"#############최종 context : {final_context_parts}\")\n",
    "#         if not final_context_parts:\n",
    "#             return \"관련 정보를 찾았으나, 모델의 입력 길이 제한으로 인해 컨텍스트를 구성할 수 없습니다.\", \"\"\n",
    "\n",
    "#         context = \"\\n\\n\".join(final_context_parts)\n",
    "        \n",
    "#         prompt = self._build_llm_prompt(question, context)\n",
    "        \n",
    "#         answer = self._call_llm_generate(prompt)\n",
    "#         return answer, context\n",
    "\n",
    "# def save_results_to_file(question: str, answer: str, context: str, output_dir: str, file_index: int):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     timestamp = datetime.now().strftime(\"%H%M%S_%f\")\n",
    "#     file_name = f\"result_{file_index}_{timestamp}.txt\"\n",
    "#     file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(f\"[질문]\\n{question}\\n\\n\")\n",
    "#         f.write(f\"[근거]\\n{context}\\n\\n\")\n",
    "#         f.write(f\"[답변]\\n{answer}\\n\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     qa_system = QASystem(\n",
    "#         graphml_path=\"./data/knowledge_graph/knowledge_graph.graphml\",\n",
    "#         md_path=\"./data/split_file/anatomy/\"\n",
    "#     )\n",
    "    \n",
    "#     qa_system.llm_loader = generation_loader\n",
    "    \n",
    "#     questions = [\n",
    "#         ############## 1_Embryology.md\n",
    "#         \"What are the two essential components of a higher organism cell as defined in the text?\", # 7페이지\n",
    "#         \"Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\", # 7페이지\n",
    "#         \"What is the primary role of the yolk-sac in the embryo's early development?\", # 20페이지\n",
    "#         \"How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\", # 19페이지\n",
    "#         \"What significant developments occur in a human embryo during the Second Week?\", # 33페이지\n",
    "#         \"What are the key characteristics of the human embryo by the end of the Third Week?\", # 33페이지\n",
    "        \n",
    "#         ############## 2_Osteology.md\n",
    "#         \"What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\", # 38페이지\n",
    "#         \"How is each vertebral body formed from primitive segments during development?\", # 38페이지\n",
    "#         \"What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\", # 88페이지\n",
    "#         \"Describe the sphenoidal rostrum and its articulation.\",# 88\n",
    "#         \"What is the tibia, and where is it located in the human leg?\", # 158\n",
    "#         \"Describe the superior articular surface of the tibia's upper extremity.\", # 158\n",
    "\n",
    "#         ############## 3_Syndesmology.md\n",
    "#         \"What are joints or articulations, and how are immovable joints characterized?\", # 174\n",
    "#         \"How does the articular lamella differ from ordinary bone tissue?\", # 174\n",
    "#         \"Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\", # 207\n",
    "#         \"List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\", # 207\n",
    "#         \"What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\", # 236\n",
    "#         \"How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\", # 236\n",
    "\n",
    "#         ############## 4_Myology.md\n",
    "#         \"How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\", # 250\n",
    "#         \"Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\", # 250\n",
    "#         \"What is the triangular ligament and where is it located?\", # 290\n",
    "#         \"What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\", # 290\n",
    "#         \"Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\", # 322\n",
    "#         \"What is the Peronæus tertius, and where is it inserted?\", # 322\n",
    "\n",
    "#         ############## 5_Angiology.md\n",
    "#         \"What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\", # 334\n",
    "#         \"Describe the composition and variations of the external coat (tunica adventitia) in arteries.\", # 334\n",
    "#         \"How do the Vitelline Veins develop into parts of the portal and hepatic veins?\", # 345\n",
    "#         \"What happens to the Umbilical Veins during embryonic development and after birth?\", # 345\n",
    "#         \"What are the three phases of a cardiac cycle and what happens during each?\", # 358\n",
    "#         \"What are the main peculiarities observed in the fetal heart's vascular system?\" # 359\n",
    "#     ]   \n",
    "\n",
    "#     today = datetime.now()\n",
    "#     folder_name = f\"{today.month}월{today.day}일\"\n",
    "#     output_dir = os.path.join(\"./result\", \"knowledge_graph\", folder_name)\n",
    "#     for i, q in enumerate(questions):\n",
    "#         print(f\"질문: {q}\")\n",
    "#         response, context = qa_system.generate_response(q)\n",
    "#         print(f\"답변: {response}\\n\")\n",
    "#         save_results_to_file(q, response, context, output_dir, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc04c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================Embedding_loader.py 로드됨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e391e6494774021a05acafcdc9031cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "using automatically assigned random_state=1187335374\n",
      "No random seed is specified. Setting to 282327934.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AdamW.__init__() got an unexpected keyword argument 'decoupled_weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 524\u001b[39m\n\u001b[32m    521\u001b[39m         f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[근거]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     qa_system = \u001b[43mQASystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraphml_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/knowledge_graph/knowledge_graph_1.graphml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmd_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/split_file/anatomy/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     qa_system.llm_loader = generation_loader\n\u001b[32m    531\u001b[39m     questions = [\n\u001b[32m    532\u001b[39m         \u001b[38;5;66;03m############## 1_Embryology.md\u001b[39;00m\n\u001b[32m    533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat are the two essential components of a higher organism cell as defined in the text?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# 7페이지\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    570\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat are the main peculiarities observed in the fetal heart\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms vascular system?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# 359\u001b[39;00m\n\u001b[32m    571\u001b[39m     ]   \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mQASystem.__init__\u001b[39m\u001b[34m(self, graphml_path, md_path, vector_db_path, similarity_threshold, chunk_token_threshold, embedding_model_path)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_vector_db()\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# self._initialize_chunk_db()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.kge_model, \u001b[38;5;28mself\u001b[39m.triples_factory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_kge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mQASystem._train_kge_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# print(f\"TriplesFactory 생성 완료. 엔티티 수 : {training_triples_factory.num_entities}, 관계 수 : {training_triples_factory.num_relations}\")\u001b[39;00m\n\u001b[32m    214\u001b[39m training_set, testing_set = training_triples_factory.split()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtesting_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mComplEx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPyKeenAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKGE 모델 학습 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.model, training_triples_factory\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/pipeline/api.py:1540\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[39m\n\u001b[32m   1519\u001b[39m training_loop_instance = _handle_training_loop(\n\u001b[32m   1520\u001b[39m     _result_tracker=_result_tracker,\n\u001b[32m   1521\u001b[39m     model_instance=model_instance,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m     negative_sampler_kwargs=negative_sampler_kwargs,\n\u001b[32m   1531\u001b[39m )\n\u001b[32m   1533\u001b[39m evaluator_instance, evaluation_kwargs = _handle_evaluator(\n\u001b[32m   1534\u001b[39m     _result_tracker=_result_tracker,\n\u001b[32m   1535\u001b[39m     evaluator=evaluator,\n\u001b[32m   1536\u001b[39m     evaluator_kwargs=evaluator_kwargs,\n\u001b[32m   1537\u001b[39m     evaluation_kwargs=evaluation_kwargs,\n\u001b[32m   1538\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m stopper_instance, configuration, losses, train_seconds = \u001b[43m_handle_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_result_tracker\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_result_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluator_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_loop_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_loop_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclear_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopper_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopper_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1556\u001b[39m metric_results, evaluate_seconds = _handle_evaluation(\n\u001b[32m   1557\u001b[39m     _result_tracker=_result_tracker,\n\u001b[32m   1558\u001b[39m     model_instance=model_instance,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1569\u001b[39m     use_tqdm=use_tqdm,\n\u001b[32m   1570\u001b[39m )\n\u001b[32m   1571\u001b[39m _result_tracker.end_run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/pipeline/api.py:1181\u001b[39m, in \u001b[36m_handle_training\u001b[39m\u001b[34m(_result_tracker, training, validation, model_instance, evaluator_instance, training_loop_instance, clear_optimizer, evaluation_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, use_tqdm)\u001b[39m\n\u001b[32m   1179\u001b[39m \u001b[38;5;66;03m# Train like Cristiano Ronaldo\u001b[39;00m\n\u001b[32m   1180\u001b[39m training_start_time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m losses = \u001b[43mtraining_loop_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopper_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclear_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m losses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# losses is only none if it's doing search mode\u001b[39;00m\n\u001b[32m   1188\u001b[39m training_end_time = time.time() - training_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/training/training_loop.py:383\u001b[39m, in \u001b[36mTrainingLoop.train\u001b[39m\u001b[34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, clear_optimizer, checkpoint_directory, checkpoint_name, checkpoint_frequency, checkpoint_on_failure, drop_last, callbacks, callbacks_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001b[39m\n\u001b[32m    380\u001b[39m             temporary_directory = exit_stack.enter_context(TemporaryDirectory())\n\u001b[32m    381\u001b[39m             best_epoch_model_file_path = pathlib.Path(temporary_directory).joinpath(\u001b[33m\"\u001b[39m\u001b[33mbest_model.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mslice_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslice_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43monly_size_probing\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_size_probing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_tqdm_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstopper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_on_failure_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_on_failure_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbest_epoch_model_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_epoch_model_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlast_best_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_best_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_max_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_clipping_max_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_norm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_clipping_norm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_max_abs_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_clipping_max_abs_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Ensure the release of memory\u001b[39;00m\n\u001b[32m    414\u001b[39m torch.cuda.empty_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/training/training_loop.py:589\u001b[39m, in \u001b[36mTrainingLoop._train\u001b[39m\u001b[34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks, callbacks_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# This will find necessary parameters to optimize the use of the hardware at hand\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    583\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m only_size_probing\n\u001b[32m    584\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.automatic_memory_optimization\n\u001b[32m   (...)\u001b[39m\u001b[32m    587\u001b[39m ):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# return the relevant parameters slice_size and batch_size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m     sub_batch_size, slice_size = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msub_batch_and_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtriples_factory\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sub_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sub_batch_size == batch_size:  \u001b[38;5;66;03m# by default do not split batches in sub-batches\u001b[39;00m\n\u001b[32m    594\u001b[39m     sub_batch_size = batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/training/training_loop.py:962\u001b[39m, in \u001b[36mTrainingLoop.sub_batch_and_slice\u001b[39m\u001b[34m(self, batch_size, sampler, triples_factory)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msub_batch_and_slice\u001b[39m(\n\u001b[32m    955\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    956\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m     triples_factory: CoreTriplesFactory,\n\u001b[32m    960\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    961\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check if sub-batching and/or slicing is necessary to train the model on the hardware at hand.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m     sub_batch_size, finished_search, supports_sub_batching = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sub_batch_size_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m finished_search:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/training/training_loop.py:1036\u001b[39m, in \u001b[36mTrainingLoop._sub_batch_size_search\u001b[39m\u001b[34m(self, batch_size, sampler, triples_factory)\u001b[39m\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28mself\u001b[39m._free_graph_and_cache()\n\u001b[32m   1035\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m=:\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for training now.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_size_probing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m runtime_error:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;28mself\u001b[39m._free_graph_and_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/sangwon/lib/python3.11/site-packages/pykeen/training/training_loop.py:614\u001b[39m, in \u001b[36mTrainingLoop._train\u001b[39m\u001b[34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks, callbacks_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;66;03m# Create new optimizer\u001b[39;00m\n\u001b[32m    613\u001b[39m optimizer_kwargs = _get_optimizer_kwargs(\u001b[38;5;28mself\u001b[39m.optimizer)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_grad_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# Create a new lr scheduler and add the optimizer\u001b[39;00m\n\u001b[32m    621\u001b[39m     lr_scheduler_kwargs = _get_lr_scheduler_kwargs(\u001b[38;5;28mself\u001b[39m.lr_scheduler)\n",
      "\u001b[31mTypeError\u001b[39m: AdamW.__init__() got an unexpected keyword argument 'decoupled_weight_decay'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk.downloader\n",
    "from datetime import datetime\n",
    "from pykeen.models import ComplEx\n",
    "from model_loader_folder.config import *\n",
    "from pykeen.pipeline import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pykeen.triples import TriplesFactory\n",
    "from chromadb.utils import embedding_functions\n",
    "from pykeen.optimizers import AdamW as PyKeenAdamW\n",
    "from typing import List, Dict, Any, Tuple, Optional, Set\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "class CustomEmbeddingFunction(embedding_functions.EmbeddingFunction):\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embedding_model.encode(texts).tolist()\n",
    "    \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "generation_loader = generation_loader\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, \n",
    "                 graphml_path: str, \n",
    "                 md_path: str,\n",
    "                 vector_db_path: str = \"./chroma_db_split\", \n",
    "                 similarity_threshold: float = 0.5,\n",
    "                 chunk_token_threshold: int = 250,\n",
    "                 embedding_model_path: str = \"./model/embedding/bge-m3\"):\n",
    "        self.graphml_path = graphml_path\n",
    "        self.md_path = md_path\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.chunk_token_threshold = chunk_token_threshold\n",
    "        self.llm_loader = None\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_path)\n",
    "        self.custom_embedding_function = CustomEmbeddingFunction(self.embedding_model)\n",
    "\n",
    "        self.graph = nx.read_graphml(graphml_path)\n",
    "        self.node_name_map = {node.lower(): node for node in self.graph.nodes()}\n",
    "        self.client = chromadb.PersistentClient(path=vector_db_path)\n",
    "        \n",
    "        self.entity_collection = self.client.get_or_create_collection(name=\"entities_split\", embedding_function=self.custom_embedding_function)\n",
    "        self.relation_collection = self.client.get_or_create_collection(name=\"relations_split\", embedding_function=self.custom_embedding_function)\n",
    "        self.chunk_collection = self.client.get_or_create_collection(name=\"chunks\", embedding_function=self.custom_embedding_function)\n",
    "        self.entity_relation_extraction_prompt_template = \"\"\"\n",
    "            Extract entities and their relations from the following sentence.\n",
    "\n",
    "            **Entities** should be **unique nouns or concepts**, extracted as **noun phrases** whenever possible. Identify **concrete objects or concepts** rather than complex activities or phenomena as entities.\n",
    "\n",
    "            **Relations** should clearly describe the connection between two entities, preferring **reusable predicate verbs** for a knowledge graph. Use **concise verbs** or clear, hyphenated forms like **'part_of' or 'includes'**.\n",
    "\n",
    "            Output the result **only in the following JSON format**, with no other explanations or text:\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"entities\": [\n",
    "                    {{\"name\": \"Entity1\", \"type\": \"Type (e.g., Organ, System, Substance, Function, Disease)\"}},\n",
    "                    {{\"name\": \"Entity2\", \"type\": \"Type\"}}\n",
    "                ],\n",
    "                \"relations\": [\n",
    "                    {{\"head\": \"Entity1\", \"relation\": \"Relation_Type (e.g., part_of, causes)\", \"tail\": \"Entity2\"}},\n",
    "                    {{\"head\": \"Entity3\", \"relation\": \"generates\", \"tail\": \"Entity4\"}}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            sentence : \"{text_to_analyze}\"\n",
    "            JSON result :\n",
    "        \"\"\"\n",
    "\n",
    "        self._initialize_vector_db()\n",
    "        # self._initialize_chunk_db()\n",
    "        self.kge_model, self.triples_factory = self._train_kge_model()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        return text.upper().replace(' ', '_')\n",
    "\n",
    "    def _create_chunks_from_text(self, text: str, page_num: str) -> List[Dict[str, Any]] :\n",
    "        chunks = []\n",
    "        paragraphs = re.split(\"\\n\\n+\", text)\n",
    "        for para in paragraphs :\n",
    "            para = para.strip()\n",
    "            if not para :\n",
    "                continue\n",
    "\n",
    "            para_tokens = self.tokenizer.tokenize(para)\n",
    "\n",
    "            if len(para_tokens) <= self.chunk_token_threshold :\n",
    "                chunks.append({\"document\": para, \"metadata\": {\"source_page\": page_num}})\n",
    "            else :\n",
    "                sentences = sent_tokenize(para)\n",
    "                current_chunk_sentences = []\n",
    "                current_chunk_tokens = 0\n",
    "\n",
    "                for sentence in sentences :\n",
    "                    sentence_tokens = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "                    if current_chunk_tokens + len(sentence_tokens) > self.chunk_token_threshold and current_chunk_sentences :\n",
    "                        chunk_text = \" \".join(current_chunk_sentences)\n",
    "                        chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "                        current_chunk_sentences = [sentence]\n",
    "                        current_chunk_tokens = len(sentence_tokens)\n",
    "                    else :\n",
    "                        current_chunk_sentences.append(sentence)\n",
    "                        current_chunk_tokens += len(sentence_tokens)\n",
    "\n",
    "                if current_chunk_sentences :\n",
    "                    chunk_text = \" \".join(current_chunk_sentences)\n",
    "                    chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _initialize_chunk_db(self) :\n",
    "        if self.chunk_collection.count() > 0 :\n",
    "            print(\"DB가 이미 초기화되어있음\")\n",
    "            return\n",
    "        \n",
    "        print(\"청크 DB 초기화 시작\")\n",
    "        all_chunks = []\n",
    "        all_md_files = [f for f in os.listdir(self.md_path) if f.endswith(\".md\")]\n",
    "\n",
    "        for md_file in all_md_files :\n",
    "            with open(os.path.join(self.md_path, md_file), 'r', encoding=\"utf-8\") as f :\n",
    "                content = f.read()\n",
    "\n",
    "            page_matches = re.finditer(r\"####\\s+Page\\s+(\\d+)\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "            for match in page_matches :\n",
    "                page_num = match.group(1).strip()\n",
    "                page_content = match.group(2).strip()\n",
    "                if page_content :\n",
    "                    chunks = self._create_chunks_from_text(page_content, page_num)\n",
    "                    all_chunks.extend(chunks)\n",
    "\n",
    "        if all_chunks :\n",
    "            documents = [chunk[\"document\"] for chunk in all_chunks]\n",
    "            metadatas = [chunk[\"metadata\"] for chunk in all_chunks]\n",
    "\n",
    "            ids = [f\"chunk_{i}_{datetime.now().timestamp()}\" for i in range(len(documents))]\n",
    "            self.chunk_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "        print(f\"청크DB 초기화 완료. {self.chunk_collection.count()}개의 청크 추가\")\n",
    "\n",
    "    def _initialize_vector_db(self):\n",
    "        if self.entity_collection.count() == 0:\n",
    "            nodes_to_add = []\n",
    "            unique_nodes = set()\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                processed_node = self._preprocess_text(node)\n",
    "                if processed_node not in unique_nodes :\n",
    "                    metadata = {k: str(v) for k, v in data.items()}\n",
    "                    metadata['original_name'] = node\n",
    "                    nodes_to_add.append({'id': processed_node, 'document': node, 'metadata': metadata})\n",
    "                    unique_nodes.add(processed_node)\n",
    "            \n",
    "            if nodes_to_add:\n",
    "                ids = [item['id'] for item in nodes_to_add]\n",
    "                documents = [item['document'] for item in nodes_to_add]\n",
    "                metadatas = [item['metadata'] for item in nodes_to_add]\n",
    "                self.entity_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "        if self.relation_collection.count() == 0:\n",
    "            edges_to_add = []\n",
    "            unique_processed_relations = set()\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                relation_type = data.get('type')\n",
    "                if relation_type:\n",
    "                    processed_relation = self._preprocess_text(relation_type)\n",
    "                    if processed_relation not in unique_processed_relations :\n",
    "                        metadata = {'original_name': relation_type}\n",
    "                        edges_to_add.append({'id': processed_relation, 'document': processed_relation, 'metadata': metadata})\n",
    "                        unique_processed_relations.add(processed_relation)\n",
    "\n",
    "            if edges_to_add:\n",
    "                ids = [item['id'] for item in edges_to_add]\n",
    "                documents = [item['document'] for item in edges_to_add]\n",
    "                metadatas = [item['metadata'] for item in edges_to_add]\n",
    "                self.relation_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    \n",
    "    def _train_kge_model(self) :\n",
    "        triples = []\n",
    "        for u, v, data in self.graph.edges(data=True) :\n",
    "            relation_type = data.get(\"type\")\n",
    "            if relation_type and isinstance(relation_type, str):\n",
    "                triples.append((str(u), str(relation_type), str(v)))\n",
    "\n",
    "        if not triples :\n",
    "            print(\"KGE 모델 학습을 위한 트리플이 없음\")\n",
    "            return None, None\n",
    "\n",
    "        triples_array = np.array(triples)\n",
    "        # print(f\"Numpy 배열의 형태 : {triples_array.shape}\")\n",
    "\n",
    "        training_triples_factory = TriplesFactory.from_labeled_triples(\n",
    "            triples=triples_array,\n",
    "            create_inverse_triples=True\n",
    "        )\n",
    "        # print(f\"TriplesFactory 생성 완료. 엔티티 수 : {training_triples_factory.num_entities}, 관계 수 : {training_triples_factory.num_relations}\")\n",
    "\n",
    "        training_set, testing_set = training_triples_factory.split()\n",
    "\n",
    "        result = pipeline(\n",
    "            training=training_set,\n",
    "            testing=testing_set,\n",
    "            model=ComplEx,\n",
    "            optimizer=PyKeenAdamW,\n",
    "            training_kwargs=dict(num_epochs=100, batch_size=256, use_tqdm_batch=False),\n",
    "            optimizer_kwargs=dict(lr=0.01),\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        print(\"KGE 모델 학습 완료\")\n",
    "        return result.model, training_triples_factory\n",
    "    \n",
    "    def _get_kge_embedding(self, entity_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if entity_name in self.triples_factory.entity_to_id :\n",
    "            entity_id = self.triples_factory.entity_to_id[entity_name]\n",
    "            return self.kge_model.entity_representations[0](torch.tensor([entity_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "    \n",
    "    def _get_kge_relation_embedding(self, relation_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if relation_name in self.triples_factory.relation_to_id :\n",
    "            relation_id = self.triples_factory.relation_to_id[relation_name]\n",
    "            return self.kge_model.relation_representations[0](torch.tensor([relation_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "\n",
    "    def _extract_entities_relations(self, question) :\n",
    "        prompt = self.entity_relation_extraction_prompt_template.format(text_to_analyze=question)\n",
    "        raw_llm_output = self._call_llm_generate(prompt)\n",
    "\n",
    "        try :\n",
    "            json_start = raw_llm_output.find(\"{\")\n",
    "            json_end = raw_llm_output.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1 and json_end > json_start :\n",
    "                json_str = raw_llm_output[json_start:json_end]\n",
    "                extracted_data = json.loads(json_str)\n",
    "                return extracted_data.get(\"entities\", []), extracted_data.get(\"relations\", [])\n",
    "            else :\n",
    "                print(f\"LLM 답변에서 유효한 JSON 형태를 찾을 수 없음 : {raw_llm_output}\")\n",
    "                return [], []\n",
    "            \n",
    "        except json.JSONDecodeError as e :\n",
    "            print(f\"개체 추출 과정에서 JSON 디코딩 오류 발생: {e}\")\n",
    "            print(f\"오류 발생 원문: {raw_llm_output}\")\n",
    "            return [], []\n",
    "    \n",
    "    def _find_pages_from_entities(self, entities: List[Dict[str, Any]]) -> Set[str]:\n",
    "        query_texts = [e['name'] for e in entities if 'name' in e]\n",
    "        print(f\"검색할 쿼리 : {query_texts}\")\n",
    "        if not query_texts:\n",
    "            return set()\n",
    "\n",
    "        # 1. 벡터 DB에서 유사 엔티티 검색\n",
    "        entity_results = self.entity_collection.query(\n",
    "            query_texts=query_texts,\n",
    "            n_results=5,\n",
    "            include=[\"metadatas\", \"distances\"]\n",
    "        )\n",
    "        print(f\"벡터DB 검색 결과 : {entity_results.get('distances')}\")\n",
    "        # print(f\"쿼리 : {query_texts}\")\n",
    "        # print(f\"검색 결과 : {entity_results}\")\n",
    "\n",
    "        similar_entity_names = set()\n",
    "        if entity_results.get('distances'):\n",
    "            for i, dists in enumerate(entity_results['distances']):\n",
    "                for j, dist in enumerate(dists):\n",
    "                    if dist <= self.similarity_threshold:\n",
    "                        meta = entity_results['metadatas'][i][j]\n",
    "                        similar_entity_names.add(meta['original_name'])\n",
    "        \n",
    "        print(f\"유사도 필터링 후 엔티티 : {similar_entity_names}\")\n",
    "        # 2. 그래프에서 엔티티를 찾아 페이지 번호 추출\n",
    "        page_numbers = set()\n",
    "        # 'source_page' : 페이지 정보\n",
    "        for entity_name in similar_entity_names:\n",
    "            lower_entity_name = entity_name.lower()\n",
    "            if lower_entity_name in self.node_name_map :\n",
    "                original_case_node_name = self.node_name_map[lower_entity_name]\n",
    "                node_data = self.graph.nodes[original_case_node_name]\n",
    "                print(f\"node_data : {node_data}\")\n",
    "                pages_str = node_data.get(\"source_page\")\n",
    "\n",
    "                print(f\"{entity_name} 노드 데이터 : {node_data}\")\n",
    "                if pages_str :\n",
    "                    for page in pages_str.split(',') :\n",
    "                        if page.strip() :\n",
    "                            page_numbers.add(page.strip())\n",
    "            else :\n",
    "                print(f\"{entity_name}을 그래프에서 찾을 수 없음\")\n",
    "        return page_numbers\n",
    "\n",
    "    def _retrieve_and_rerank_context(self, question: str, page_numbers: Set[str], top_k_rerank: int = 5) -> List[Dict[str, Any]]:\n",
    "        if not page_numbers:\n",
    "            return []\n",
    "\n",
    "        # 3. 모든 MD 파일 내용을 읽어 하나의 문자열로 합침\n",
    "        all_md_content = \"\"\n",
    "        for md_file in os.listdir(self.md_path):\n",
    "            if md_file.endswith(\".md\"):\n",
    "                with open(os.path.join(self.md_path, md_file), 'r', encoding='utf-8') as f:\n",
    "                    all_md_content += f.read() + \"\\n\\n\"\n",
    "\n",
    "        # 페이지 번호에 해당하는 내용 추출 및 청킹\n",
    "        candidate_chunks = []\n",
    "        for page_num in page_numbers:\n",
    "            # 정규식을 사용하여 '#### Page X' 형식의 섹션 찾기\n",
    "            pattern = re.compile(rf\"####\\s+Page\\s+{re.escape(page_num)}\\b(.*?)(?=####\\s+Page|\\Z)\", re.S)\n",
    "            match = pattern.search(all_md_content)\n",
    "            \n",
    "            if match:\n",
    "                page_content = match.group(1).strip()\n",
    "                # 4. 새로운 규칙에 따라 텍스트 청킹\n",
    "                chunks = self._create_chunks_from_text(page_content, page_num)\n",
    "                candidate_chunks.extend(chunks)\n",
    "        \n",
    "        if not candidate_chunks:\n",
    "            return []\n",
    "            \n",
    "        # 5. Cross-encoder를 사용하여 재정렬\n",
    "        rerank_pairs = [(question, chunk['document']) for chunk in candidate_chunks]\n",
    "        if not rerank_pairs:\n",
    "            return []\n",
    "\n",
    "        scores = self.reranker.predict(rerank_pairs)\n",
    "\n",
    "        reranked_results = []\n",
    "        for score, chunk in zip(scores, candidate_chunks):\n",
    "            chunk[\"rerank_score\"] = score\n",
    "            reranked_results.append(chunk)\n",
    "\n",
    "        reranked_results.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "        return reranked_results[:top_k_rerank]\n",
    "    \n",
    "    def _build_llm_prompt(self, question: str, context: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant who answers questions based on the provided context.\n",
    "        You MUST cite the source page number for every piece of information you use.\n",
    "\n",
    "        **Instructions:**\n",
    "        1. Answer the user's question clearly and concisely using ONLY the provided context and knowledge graph information.\n",
    "        2. For every statement, you MUST provide the source page number in parentheses, like this: (Page XX).\n",
    "        3. If a single piece of information is supported by multiple pages, cite all of them: (Page X, Y, Z).\n",
    "        4. If no context is available, state that you are answering based on the graph structure alone.\n",
    "\n",
    "        **Example of a GOOD answer:**\n",
    "        The ductus arteriosus degenerates into the ligamentum arteriosum after birth(page 360). This is a normal physiological change that happens post-delivery(page 361).\n",
    "\n",
    "        **Example of a BAD answer:** -> (This is a bad answer because it lacks the mandatory citation)\n",
    "        The ductus arteriosus becomes the ligamentum arteriosum.\n",
    "\n",
    "        ---\n",
    "        **Context:**\n",
    "        {context}\n",
    "        ---\n",
    "        **Question:**\n",
    "        {question}\n",
    "        ---\n",
    "        **Answer:**\n",
    "        \"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def _call_llm_generate(self, prompt: str) -> str:\n",
    "        if self.llm_loader:\n",
    "            if hasattr(self.llm_loader, \"tokenizer\") and hasattr(self.llm_loader, \"model\"):\n",
    "                tokenizer = self.llm_loader.tokenizer\n",
    "                model = self.llm_loader.model\n",
    "\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    num_beams=3,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_ids = output[0][input_ids.shape[-1]:]\n",
    "                raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                return raw_answer\n",
    "            else:\n",
    "                raw_answer = self.llm_loader.generate(prompt)\n",
    "                return raw_answer\n",
    "        else:\n",
    "            print(\"generation_loader가 로드되지 않음\")\n",
    "            return \"LLM 로더가 설정되지 않았습니다.\"\n",
    "\n",
    "    def _expand_entities(self, initial_entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        if not initial_entities:\n",
    "            return []\n",
    "\n",
    "        initial_entity_names = [e['name'] for e in initial_entities]\n",
    "        prompt = f\"\"\"\n",
    "        Given the following list of medical or anatomical entities, provide a list of related or synonymous entities.\n",
    "        This will be used to improve search recall in a knowledge base.\n",
    "        Focus on providing closely related concepts, components, or alternative names.\n",
    "        Provide the output as a simple comma-separated list, without numbers or bullets.\n",
    "\n",
    "        Initial Entities: {', '.join(initial_entity_names)}\n",
    "\n",
    "        Expanded Entities:\n",
    "        \"\"\"\n",
    "\n",
    "        raw_llm_output = self._call_llm_generate(prompt.strip())\n",
    "\n",
    "        expanded_entity_names = {name.strip() for name in raw_llm_output.split(',') if name.strip()}\n",
    "\n",
    "        all_entity_names = set(initial_entity_names)\n",
    "        all_entity_names.update(expanded_entity_names)\n",
    "\n",
    "        final_entities = [{'name': name} for name in all_entity_names]\n",
    "\n",
    "        print(f\"초기 엔티티: {initial_entity_names}\")\n",
    "        print(f\"확장된 엔티티: {[e['name'] for e in final_entities]}\")\n",
    "\n",
    "        return final_entities\n",
    "\n",
    "    def generate_response(self, question: str) -> Tuple[str, str]:\n",
    "        # 질문에서 엔티티 추출\n",
    "        initial_entities, _ = self._extract_entities_relations(question)\n",
    "        if not initial_entities:\n",
    "            return \"질문에서 유효한 엔티티를 추출할 수 없습니다.\", \"\"\n",
    "\n",
    "        # 엔티티 확장\n",
    "        expanded_entities = self._expand_entities(initial_entities)\n",
    "\n",
    "        page_numbers = self._find_pages_from_entities(expanded_entities)\n",
    "        if not page_numbers :\n",
    "            return \"페이지를 찾을 수 없음\", \"\"\n",
    "        print(f\"찾은 페이지 : {page_numbers}\")\n",
    "\n",
    "        reranked_chunks = self._retrieve_and_rerank_context(question, page_numbers, top_k_rerank=5)\n",
    "        if not reranked_chunks:\n",
    "            return \"관련 페이지는 찾았으나, 질문과 직접적으로 연관된 문맥이 없음\", \"\"\n",
    "\n",
    "        final_context_parts = []\n",
    "        current_len = 0\n",
    "\n",
    "        if hasattr(self.llm_loader, 'tokenizer') and self.llm_loader.tokenizer is not None:\n",
    "            print(\"LLM 로더의 특정 토크나이저를 사용하여 길이를 계산합니다.\")\n",
    "            llm_tokenizer = self.llm_loader.tokenizer\n",
    "            max_len = getattr(llm_tokenizer, 'model_max_length', 4096) - 500\n",
    "\n",
    "            base_prompt = self._build_llm_prompt(question, \"\")\n",
    "            base_prompt_len = len(llm_tokenizer.tokenize(base_prompt))\n",
    "            current_len += base_prompt_len\n",
    "\n",
    "            for chunk in reranked_chunks:\n",
    "                page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "                context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "                chunk_token_len = len(llm_tokenizer.tokenize(context_snippet))\n",
    "\n",
    "                if current_len + chunk_token_len <= max_len:\n",
    "                    final_context_parts.append(context_snippet)\n",
    "                    current_len += chunk_token_len\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            print(\"범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\")\n",
    "            proxy_tokenizer = self.tokenizer  \n",
    "            max_len = 2048 - 500\n",
    "\n",
    "            for chunk in reranked_chunks:\n",
    "                page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "                context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "                chunk_token_len = len(proxy_tokenizer.tokenize(context_snippet))\n",
    "\n",
    "                if current_len + chunk_token_len <= max_len:\n",
    "                    final_context_parts.append(context_snippet)\n",
    "                    current_len += chunk_token_len\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if not final_context_parts:\n",
    "            return \"관련 정보를 찾았으나, 모델의 입력 길이 제한으로 인해 컨텍스트를 구성할 수 없습니다.\", \"\"\n",
    "\n",
    "        context = \"\\n\\n\".join(final_context_parts)\n",
    "\n",
    "        prompt = self._build_llm_prompt(question, context)\n",
    "\n",
    "        answer = self._call_llm_generate(prompt)\n",
    "        return answer, context\n",
    "\n",
    "def save_results_to_file(question: str, answer: str, context: str, output_dir: str, file_index: int):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%H%M%S_%f\")\n",
    "    file_name = f\"result_{file_index}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"[질문]\\n{question}\\n\\n\")\n",
    "        f.write(f\"[답변]\\n{answer}\\n\\n\")\n",
    "        f.write(f\"[근거]\\n{context}\\n\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qa_system = QASystem(\n",
    "        graphml_path=\"./data/knowledge_graph/knowledge_graph_1.graphml\",\n",
    "        md_path=\"./data/split_file/anatomy/\"\n",
    "    )\n",
    "    \n",
    "    qa_system.llm_loader = generation_loader\n",
    "    \n",
    "    questions = [\n",
    "        ############## 1_Embryology.md\n",
    "        \"What are the two essential components of a higher organism cell as defined in the text?\", # 7페이지\n",
    "        \"Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\", # 7페이지\n",
    "        \"What is the primary role of the yolk-sac in the embryo's early development?\", # 20페이지\n",
    "        \"How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\", # 19페이지\n",
    "        \"What significant developments occur in a human embryo during the Second Week?\", # 33페이지\n",
    "        \"What are the key characteristics of the human embryo by the end of the Third Week?\", # 33페이지\n",
    "        \n",
    "        ############## 2_Osteology.md\n",
    "        \"What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\", # 38페이지\n",
    "        \"How is each vertebral body formed from primitive segments during development?\", # 38페이지\n",
    "        \"What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\", # 88페이지\n",
    "        \"Describe the sphenoidal rostrum and its articulation.\",# 88\n",
    "        \"What is the tibia, and where is it located in the human leg?\", # 158\n",
    "        \"Describe the superior articular surface of the tibia's upper extremity.\", # 158\n",
    "\n",
    "        ############## 3_Syndesmology.md\n",
    "        \"What are joints or articulations, and how are immovable joints characterized?\", # 174\n",
    "        \"How does the articular lamella differ from ordinary bone tissue?\", # 174\n",
    "        \"Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\", # 207\n",
    "        \"List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\", # 207\n",
    "        \"What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\", # 236\n",
    "        \"How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\", # 236\n",
    "\n",
    "        ############## 4_Myology.md\n",
    "        \"How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\", # 250\n",
    "        \"Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\", # 250\n",
    "        \"What is the triangular ligament and where is it located?\", # 290\n",
    "        \"What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\", # 290\n",
    "        \"Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\", # 322\n",
    "        \"What is the Peronæus tertius, and where is it inserted?\", # 322\n",
    "\n",
    "        ############## 5_Angiology.md\n",
    "        \"What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\", # 334\n",
    "        \"Describe the composition and variations of the external coat (tunica adventitia) in arteries.\", # 334\n",
    "        \"How do the Vitelline Veins develop into parts of the portal and hepatic veins?\", # 345\n",
    "        \"What happens to the Umbilical Veins during embryonic development and after birth?\", # 345\n",
    "        \"What are the three phases of a cardiac cycle and what happens during each?\", # 358\n",
    "        \"What are the main peculiarities observed in the fetal heart's vascular system?\" # 359\n",
    "    ]   \n",
    "    today = datetime.now()\n",
    "    folder_name = f\"{today.month}월{today.day}일\"\n",
    "    output_dir = os.path.join(\"./result\", \"knowledge_graph\", folder_name, '2')\n",
    "    total_time = 0\n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"질문: {q}\")\n",
    "        start_time = time.time()\n",
    "        response, context = qa_system.generate_response(q)\n",
    "        end_time = time.time()\n",
    "        elapse_time = end_time - start_time\n",
    "        total_time += elapse_time\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"답변: {response}\\n\\n\\n\")\n",
    "        print(\"=\" * 30)\n",
    "        save_results_to_file(q, response, context, output_dir, i + 1)\n",
    "    avg_time = total_time / len(questions)\n",
    "    print(\"평균 답변 시간 : %.2f\" % avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab59d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
