{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bba8277",
   "metadata": {},
   "source": [
    "# 프로세스 순서\n",
    "1. _extract_entities_relations() : 사용자의 질문에서 핵심 엔티티/관계 추출\n",
    "2. _search_knowledge_graph() : 1번에서 추출된 단어를 바탕으로 지식그래프에서 유사한 단어를 찾고, 연결된 엣지를 가져옴\n",
    "3. _retrieve_and_rerank_context() : 2번에서 찾은 정보를 이용해 전체 문서에서 관련 있는 후보 문단을 가져온 뒤, Reranker 모델을 이용해 사용자 질문과 가장 관련성이 높은 순서대로 정렬\n",
    "4. generate_response() : 3번에서 정렬된 문단들을 관련성이 높은 것부터 LLM이 처리할 수 있는 최대 길이를 넘지 않도록 최종 context를 만듬\n",
    "5. _build_llm_prompt() : context와 질문을 정해진 템플릿에 포맷팅\n",
    "6. _call_llm_generate() : 최종 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92f2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729d2db4e8194ea99e7b061753ff6a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "using automatically assigned random_state=756960678\n",
      "No random seed is specified. Setting to 1382302168.\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB가 이미 초기화되어있음\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af4380c4333426a821268ee0dddc6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:0:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61781e250cb040538cc1d13cb12ec2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/827 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.07s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGE 모델 학습 완료\n",
      "질문: What are the two essential components of a higher organism cell as defined in the text?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: According to the text, the two essential components of a higher organism cell are cytoplasm and a nucleus (Page 6). The cytoplasm is described as a soft, jelly-like material, while the nucleus is a small spherical body embedded within it (Page 6).\n",
      "\n",
      "질문: Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\n",
      "답변: 관련 정보를 지식 그래프에서 찾을 수 없습니다.\n",
      "\n",
      "질문: What is the primary role of the yolk-sac in the embryo's early development?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The yolk-sac provides nourishment to the embryo during its earlier stages of existence (Page 20). It contains vitelline fluid which may be utilized for this nourishment (Page 20). Nutritive material is absorbed from the yolk-sac and conveyed to the embryo via the vitelline circulation (Page 20).\n",
      "\n",
      "질문: How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The embryo separates from the yolk-sac by a constriction forming between the embryo and the greater part of the yolk-sac (Page 19). This constriction encloses a small part of the yolk-sac, which constitutes the primitive digestive tube (Page 19).\n",
      "\n",
      "질문: What significant developments occur in a human embryo during the Second Week?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: During the second week of human embryonic development, the mesoderm outside the embryonic disk splits into two layers, creating an extra-embryonic coelom (Page 31). Four cavities then form within the embryo – two in the pericardial area and two in the lateral masses of the general mesoderm (Page 31). These cavities initially are separate but later become continuous with each other and the extra-embryonic coelom (Page 31). Specifically, the two cavities in the general mesoderm unite on the ventral aspect of the gut to form the pleuro-peritoneal cavity (Page 31), while the two pericardial cavities join to form a single pericardial cavity (Page 31).\n",
      "\n",
      "질문: What are the key characteristics of the human embryo by the end of the Third Week?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: By the end of the third week, the amnion is present as a closed sac in the human embryo (Page 22). It is roofed in by a single stratum of flattened, ectodermal cells, called the amniotic ectoderm, and its floor consists of the prismatic ectoderm of the embryonic disk (Page 22). Additionally, a transverse section of a human embryo of the third week shows the differentiation of the primitive segment (Page 38).\n",
      "\n",
      "질문: What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The cells of a primitive segment differentiate into three groups: the cutis-plate or dermatome, the muscle-plate or myotome, and the sclerotome (Page 38). These groups respectively form the cutis-plate or dermatome, the muscle-plate or myotome, and the sclerotome (Page 38).\n",
      "\n",
      "질문: How is each vertebral body formed from primitive segments during development?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: Each vertebral body is a composite of two segments, formed from the posterior portion of one segment and the anterior part of the segment immediately behind it (Page 38). Cells from the posterior mass grow into the intervals between the myotomes and extend both dorsally and ventrally to form the vertebral body (Page 38).\n",
      "\n",
      "질문: What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The sphenoidal air sinuses are two large cavities hollowed out within the body of the sphenoid bone (Page 87, 88). These sinuses are separated from each other by a bony septum (Page 87, 88). They are located within the body of the sphenoid bone and vary in size and shape (Page 626, 88). They communicate with the upper and back part of the nasal cavity through an opening (Page 88).\n",
      "\n",
      "질문: Describe the sphenoidal rostrum and its articulation.\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The sphenoid articulates with twelve bones: four single (vomer, ethmoid, frontal, and occipital) and four paired (parietal, temporal, zygomatic, and palatine) (Page 90).\n",
      "\n",
      "질문: What is the tibia, and where is it located in the human leg?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The Tibialis anterior is situated on the lateral side of the tibia; it is thick and fleshy above, and tendinous below (Page 321, 848). It projects beyond the anterior crest of the bone and its tendon can be traced on the front of the tibia and ankle-joint, along the medial side of the foot to the base of the first metatarsal bone (Page 848).\n",
      "\n",
      "질문: Describe the superior articular surface of the tibia's upper extremity.\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: I am sorry, but the provided context does not contain information about the tibia or its superior articular surface. The text focuses on the clavicle, muscles of the upper extremity, and the thoracic vertebrae (Page 124, Page 53, Page 845). Therefore, I cannot answer your question based on the given information.\n",
      "\n",
      "질문: What are joints or articulations, and how are immovable joints characterized?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: Diarthroses are freely movable articulations (Page 179). These joints have contiguous bony surfaces covered with articular cartilage and connected by ligaments lined by a synovial membrane (Page 179). Some joints may be divided by an articular disk or meniscus, with the periphery continuous with the fibrous capsule and free surfaces covered by synovial membrane (Page 179).\n",
      "\n",
      "The provided text does not describe how *immovable* joints are characterized. It primarily focuses on diarthroses (freely movable joints) and different types of movable articulations like arthrodial joints (Page 215), condyloid joints, and amphiarthrodial joints (Page 198).\n",
      "\n",
      "질문: How does the articular lamella differ from ordinary bone tissue?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: I am sorry, but the provided text does not contain information about how the articular lamella differs from ordinary bone tissue. Therefore, I cannot answer your question based on the given context.\n",
      "\n",
      "질문: Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The synovial membrane is reflected from the margin of the glenoid cavity over the labrum and covers the lower part and sides of the anatomical neck of the humerus as far as the articular cartilage on the head of the bone (Page 207). The tendon of the long head of the Biceps brachii passes through the capsule and is enclosed in a tubular sheath of synovial membrane, which is reflected upon it from the summit of the glenoid cavity and continues around the tendon into the intertubercular groove as far as the surgical neck of the humerus (Page 207). The tendon thus traverses the articulation, but is not contained within the synovial cavity (Page 207).\n",
      "\n",
      "질문: List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The provided text does not contain information about bursae located near the shoulder-joint, nor does it specify which ones communicate with the synovial cavity (Page 206, 207, 300). It only details the synovial membrane's reflection over the glenoid cavity, capsule, and humerus, and notes the passage of the long head of the Biceps brachii tendon through the capsule enclosed in a synovial sheath (Page 207).\n",
      "\n",
      "질문: What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The plantar calcaneonavicular ligament connects the anterior margin of the sustentaculum tali of the calcaneus to the plantar surface of the navicular (Page 235). It not only connects the calcaneus and navicular but also supports the head of the talus, forming part of the articular cavity in which it is received (Page 235). \n",
      "\n",
      "The provided text does not mention any condition that results if the ligament yields.\n",
      "\n",
      "질문: How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The navicular is connected to the three cuneiform bones by dorsal and plantar ligaments (Page 236). The three cuneiform bones and the cuboid are connected by dorsal, plantar, and interosseous ligaments (Page 236). These are arthrodial joints, permitting gliding movements (Page 238).\n",
      "\n",
      "질문: How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The provided text does not contain information about how the nervous system indicates the origin and migration paths of developing muscles, nor does it discuss the relationship between the nervous system and muscle differentiation. Therefore, I cannot answer this question based on the given context.\n",
      "\n",
      "질문: Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The provided text does not contain information about the structural components of striped or voluntary muscle from bundles to individual fibers. Therefore, I cannot answer your question using the provided context. I am answering based on the graph structure alone.\n",
      "\n",
      "질문: What is the triangular ligament and where is it located?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The provided text does not contain information about a \"triangular ligament\". It details the Temporomandibular Ligament (Page 189), Stylomandibular Ligament (Page 190), Sphenomandibular Ligament (Page 189), anterior ligament of the malleus (Page 657), and medial palpebral ligament (Page 257), but does not mention a triangular ligament. Therefore, I cannot answer your question based on the provided context.\n",
      "\n",
      "질문: What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The deep dorsal vein of the penis perforates the superficial layer (inferior fascia) of the urogenital diaphragm through an oval opening (Page 290).\n",
      "\n",
      "질문: Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The Extensor digitorum longus originates from the lateral condyle of the tibia, the upper three-fourths of the anterior surface of the fibula, the upper part of the interosseous membrane, the deep surface of the fascia, and the intermuscular septa between it and the Tibialis anterior on the medial side and the Peronæi on the lateral side (Page 322). \n",
      "\n",
      "Between the Extensor digitorum longus and the Tibialis anterior are the upper portions of the anterior tibial vessels and deep peroneal nerve (Page 322). In the upper third of its course, the anterior tibial artery lies between the Tibialis anterior and Extensor digitorum longus (Page 419).\n",
      "\n",
      "질문: What is the Peronæus tertius, and where is it inserted?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The Peronæus tertius is a part of the Extensor digitorum longus, and might be described as its fifth tendon (Page 322). It arises from the lower third or more of the anterior surface of the fibula, from the lower part of the interosseous membrane, and from an intermuscular septum between it and the Peronæus brevis (Page 322). The tendon is inserted into the dorsal surface of the base of the metatarsal bone of the little toe (Page 322). It passes under the transverse and cruciate crural ligaments along with the Extensor digitorum longus (Page 322, 326).\n",
      "\n",
      "질문: What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The middle coat (tunica media) is distinguished from the inner coat by its color and the transverse arrangement of its fibers (Page 334). In smaller arteries, it consists mainly of plain muscle fibers in fine bundles, arranged in lamellae and disposed circularly around the vessel, with the smallest arteries having only a single layer (Page 334). In larger arteries, like the iliac, femoral, and carotid, elastic fibers unite to form lamellae that alternate with the layers of muscular fibers, connected by elastic fibers to the inner coat's fenestrated membrane (Page 334). In the largest arteries, such as the aorta and innominate, there's a considerable amount of elastic tissue, along with some white connective tissue bundles (Page 334).\n",
      "\n",
      "질문: Describe the composition and variations of the external coat (tunica adventitia) in arteries.\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The external coat (tunica adventitia) consists mainly of fine and closely felted bundles of white connective tissue, but also contains elastic fibers (Page 334). The elastic tissue is more abundant next to the tunica media, sometimes forming a layer called the tunica elastica externa of Henle (Page 334). This layer is most marked in arteries of medium size (Page 334). In the largest vessels, the external coat is relatively thin, while in small arteries, it is of greater proportionate thickness (Page 334). In the smallest arteries, it consists of a single layer of white connective tissue and elastic fibers; nearing the capillaries, the connective tissue becomes more homogeneous and gradually reduces to a thin membranous envelope that eventually disappears (Page 334).\n",
      "\n",
      "질문: How do the Vitelline Veins develop into parts of the portal and hepatic veins?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The portions of the Vitelline Veins above the upper ring are interrupted by the developing liver and broken up into a plexus of small capillary-like vessels termed sinusoid s (Page 345). These branches become the branches of the portal vein (Page 345). The vessels draining this plexus into the sinus venosus are termed the venæ revehentes, and form the future hepatic veins (Page 345). The persistent part of the upper venous ring, above the opening of the superior mesenteric vein, forms the trunk of the portal vein (Page 345).\n",
      "\n",
      "질문: What happens to the Umbilical Veins during embryonic development and after birth?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: During embryonic development, the two umbilical veins fuse to form a single trunk (Page 23). The right umbilical vein shrivels up and disappears (Page 340, 345), while the left umbilical vein becomes enlarged (Page 345). After birth, the umbilical veins completely obliterate between the second and fifth days (Page 360), and the left umbilical vein forms the ligamentum teres (Page 360). Additionally, the ductus venosus, which is related to the umbilical veins, also undergoes obliteration and forms the ligamentum venosum of the liver (Page 360).\n",
      "\n",
      "질문: What are the three phases of a cardiac cycle and what happens during each?\n",
      "답변: 관련 정보를 지식 그래프에서 찾을 수 없습니다.\n",
      "\n",
      "질문: What are the main peculiarities observed in the fetal heart's vascular system?\n",
      "범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\n",
      "답변: The provided context does not contain information about the fetal heart's vascular system. Therefore, I cannot answer your question based on the given text. I am answering based on the graph structure alone.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk.downloader\n",
    "from datetime import datetime\n",
    "from pykeen.models import ComplEx\n",
    "from model_loader.config import *\n",
    "from pykeen.pipeline import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pykeen.triples import TriplesFactory\n",
    "from sentence_transformers import CrossEncoder\n",
    "from pykeen.optimizers import AdamW as PyKeenAdamW\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "generation_loader = generation_loader\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, \n",
    "                 graphml_path: str, \n",
    "                 md_path: str,\n",
    "                 vector_db_path: str = \"./chroma_db_split\", \n",
    "                 similarity_threshold: float = 0.5,\n",
    "                 chunk_token_threshold: int = 500):\n",
    "        self.graphml_path = graphml_path\n",
    "        self.md_path = md_path\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.chunk_token_threshold = chunk_token_threshold\n",
    "        self.llm_loader = None\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "        self.graph = nx.read_graphml(graphml_path)\n",
    "        self.client = chromadb.PersistentClient(path=vector_db_path)\n",
    "        \n",
    "        self.entity_collection = self.client.get_or_create_collection(name=\"entities_split\")\n",
    "        self.relation_collection = self.client.get_or_create_collection(name=\"relations_split\")\n",
    "        self.chunk_collection = self.client.get_or_create_collection(name=\"chunks\")\n",
    "        self.entity_relation_extraction_prompt_template = \"\"\"\n",
    "            Extract entities and their relations from the following sentence.\n",
    "\n",
    "            **Entities** should be **unique nouns or concepts**, extracted as **noun phrases** whenever possible. Identify **concrete objects or concepts** rather than complex activities or phenomena as entities.\n",
    "\n",
    "            **Relations** should clearly describe the connection between two entities, preferring **reusable predicate verbs** for a knowledge graph. Use **concise verbs** or clear, hyphenated forms like **'part_of' or 'includes'**.\n",
    "\n",
    "            Output the result **only in the following JSON format**, with no other explanations or text:\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"entities\": [\n",
    "                    {{\"name\": \"Entity1\", \"type\": \"Type (e.g., Organ, System, Substance, Function, Disease)\"}},\n",
    "                    {{\"name\": \"Entity2\", \"type\": \"Type\"}}\n",
    "                ],\n",
    "                \"relations\": [\n",
    "                    {{\"head\": \"Entity1\", \"relation\": \"Relation_Type (e.g., part_of, causes)\", \"tail\": \"Entity2\"}},\n",
    "                    {{\"head\": \"Entity3\", \"relation\": \"generates\", \"tail\": \"Entity4\"}}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            sentence : \"{text_to_analyze}\"\n",
    "            JSON result :\n",
    "        \"\"\"\n",
    "\n",
    "        self._initialize_vector_db()\n",
    "        self._initialize_chunk_db()\n",
    "        self.kge_model, self.triples_factory = self._train_kge_model()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        return text.upper().replace(' ', '_')\n",
    "\n",
    "    def _create_chunks_from_text(self, text: str, page_num: str) -> List[Dict[str, Any]] :\n",
    "        chunks = []\n",
    "        paragraphs = re.split(\"\\n\\n+\", text)\n",
    "        for para in paragraphs :\n",
    "            para = para.strip()\n",
    "            if not para :\n",
    "                continue\n",
    "\n",
    "            para_tokens = self.tokenizer.tokenize(para)\n",
    "\n",
    "            if len(para_tokens) <= self.chunk_token_threshold :\n",
    "                chunks.append({\"document\": para, \"metadata\": {\"source_page\": page_num}})\n",
    "            else :\n",
    "                sentences = sent_tokenize(para)\n",
    "                current_chunk_sentences = []\n",
    "                current_chunk_tokens = 0\n",
    "\n",
    "                for sentence in sentences :\n",
    "                    sentence_tokens = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "                    if current_chunk_tokens + len(sentence_tokens) > self.chunk_token_threshold and current_chunk_sentences :\n",
    "                        chunk_text = \" \".join(current_chunk_sentences)\n",
    "                        chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "                        current_chunk_sentences = [sentence]\n",
    "                        current_chunk_tokens = len(sentence_tokens)\n",
    "                    else :\n",
    "                        current_chunk_sentences.append(sentence)\n",
    "                        current_chunk_tokens += len(sentence_tokens)\n",
    "\n",
    "                if current_chunk_sentences :\n",
    "                    chunk_text = \" \".join(current_chunk_sentences)\n",
    "                    chunks.append({\"document\": chunk_text, \"metadata\": {\"source_page\": page_num}})\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def _initialize_chunk_db(self) :\n",
    "        if self.chunk_collection.count() > 0 :\n",
    "            print(\"DB가 이미 초기화되어있음\")\n",
    "            return\n",
    "        \n",
    "        print(\"청크 DB 초기화 시작\")\n",
    "        all_chunks = []\n",
    "        all_md_files = [f for f in os.listdir(self.md_path) if f.endswith(\".md\")]\n",
    "\n",
    "        for md_file in all_md_files :\n",
    "            with open(os.path.join(self.md_path, md_file), 'r', encoding=\"utf-8\") as f :\n",
    "                content = f.read()\n",
    "\n",
    "            page_matches = re.finditer(r\"####\\s+Page\\s+(\\d+)\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "            for match in page_matches :\n",
    "                page_num = match.group(1).strip()\n",
    "                page_content = match.group(2).strip()\n",
    "                if page_content :\n",
    "                    chunks = self._create_chunks_from_text(page_content, page_num)\n",
    "                    all_chunks.extend(chunks)\n",
    "\n",
    "        if all_chunks :\n",
    "            documents = [chunk[\"document\"] for chunk in all_chunks]\n",
    "            metadatas = [chunk[\"metadata\"] for chunk in all_chunks]\n",
    "\n",
    "            ids = [f\"chunk_{i}_{datetime.now().timestamp()}\" for i in range(len(documents))]\n",
    "            self.chunk_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "        print(f\"청크DB 초기화 완료. {self.chunk_collection.count()}개의 청크 추가\")\n",
    "\n",
    "\n",
    "    def _initialize_vector_db(self):\n",
    "        if self.entity_collection.count() == 0:\n",
    "            nodes_to_add = []\n",
    "            unique_nodes = set()\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                processed_node = self._preprocess_text(node)\n",
    "                if processed_node not in unique_nodes :\n",
    "                    metadata = {k: str(v) for k, v in data.items()}\n",
    "                    metadata['original_name'] = node\n",
    "                    nodes_to_add.append({'id': processed_node, 'document': processed_node, 'metadata': metadata})\n",
    "                    unique_nodes.add(processed_node)\n",
    "            \n",
    "            if nodes_to_add:\n",
    "                ids = [item['id'] for item in nodes_to_add]\n",
    "                documents = [item['document'] for item in nodes_to_add]\n",
    "                metadatas = [item['metadata'] for item in nodes_to_add]\n",
    "                self.entity_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "        if self.relation_collection.count() == 0:\n",
    "            edges_to_add = []\n",
    "            unique_processed_relations = set()\n",
    "            for u, v, data in self.graph.edges(data=True):\n",
    "                relation_type = data.get('type')\n",
    "                if relation_type:\n",
    "                    processed_relation = self._preprocess_text(relation_type)\n",
    "                    if processed_relation not in unique_processed_relations :\n",
    "                        metadata = {'original_name': relation_type}\n",
    "                        edges_to_add.append({'id': processed_relation, 'document': processed_relation, 'metadata': metadata})\n",
    "                        unique_processed_relations.add(processed_relation)\n",
    "\n",
    "            if edges_to_add:\n",
    "                ids = [item['id'] for item in edges_to_add]\n",
    "                documents = [item['document'] for item in edges_to_add]\n",
    "                metadatas = [item['metadata'] for item in edges_to_add]\n",
    "                self.relation_collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    \n",
    "    def _train_kge_model(self) :\n",
    "        triples = []\n",
    "        for u, v, data in self.graph.edges(data=True) :\n",
    "            relation_type = data.get(\"type\")\n",
    "            if relation_type and isinstance(relation_type, str):\n",
    "                triples.append((str(u), str(relation_type), str(v)))\n",
    "\n",
    "        if not triples :\n",
    "            print(\"KGE 모델 학습을 위한 트리플이 없음\")\n",
    "            return None, None\n",
    "        # print(f\"생성된 트리플 : {len(triples)}\")\n",
    "        # if len(triples) > 0 :\n",
    "        #     print(f\"첫 5개 : {triples[:5]}\")\n",
    "\n",
    "        triples_array = np.array(triples)\n",
    "        # print(f\"Numpy 배열의 형태 : {triples_array.shape}\")\n",
    "\n",
    "        training_triples_factory = TriplesFactory.from_labeled_triples(\n",
    "            triples=triples_array,\n",
    "            create_inverse_triples=True\n",
    "        )\n",
    "        # print(f\"TriplesFactory 생성 완료. 엔티티 수 : {training_triples_factory.num_entities}, 관계 수 : {training_triples_factory.num_relations}\")\n",
    "\n",
    "        training_set, testing_set = training_triples_factory.split()\n",
    "\n",
    "        result = pipeline(\n",
    "            training=training_set,\n",
    "            testing=testing_set,\n",
    "            model=ComplEx,\n",
    "            optimizer=PyKeenAdamW,\n",
    "            training_kwargs=dict(num_epochs=100, batch_size=256, use_tqdm_batch=False),\n",
    "            optimizer_kwargs=dict(lr=0.01),\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        print(\"KGE 모델 학습 완료\")\n",
    "        return result.model, training_triples_factory\n",
    "    \n",
    "    def _get_kge_embedding(self, entity_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if entity_name in self.triples_factory.entity_to_id :\n",
    "            entity_id = self.triples_factory.entity_to_id[entity_name]\n",
    "            return self.kge_model.entity_representations[0](torch.tensor([entity_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "    \n",
    "    def _get_kge_relation_embedding(self, relation_name: str) -> Optional[torch.Tensor] :\n",
    "        if self.kge_model is None or self.triples_factory is None :\n",
    "            return None\n",
    "        \n",
    "        if relation_name in self.triples_factory.relation_to_id :\n",
    "            relation_id = self.triples_factory.relation_to_id[relation_name]\n",
    "            return self.kge_model.relation_representations[0](torch.tensor([relation_id], device=self.kge_model.device)).real.detach().cpu()\n",
    "        return None\n",
    "\n",
    "    def _extract_entities_relations(self, question) :\n",
    "        prompt = self.entity_relation_extraction_prompt_template.format(text_to_analyze=question)\n",
    "        raw_llm_output = self._call_llm_generate(prompt)\n",
    "\n",
    "        try :\n",
    "            json_start = raw_llm_output.find(\"{\")\n",
    "            json_end = raw_llm_output.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1 and json_end > json_start :\n",
    "                json_str = raw_llm_output[json_start:json_end]\n",
    "                extracted_data = json.loads(json_str)\n",
    "                return extracted_data.get(\"entities\", []), extracted_data.get(\"relations\", [])\n",
    "            else :\n",
    "                print(f\"LLM 답변에서 유효한 JSON 형태를 찾을 수 없음 : {raw_llm_output}\")\n",
    "                return [], []\n",
    "            \n",
    "        except json.JSONDecodeError as e :\n",
    "            print(f\"개체 추출 과정에서 JSON 디코딩 오류 발생: {e}\")\n",
    "            print(f\"오류 발생 원문: {raw_llm_output}\")\n",
    "            return [], []\n",
    "\n",
    "    def _search_knowledge_graph(self, entities: List[str]) -> List[Dict[str, Any]]:\n",
    "        if not entities:\n",
    "            return []\n",
    "\n",
    "        # 엔티티 리스트\n",
    "        query_texts = [e['name'] for e in entities if 'name' in e]\n",
    "        if not query_texts:\n",
    "            return []\n",
    "\n",
    "        # 벡터DB를 이용해 유사 엔티티 검색\n",
    "        entity_results = self.entity_collection.query(\n",
    "            query_texts=query_texts,\n",
    "            n_results=5,\n",
    "            include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        # 유사도 거리가 작은 엔티티 필터링\n",
    "        similar_entities = set()\n",
    "        if entity_results.get('distances'):\n",
    "            for i, dists in enumerate(entity_results['distances']):\n",
    "                for j, dist in enumerate(dists):\n",
    "                    if dist <= self.similarity_threshold:\n",
    "                        meta = entity_results['metadatas'][i][j]\n",
    "                        similar_entities.add(meta['original_name'])\n",
    "        \n",
    "        found_results = []\n",
    "        seen = set()\n",
    "        # u : 시작노드, v : 끝노드, data : 엣지의 데이터\n",
    "        for u, v, data in self.graph.edges(data=True): # 그래프 객체를 순회하며 유사 엔티티를 찾음\n",
    "            if u in similar_entities or v in similar_entities:\n",
    "                identifier = (u, v, data.get('type'))\n",
    "                if identifier not in seen:\n",
    "                    result = data.copy()\n",
    "                    result['source_node'] = u\n",
    "                    result['target_node'] = v\n",
    "                    found_results.append(result)\n",
    "                    seen.add(identifier)\n",
    "                    \n",
    "        return found_results\n",
    "\n",
    "    def _retrieve_and_rerank_context(self, question: str, search_results: List[Dict[str, Any]], top_k_retrieval: int = 20, top_k_rerank: int = 5) -> List[Dict[str, Any]]:\n",
    "        mentioned_entities = set()\n",
    "        for res in search_results: # 검색된 결과의 양 끝 노드 추가\n",
    "            mentioned_entities.add(res.get('source_node'))\n",
    "            mentioned_entities.add(res.get('target_node'))\n",
    "        \n",
    "        mentioned_entities = {e for e in mentioned_entities if e}\n",
    "\n",
    "        if not mentioned_entities:\n",
    "            return []\n",
    "\n",
    "        query_text = \" \".join(list(mentioned_entities))\n",
    "        \n",
    "        results = self.chunk_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=top_k_retrieval,\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        candidate_chunks = []\n",
    "        seen_chunks = set()\n",
    "        if results['documents'] and results['documents'][0]:\n",
    "            for i in range(len(results['documents'][0])):\n",
    "                doc = results['documents'][0][i]\n",
    "                if doc not in seen_chunks:\n",
    "                    candidate_chunks.append({\n",
    "                        \"document\": doc,\n",
    "                        \"metadata\": results['metadatas'][0][i]\n",
    "                    })\n",
    "                    seen_chunks.add(doc)\n",
    "\n",
    "        if not candidate_chunks:\n",
    "            return []\n",
    "\n",
    "        rerank_pairs = [(question, chunk['document']) for chunk in candidate_chunks]\n",
    "        if not rerank_pairs:\n",
    "            return []\n",
    "\n",
    "        scores = self.reranker.predict(rerank_pairs)\n",
    "\n",
    "        reranked_results = []\n",
    "        for score, chunk in zip(scores, candidate_chunks) :\n",
    "            chunk[\"rerank_score\"] = score\n",
    "            reranked_results.append(chunk)\n",
    "\n",
    "        reranked_results.sort(key=lambda x : x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "        return reranked_results[:top_k_rerank]\n",
    "    \n",
    "    # def _build_llm_prompt(self, question: str, context: str, pages: List[str]) -> str:\n",
    "    def _build_llm_prompt(self, question: str, context: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant who answers questions based on the provided context.\n",
    "        You MUST cite the source page number for every piece of information you use.\n",
    "\n",
    "        **Instructions:**\n",
    "        1. Answer the user's question clearly and concisely using ONLY the provided context and knowledge graph information.\n",
    "        2. For every statement, you MUST provide the source page number in parentheses, like this: (Page XX).\n",
    "        3. If a single piece of information is supported by multiple pages, cite all of them: (Page X, Y, Z).\n",
    "        4. If no context is available, state that you are answering based on the graph structure alone.\n",
    "\n",
    "        **Example of a GOOD answer:**\n",
    "        The ductus arteriosus degenerates into the ligamentum arteriosum after birth(page 360). This is a normal physiological change that happens post-delivery(page 361).\n",
    "\n",
    "        **Example of a BAD answer:** -> (This is a bad answer because it lacks the mandatory citation)\n",
    "        The ductus arteriosus becomes the ligamentum arteriosum.\n",
    "\n",
    "        ---\n",
    "        **Context:**\n",
    "        {context}\n",
    "        ---\n",
    "        **Question:**\n",
    "        {question}\n",
    "        ---\n",
    "        **Answer:**\n",
    "        \"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def _call_llm_generate(self, prompt: str) -> str:\n",
    "        if self.llm_loader:\n",
    "            if hasattr(self.llm_loader, \"tokenizer\") and hasattr(self.llm_loader, \"model\"):\n",
    "                tokenizer = self.llm_loader.tokenizer\n",
    "                model = self.llm_loader.model\n",
    "\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    num_beams=3,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_ids = output[0][input_ids.shape[-1]:]\n",
    "                raw_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                return raw_answer\n",
    "            else:\n",
    "                raw_answer = self.llm_loader.generate(prompt)\n",
    "                return raw_answer\n",
    "        else:\n",
    "            print(\"generation_loader가 로드되지 않음\")\n",
    "            return \"LLM 로더가 설정되지 않았습니다.\"\n",
    "\n",
    "    def generate_response(self, question: str) -> Tuple[str, str]:\n",
    "        entities, relations = self._extract_entities_relations(question)\n",
    "        if not entities and not relations:\n",
    "            return \"질문에서 유효한 엔티티나 릴레이션을 추출할 수 없습니다.\", \"\"\n",
    "        \n",
    "        search_results = self._search_knowledge_graph(entities)\n",
    "        if not search_results:\n",
    "            return \"관련 정보를 지식 그래프에서 찾을 수 없습니다.\", \"\"\n",
    "            \n",
    "        reranked_chunks = self._retrieve_and_rerank_context(question, search_results)\n",
    "        if not reranked_chunks:\n",
    "            no_context_message = \"지식 그래프에서 관련 엔티티를 찾았지만, 문서에서 해당 내용을 포함하는 구체적인 컨텍스트를 찾을 수 없습니다.\"\n",
    "            simple_answer_parts = []\n",
    "            for res in search_results[:3] :\n",
    "                simple_answer_parts.append(f\"{res['source_node']} -[{res.get('type')}]-> {res['target_node']}\")\n",
    "            if simple_answer_parts :\n",
    "                no_context_message += \"\\n그래프 기반 정보: \" + \", \".join(simple_answer_parts)\n",
    "            return no_context_message, \"\"\n",
    "        \n",
    "        final_context_parts = []\n",
    "        final_pages = set()\n",
    "        current_len = 0\n",
    "\n",
    "        if hasattr(self.llm_loader, 'tokenizer') and self.llm_loader.tokenizer is not None:\n",
    "            print(\"LLM 로더의 특정 토크나이저를 사용하여 길이를 계산합니다.\")\n",
    "            llm_tokenizer = self.llm_loader.tokenizer\n",
    "            max_len = getattr(llm_tokenizer, 'model_max_length', 512) - 150\n",
    "            \n",
    "            base_prompt = self._build_llm_prompt(question, \"\")\n",
    "            base_prompt_len = len(llm_tokenizer.tokenize(base_prompt))\n",
    "            current_len += base_prompt_len\n",
    "\n",
    "            for chunk in reranked_chunks:\n",
    "                page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "                context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "                chunk_token_len = len(llm_tokenizer.tokenize(context_snippet))\n",
    "                \n",
    "                if current_len + chunk_token_len <= max_len:\n",
    "                    final_context_parts.append(context_snippet)\n",
    "                    current_len += chunk_token_len\n",
    "                    if page_num != 'N/A':\n",
    "                        final_pages.add(page_num)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            print(\"범용 토크나이저를 사용하여 길이를 근사치로 계산합니다. (Ollama 등)\")\n",
    "            proxy_tokenizer = self.tokenizer  \n",
    "            max_len = 2048 - 500 \n",
    "\n",
    "            for chunk in reranked_chunks:\n",
    "                page_num = chunk['metadata'].get('source_page', 'N/A')\n",
    "                context_snippet = f\"... {chunk['document']} ... (출처: Page {page_num})\"\n",
    "                chunk_token_len = len(proxy_tokenizer.tokenize(context_snippet))\n",
    "                \n",
    "                if current_len + chunk_token_len <= max_len:\n",
    "                    final_context_parts.append(context_snippet)\n",
    "                    current_len += chunk_token_len\n",
    "                    if page_num != 'N/A':\n",
    "                        final_pages.add(page_num)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if not final_context_parts:\n",
    "            return \"관련 정보를 찾았으나, 모델의 입력 길이 제한으로 인해 컨텍스트를 구성할 수 없습니다.\", \"\"\n",
    "\n",
    "        context = \"\\n\\n\".join(final_context_parts)\n",
    "        \n",
    "        prompt = self._build_llm_prompt(question, context)\n",
    "        \n",
    "        answer = self._call_llm_generate(prompt)\n",
    "        return answer, context\n",
    "\n",
    "def save_results_to_file(question: str, answer: str, context: str, output_dir: str, file_index: int):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%H%M%S_%f\")\n",
    "    file_name = f\"result_{file_index}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"[질문]\\n{question}\\n\\n\")\n",
    "        f.write(f\"[근거]\\n{context}\\n\\n\")\n",
    "        f.write(f\"[답변]\\n{answer}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qa_system = QASystem(\n",
    "        graphml_path=\"./data/knowledge_graph/knowledge_graph.graphml\",\n",
    "        md_path=\"./data/split_file/anatomy/\"\n",
    "    )\n",
    "    \n",
    "    qa_system.llm_loader = generation_loader\n",
    "    \n",
    "    questions = [\n",
    "        ############## 1_Embryology.md\n",
    "        \"What are the two essential components of a higher organism cell as defined in the text?\", # 7페이지\n",
    "        \"Describe the four main phases of indirect cell division (karyokinesis) as outlined in the text.\", # 7페이지\n",
    "        \"What is the primary role of the yolk-sac in the embryo's early development?\", # 20페이지\n",
    "        \"How does the embryo separate from the yolk-sac, and what does the enclosed part of the yolk-sac form?\", # 19페이지\n",
    "        \"What significant developments occur in a human embryo during the Second Week?\", # 33페이지\n",
    "        \"What are the key characteristics of the human embryo by the end of the Third Week?\", # 33페이지\n",
    "        \n",
    "        ############## 2_Osteology.md\n",
    "        \"What are the three groups into which the cells of a primitive segment differentiate, and what do they form?\", # 38페이지\n",
    "        \"How is each vertebral body formed from primitive segments during development?\", # 38페이지\n",
    "        \"What are the sphenoidal air sinuses, and where are they located within the sphenoid bone?\", # 88페이지\n",
    "        \"Describe the sphenoidal rostrum and its articulation.\",# 88\n",
    "        \"What is the tibia, and where is it located in the human leg?\", # 158\n",
    "        \"Describe the superior articular surface of the tibia's upper extremity.\", # 158\n",
    "\n",
    "        ############## 3_Syndesmology.md\n",
    "        \"What are joints or articulations, and how are immovable joints characterized?\", # 174\n",
    "        \"How does the articular lamella differ from ordinary bone tissue?\", # 174\n",
    "        \"Where is the synovial membrane located in relation to the glenoid cavity and humerus, and how does it interact with the Biceps brachii tendon?\", # 207\n",
    "        \"List some of the bursae located near the shoulder-joint and specify which ones communicate with the synovial cavity.\", # 207\n",
    "        \"What is the function of the plantar calcaneonavicular ligament, and what condition results if it yields?\", # 236\n",
    "        \"How are the navicular bone and the three cuneiform bones connected, and what type of movement do they permit?\", # 236\n",
    "\n",
    "        ############## 4_Myology.md\n",
    "        \"How does the nervous system serve as an indicator for the origin and migration paths of developing muscles, despite not influencing muscle differentiation?\", # 250\n",
    "        \"Describe the structural components of striped or voluntary muscle, from bundles to individual fibers.\", # 250\n",
    "        \"What is the triangular ligament and where is it located?\", # 290\n",
    "        \"What structures perforate the superficial layer (inferior fascia) of the urogenital diaphragm?\", # 290\n",
    "        \"Where does the Extensor digitorum longus muscle originate, and what structures are located between it and the Tibialis anterior?\", # 322\n",
    "        \"What is the Peronæus tertius, and where is it inserted?\", # 322\n",
    "\n",
    "        ############## 5_Angiology.md\n",
    "        \"What are the main characteristics of the middle coat (tunica media) of arteries, and how does its composition vary with vessel size?\", # 334\n",
    "        \"Describe the composition and variations of the external coat (tunica adventitia) in arteries.\", # 334\n",
    "        \"How do the Vitelline Veins develop into parts of the portal and hepatic veins?\", # 345\n",
    "        \"What happens to the Umbilical Veins during embryonic development and after birth?\", # 345\n",
    "        \"What are the three phases of a cardiac cycle and what happens during each?\", # 358\n",
    "        \"What are the main peculiarities observed in the fetal heart's vascular system?\" # 359\n",
    "    ]   \n",
    "\n",
    "    today = datetime.now()\n",
    "    folder_name = f\"{today.month}월{today.day}일\"\n",
    "    output_dir = os.path.join(\"./result\", \"knowledge_graph\", folder_name)\n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"질문: {q}\")\n",
    "        response, context = qa_system.generate_response(q)\n",
    "        print(f\"답변: {response}\\n\")\n",
    "        save_results_to_file(q, response, context, output_dir, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _search_knowledge_graph(self, entities: List[str], relations: List[str]) -> List[Dict[str, Any]]:\n",
    "    #     processed_entities = [self._preprocess_text(e[\"name\"]) for e in entities if \"name\" in e]\n",
    "    #     processed_relations = [self._preprocess_text(r[\"relation\"]) for r in relations if \"relation\" in r]\n",
    "\n",
    "    #     found_results = []\n",
    "\n",
    "    #     similar_entities = []\n",
    "    #     if processed_entities:\n",
    "    #         entity_results = self.entity_collection.query(\n",
    "    #             query_texts=processed_entities,\n",
    "    #             n_results=2, \n",
    "    #             include=[\"metadatas\", \"distances\"]\n",
    "    #         )\n",
    "    #         if entity_results['distances']:\n",
    "    #             for i, dists in enumerate(entity_results['distances']):\n",
    "    #                 for j, dist in enumerate(dists):\n",
    "    #                     if dist <= self.similarity_threshold:\n",
    "    #                         meta = entity_results['metadatas'][i][j]\n",
    "    #                         similar_entities.append(meta['original_name'])\n",
    "        \n",
    "    #     similar_relations = []\n",
    "    #     if processed_relations:\n",
    "    #         relation_results = self.relation_collection.query(\n",
    "    #             query_texts=processed_relations,\n",
    "    #             n_results=1,\n",
    "    #             include=[\"metadatas\", \"distances\"]\n",
    "    #         )\n",
    "    #         if relation_results['distances'] and relation_results['distances'][0]:\n",
    "    #             if relation_results['distances'][0][0] <= self.similarity_threshold:\n",
    "    #                 meta = relation_results['metadatas'][0][0]\n",
    "    #                 similar_relations.append(meta['original_name'])\n",
    "\n",
    "    #     similar_entities = list(set(similar_entities))\n",
    "\n",
    "    #     inferred_relations = []\n",
    "    #     if self.kge_model and entities :\n",
    "    #         for entity_data in entities :\n",
    "    #             entity_name = entity_data[\"name\"]\n",
    "    #             head_emb = self._get_kge_embedding(entity_name)\n",
    "    #             if head_emb is not None :\n",
    "    #                 for rel_name in self.triples_factory.relation_to_id.keys() :\n",
    "    #                     rel_emb = self._get_kge_relation_embedding(rel_name)\n",
    "    #                     if rel_emb is not None :\n",
    "    #                         if rel_name not in similar_relations :\n",
    "    #                             inferred_relations.append(rel_name)\n",
    "\n",
    "    #     all_relevant_relations = list(set(similar_relations + inferred_relations))\n",
    "\n",
    "    #     if similar_entities and all_relevant_relations:\n",
    "    #         for u, v, data in self.graph.edges(data=True):\n",
    "    #             if (u in similar_entities or v in similar_entities) and data.get('type') in all_relevant_relations:\n",
    "    #                 result = data.copy()\n",
    "    #                 result['source_node'] = u\n",
    "    #                 result['target_node'] = v\n",
    "    #                 if 'source_page' in result:\n",
    "    #                     found_results.append(result)\n",
    "        \n",
    "    #     if not found_results and similar_entities:\n",
    "    #         for u, v, data in self.graph.edges(data=True):\n",
    "    #             if u in similar_entities or v in similar_entities:\n",
    "    #                 result = data.copy()\n",
    "    #                 result['source_node'] = u\n",
    "    #                 result['target_node'] = v\n",
    "    #                 if 'source_page' in result:\n",
    "    #                     found_results.append(result)\n",
    "        \n",
    "    #     if not found_results:\n",
    "    #         return []\n",
    "            \n",
    "    #     unique_results = []\n",
    "    #     seen = set()\n",
    "    #     for res in found_results:\n",
    "    #         identifier = (res.get('source_node'), res.get('target_node'), res.get('type'))\n",
    "    #         if identifier not in seen:\n",
    "    #             unique_results.append(res)\n",
    "    #             seen.add(identifier)\n",
    "                \n",
    "    #     return unique_results\n",
    "    \n",
    "    # def _retrieve_context_from_md(self, search_results: List[Dict[str, Any]], n_sentences: int = 2) -> Tuple[str, List[str]]:\n",
    "    #     context = \"\"\n",
    "    #     pages = sorted(list(set(res.get('source_page') for res in search_results if res.get('source_page'))))\n",
    "        \n",
    "    #     if not pages:\n",
    "    #         return \"\", []\n",
    "\n",
    "    #     all_md_files = [f for f in os.listdir(self.md_path) if f.endswith('.md')]\n",
    "        \n",
    "    #     page_texts = {}\n",
    "    #     for page_num_str in pages:\n",
    "    #         page_num = int(page_num_str)\n",
    "    #         for md_file in all_md_files:\n",
    "    #             with open(os.path.join(self.md_path, md_file), 'r', encoding='utf-8') as f:\n",
    "    #                 content = f.read()\n",
    "                \n",
    "    #             match = re.search(rf\"####\\s+Page\\s+{page_num}\\b(.*?)(?=####\\s+Page|\\Z)\", content, re.S)\n",
    "    #             if match:\n",
    "    #                 page_texts[page_num_str] = match.group(1).strip()\n",
    "    #                 break\n",
    "        \n",
    "    #     context_parts = []\n",
    "    #     for result in search_results:\n",
    "    #         page_num = result.get('source_page')\n",
    "    #         page_content = page_texts.get(page_num)\n",
    "            \n",
    "    #         if not page_content:\n",
    "    #             continue\n",
    "                \n",
    "    #         source_node = result.get('source_node')\n",
    "    #         target_node = result.get('target_node')\n",
    "            \n",
    "    #         sentences = sent_tokenize(page_content)\n",
    "    #         for i, sent in enumerate(sentences):\n",
    "    #             if source_node and source_node in sent and target_node and target_node in sent:\n",
    "    #                 start = max(0, i - n_sentences)\n",
    "    #                 end = min(len(sentences), i + n_sentences + 1)\n",
    "    #                 context_snippet = \" \".join(sentences[start:end])\n",
    "    #                 context_parts.append(f\"... {context_snippet} ... (출처: Page {page_num})\")\n",
    "    #                 break\n",
    "        \n",
    "    #     context = \"\\n\".join(context_parts)\n",
    "    #     return context, pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
