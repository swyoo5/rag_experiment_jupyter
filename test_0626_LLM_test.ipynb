{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23f9fc1",
   "metadata": {},
   "source": [
    "# 기본적인 질문 답변 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ca2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/LLM/deepseek-qwen-bllossom-32b 테스트\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a342775aa74fe2a4616217915f5c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 응답 : <｜begin▁of▁sentence｜>You are a helpful AI assistant.<｜User｜>What is LLM?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out what an LLM is. The user mentioned that it stands for Large Language Model, but I want to make sure I understand it thoroughly. Let me start by breaking down the term. \"Large\" probably refers to the size of the model in terms of parameters. \"Language Model\" suggests it's related to processing or generating human language. \n",
      "\n",
      "I remember hearing about models like GPT-3 or BERT. They're used for tasks like text generation, translation, and answering questions. So maybe LLMs are a category that includes these models. But how exactly do they work? I think they use machine learning, specifically deep learning, with neural networks. The architecture might involve transformers, which I've heard are a type of neural network architecture introduced by Vaswani et al. in 2017. Transformers use attention mechanisms to process sequences of data, which is useful for language tasks.\n",
      "\n",
      "Wait, so LLMs are trained on vast amounts of text data from the internet, books, articles, etc. This training allows them to learn patterns and relationships in language. When you input a prompt, the model predicts the next word based on the probability distribution of possible words, building up a response sentence by sentence. That\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    \"microsoft/Phi-mini-MoE-instruct\",\n",
    "    \"Gensyn/Qwen2.5-1.5B-Instruct\",\n",
    "    \"./model/LLM/deepseek-qwen-bllossom-32b\"\n",
    "    \n",
    "]\n",
    "question = \"What is LLM?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"You are a helpful AI assistant.\"},\n",
    "    {\"role\" : \"user\", \"content\" : question}\n",
    "]\n",
    "\n",
    "for model_name in model_list :\n",
    "    print(f\"{model_name} 테스트\")\n",
    "\n",
    "    if not model_name :\n",
    "        print(\"모델이 입력되지 않았습니다.\")\n",
    "        continue\n",
    "    if model_name.startswith(\"./\") and not os.path.isdir(model_name) :\n",
    "        print(f\"로컬 경로 {model_name}에 모델이 없음\")\n",
    "        continue\n",
    "    try :\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        prompt = pipe.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        outputs = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        print(f\"모델 응답 : {outputs[0]['generated_text']}\")\n",
    "\n",
    "    except Exception as e :\n",
    "        print(f\"{model_name} 모델을 로드하거나 사용하는 중 오류 발생 : {e}\")\n",
    "\n",
    "    finally :\n",
    "        print('-' * 20)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92136939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e6b9eb3e7044a7ab0e9f78cc8fe243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61293aa6cf342bfa59641b18a8e07e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9ae21bc77c445780fabda802c232f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff00a20d97f499aae075abba7e91f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f643afcdc64b37ba0ea2583a2084d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf4d8f9477e4cbd9b48c75e4701bb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098477d48cd541bc9aa65b0a4cac2208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec20d58ba9744b2e92675f83e9366105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c3236a7a8d44d4a05fac971351d254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c796f573dfc427f80412de185a66e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7948f8acfcb647aeb29173aa4003bbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61e742504ed48dda12629dddc86e89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fff980537e46aaadd65d12f8417a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 GPU에 로드되었습니다.\n",
      "프롬프트: What is LLM?\n",
      "모델 답변: Language Large Language Models (LLMs) are a type of deep learning model that are specifically designed to understand and generate human-like text. LLMs are trained on vast amounts of text data and are able to perform a wide range of tasks, including text summarization, translation, code generation, and information retrieval.\n",
      "\n",
      "Here are some key features of LLMs:\n",
      "\n",
      "* **Large-scale training:** LLMs are trained on massive datasets of text data, typically billions or even trillions of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"google/gemma-7b-it\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"모델이 GPU에 로드되었습니다.\")\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없어 모델이 CPU에 로드되었습니다. 추론 속도가 느릴 수 있습니다.\")\n",
    "\n",
    "prompt = \"What is LLM?\"\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id \n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0, input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"프롬프트: {prompt}\")\n",
    "print(f\"모델 답변: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 테스트 시작: google/gemma-7b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6eed5de84d4c77b4b5216039144219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'google/gemma-7b-it'이 성공적으로 로드되었습니다.\n",
      "오류: 'google/gemma-7b-it' 모델을 로드하거나 사용하는 중 문제가 발생했습니다: System role not supported\n",
      "CUDA 캐시가 비워졌습니다.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "모델 테스트 시작: microsoft/Phi-mini-MoE-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c653d71d4b54ab99a5f3aaff7984f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'microsoft/Phi-mini-MoE-instruct'이 성공적으로 로드되었습니다.\n",
      "프롬프트: What is LLM?\n",
      "모델 응답 (microsoft/Phi-mini-MoE-instruct): LLM stands for \"Master of Laws.\" It is a postgraduate academic degree in law, offered by law schools in many countries. The LLM is designed for individuals who have already completed an undergraduate law degree (such as a Juris Doctor, LLB, or JD) and wish to specialize in a particular area of law, gain advanced knowledge in a specific legal field, or prepare for a career in international law, law and business, or other specialized areas.\n",
      "\n",
      "The LLM program typically takes one year to complete, although some programs may take up to two years. Candidates may need to have a certain number of years of work experience in the legal field, depending on the country and institution offering the program. The LLM program may include courses in various areas of law, such as international law, human rights, environmental law, intellectual property law, or commercial law, among others.\n",
      "\n",
      "In addition to coursework, LLM programs often require students to complete a thesis or a research project. Some LLM programs also offer opportunities for internships, study abroad programs, or practical training in legal research and writing.\n",
      "\n",
      "Overall, the LLM degree is a valuable tool for law graduates who wish to further their legal education and specialize in a particular area of law. It is also a popular choice for non-law graduates who are interested in pursuing a career in international law or law and business.\n",
      "CUDA 캐시가 비워졌습니다.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "모델 테스트 시작: Gensyn/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'Gensyn/Qwen2.5-1.5B-Instruct'이 성공적으로 로드되었습니다.\n",
      "프롬프트: What is LLM?\n",
      "모델 응답 (Gensyn/Qwen2.5-1.5B-Instruct): LLM stands for Large Language Model. It refers to advanced artificial intelligence models that have the ability to generate human-like text across a wide range of topics, from writing stories and articles to answering questions on specific subjects. These models use deep learning algorithms to analyze vast amounts of data and learn patterns in language, allowing them to produce coherent and contextually appropriate responses.\n",
      "\n",
      "Some popular examples of large language models include:\n",
      "\n",
      "1. GPT-3 (Generative Pre-trained Transformer 3)\n",
      "2. BERT (Bidirectional Encoder Representations from Transformers)\n",
      "3. T5 (Text-to-Video)\n",
      "4. DALL-E\n",
      "\n",
      "These models have been trained using massive datasets and can perform tasks such as machine translation, question answering, summarization, and more. However, it's important to note that they may not always be accurate or fair, and their outputs should be reviewed critically before being used in serious applications.\n",
      "CUDA 캐시가 비워졌습니다.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "모델 테스트 시작: ./model/LLM/deepseek-qwen-bllossom-32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325cd960ed6d4eb69fa8e87339797ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 './model/LLM/deepseek-qwen-bllossom-32b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import os\n",
    "\n",
    "question = \"What is LLM?\"\n",
    "\n",
    "common_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "model_list = [\n",
    "    \"./model/LLM/gemma-7b\", \n",
    "    \"./model/LLM/phi-mini-moe\", \n",
    "    \"./model/LLM/qwen2.5-1.5b\", \n",
    "    \"./model/LLM/deepseek-qwen-bllossom-32b\" \n",
    "]\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"모델 테스트 시작: {model_name}\")\n",
    "\n",
    "    if model_name.startswith(\"./\") and not os.path.isdir(model_name):\n",
    "        print(f\"오류: 로컬 경로 '{model_name}'에 모델 디렉토리가 존재하지 않습니다. 스킵합니다.\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(f\"모델 '{model_name}'이 성공적으로 로드되었습니다.\")\n",
    "\n",
    "        if \"Qwen\" in model_name:\n",
    "            formatted_input = pipe.tokenizer.apply_chat_template(\n",
    "                common_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        elif \"Phi\" in model_name:\n",
    "             formatted_input = pipe.tokenizer.apply_chat_template(\n",
    "                common_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        elif \"gemma\" in model_name.lower():\n",
    "            formatted_input = pipe.tokenizer.apply_chat_template(\n",
    "                common_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else: \n",
    "            formatted_input = pipe.tokenizer.apply_chat_template(\n",
    "                common_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "        outputs = pipe(\n",
    "            formatted_input,\n",
    "            max_new_tokens=500, \n",
    "            do_sample=True, \n",
    "            temperature=0.7, \n",
    "            top_k=50,\n",
    "            top_p=0.9, \n",
    "            eos_token_id=pipe.tokenizer.eos_token_id \n",
    "        )\n",
    "\n",
    "        \n",
    "        full_generated_text = outputs[0]['generated_text']\n",
    "        answer_start_index = len(formatted_input)\n",
    "        \n",
    "        cleaned_answer = full_generated_text[answer_start_index:].strip()\n",
    "        \n",
    "        print(f\"프롬프트: {question}\")\n",
    "        print(f\"모델 응답 ({model_name}): {cleaned_answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{model_name}' 모델을 로드하거나 사용하는 중 문제가 발생했습니다: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if 'pipe' in locals():\n",
    "            del pipe\n",
    "        if 'model' in locals(): \n",
    "            del model\n",
    "        if 'tokenizer' in locals(): \n",
    "            del tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"CUDA 캐시가 비워졌습니다.\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72282841",
   "metadata": {},
   "source": [
    "# 해리포터 QA 데이터 답변 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577db11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3d032fc5d54c0ca05e91e66412f5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'gemma-7b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  14%|█▍        | 1/7 [00:23<02:18, 23.14s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01_Harry_Potter_and_the_Sorcerers_Stone.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/01_Harry_Potter_and_the_Sorcerers_Stone_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  29%|██▊       | 2/7 [00:43<01:48, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'02_Harry_Potter_and_the_Chamber_of_Secrets.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/02_Harry_Potter_and_the_Chamber_of_Secrets_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  43%|████▎     | 3/7 [01:17<01:48, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'03_Harry_Potter_and_the_Prisoner_of_Azkaban.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/03_Harry_Potter_and_the_Prisoner_of_Azkaban_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  57%|█████▋    | 4/7 [01:48<01:25, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'04_Harry_Potter_and_the_Goblet_of_Fire.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/04_Harry_Potter_and_the_Goblet_of_Fire_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  71%|███████▏  | 5/7 [02:00<00:45, 22.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'05_Harry_Potter_and_the_Order_of_the_Phoenix.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/05_Harry_Potter_and_the_Order_of_the_Phoenix_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b:  86%|████████▌ | 6/7 [02:17<00:20, 20.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'06_Harry_Potter_and_the_Half_Blood_Prince.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/06_Harry_Potter_and_the_Half_Blood_Prince_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gemma-7b: 100%|██████████| 7/7 [02:26<00:00, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'07_Harry_Potter_and_the_Deathly_Hallows.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/gemma-7b/07_Harry_Potter_and_the_Deathly_Hallows_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41e3ea703e427fb574cc71d76db62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'phi-mini-moe'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  14%|█▍        | 1/7 [03:34<21:27, 214.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01_Harry_Potter_and_the_Sorcerers_Stone.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/01_Harry_Potter_and_the_Sorcerers_Stone_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  29%|██▊       | 2/7 [06:56<17:16, 207.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'02_Harry_Potter_and_the_Chamber_of_Secrets.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/02_Harry_Potter_and_the_Chamber_of_Secrets_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  43%|████▎     | 3/7 [10:30<14:00, 210.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'03_Harry_Potter_and_the_Prisoner_of_Azkaban.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/03_Harry_Potter_and_the_Prisoner_of_Azkaban_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  57%|█████▋    | 4/7 [14:04<10:35, 211.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'04_Harry_Potter_and_the_Goblet_of_Fire.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/04_Harry_Potter_and_the_Goblet_of_Fire_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  71%|███████▏  | 5/7 [17:40<07:06, 213.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'05_Harry_Potter_and_the_Order_of_the_Phoenix.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/05_Harry_Potter_and_the_Order_of_the_Phoenix_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe:  86%|████████▌ | 6/7 [21:19<03:35, 215.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'06_Harry_Potter_and_the_Half_Blood_Prince.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/06_Harry_Potter_and_the_Half_Blood_Prince_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing phi-mini-moe: 100%|██████████| 7/7 [25:02<00:00, 214.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'07_Harry_Potter_and_the_Deathly_Hallows.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/phi-mini-moe/07_Harry_Potter_and_the_Deathly_Hallows_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'qwen2.5-1.5b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  14%|█▍        | 1/7 [00:14<01:29, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01_Harry_Potter_and_the_Sorcerers_Stone.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/01_Harry_Potter_and_the_Sorcerers_Stone_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  29%|██▊       | 2/7 [00:27<01:08, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'02_Harry_Potter_and_the_Chamber_of_Secrets.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/02_Harry_Potter_and_the_Chamber_of_Secrets_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  43%|████▎     | 3/7 [00:45<01:02, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'03_Harry_Potter_and_the_Prisoner_of_Azkaban.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/03_Harry_Potter_and_the_Prisoner_of_Azkaban_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  57%|█████▋    | 4/7 [00:55<00:39, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'04_Harry_Potter_and_the_Goblet_of_Fire.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/04_Harry_Potter_and_the_Goblet_of_Fire_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  71%|███████▏  | 5/7 [01:06<00:25, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'05_Harry_Potter_and_the_Order_of_the_Phoenix.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/05_Harry_Potter_and_the_Order_of_the_Phoenix_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b:  86%|████████▌ | 6/7 [01:15<00:11, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'06_Harry_Potter_and_the_Half_Blood_Prince.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/06_Harry_Potter_and_the_Half_Blood_Prince_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qwen2.5-1.5b: 100%|██████████| 7/7 [01:26<00:00, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'07_Harry_Potter_and_the_Deathly_Hallows.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/qwen2.5-1.5b/07_Harry_Potter_and_the_Deathly_Hallows_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176dcfa4e0c54ec6be46f9aa71496035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'deepseek-qwen-bllossom-32b'이 성공적으로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  14%|█▍        | 1/7 [06:37<39:44, 397.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01_Harry_Potter_and_the_Sorcerers_Stone.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/01_Harry_Potter_and_the_Sorcerers_Stone_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  29%|██▊       | 2/7 [13:50<34:51, 418.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'02_Harry_Potter_and_the_Chamber_of_Secrets.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/02_Harry_Potter_and_the_Chamber_of_Secrets_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  43%|████▎     | 3/7 [20:25<27:11, 407.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'03_Harry_Potter_and_the_Prisoner_of_Azkaban.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/03_Harry_Potter_and_the_Prisoner_of_Azkaban_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  57%|█████▋    | 4/7 [27:05<20:14, 404.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'04_Harry_Potter_and_the_Goblet_of_Fire.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/04_Harry_Potter_and_the_Goblet_of_Fire_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  71%|███████▏  | 5/7 [33:07<12:58, 389.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'05_Harry_Potter_and_the_Order_of_the_Phoenix.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/05_Harry_Potter_and_the_Order_of_the_Phoenix_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b:  86%|████████▌ | 6/7 [39:52<06:34, 394.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'06_Harry_Potter_and_the_Half_Blood_Prince.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/06_Harry_Potter_and_the_Half_Blood_Prince_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deepseek-qwen-bllossom-32b: 100%|██████████| 7/7 [46:47<00:00, 401.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'07_Harry_Potter_and_the_Deathly_Hallows.json' 파일의 10개 질문에 대한 답변을 저장했습니다: ./result/HarryPotter/deepseek-qwen-bllossom-32b/07_Harry_Potter_and_the_Deathly_Hallows_responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269883b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
