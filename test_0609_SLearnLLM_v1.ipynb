{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a388b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea27e2c0de304c23801b5ec3c50d60a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0297b4b6972f4f279e57439ec96dceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c691dc345e47408080bece2ff7fdda51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa2092e1271442b9eb3d718bb081eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9474c12ddd494d32bd17be9c48bc4cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 답변 생성 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "답변 생성 중: 100%|██████████| 60/60 [06:25<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLM Judge를 이용한 판단 진행 중 (다수결 투표) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "판단 진행 중:   0%|          | 0/60 [00:00<?, ?it/s]/opt/conda/envs/sangwon/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "판단 진행 중: 100%|██████████| 60/60 [01:36<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 60개의 QA 쌍 중:\n",
      "- 0개가 '오답'으로 확정되어 재학습 대상입니다.\n",
      "- 34개가 '모호'로 판단되어 사람의 검토가 필요합니다.\n",
      "모호 판단 데이터가 './data/slearn/human_review_qna_pairs.json'에 저장되었습니다.\n",
      "재학습할 오답 데이터가 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "_global_models = {}\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, judge_llm_name: str, max_seq_length: int = 512):\n",
    "        \n",
    "        config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        if judge_llm_name not in _global_models:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(judge_llm_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(judge_llm_name, torch_dtype=torch.bfloat16, quantization_config=config, device_map=\"auto\")\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "            self.model.eval()\n",
    "            \n",
    "            self.device = self.model.device\n",
    "            _global_models[judge_llm_name] = {\"tokenizer\": self.tokenizer, \"model\": self.model, \"device\": self.device}\n",
    "        else:\n",
    "            loaded_data = _global_models[judge_llm_name]\n",
    "            self.tokenizer = loaded_data[\"tokenizer\"]\n",
    "            self.model = loaded_data[\"model\"]\n",
    "            self.device = loaded_data[\"device\"]\n",
    "            \n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _create_judge_prompt(self, question: str, generated_answer: str, true_answer: str) -> str:\n",
    "        return (\n",
    "            f\"다음은 [질문], [생성된 답변], 그리고 [기준 정답]입니다.\\n\"\n",
    "            f\"[기준 정답]을 기준으로 [생성된 답변]이 정확한지 판단하세요.\\n\"\n",
    "            f\"판단은 '정답', '오답', '모호' 셋 중 하나로만 답하고, 그 뒤에 간략한 이유를 덧붙이세요.\\n\"\n",
    "            f\"[생성된 답변]이 **실제로 맞는 답변이더라도 [기준 정답]과 다르면 '오답'으로 판단**해야 합니다.\\n\"\n",
    "            f\"[생성된 답변]이 [기준 정답]과 비교하여 애매하거나, 부분적으로 맞지만 명확히 '정답'이나 '오답'으로 분류하기 어렵다면 '모호'로 판단하세요.\\n\\n\"\n",
    "            f\"[질문]: {question}\\n\"\n",
    "            f\"[생성된 답변]: {generated_answer}\\n\"\n",
    "            f\"[기준 정답]: {true_answer}\\n\\n\" \n",
    "            f\"판단: \"\n",
    "        )\n",
    "\n",
    "    def judge_answer(self, question: str, generated_answer: str, true_answer: str) -> str:\n",
    "        judge_prompt = self._create_judge_prompt(question, generated_answer, true_answer)\n",
    "        \n",
    "        inputs = self.tokenizer(judge_prompt, return_tensors=\"pt\", \n",
    "                                max_length=self.max_seq_length, truncation=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20, \n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        judge_full_output = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], \n",
    "                                                 skip_special_tokens=True).strip().lower()\n",
    "\n",
    "        \n",
    "        if \"정답\" in judge_full_output:\n",
    "            return \"정답\"\n",
    "        elif \"오답\" in judge_full_output:\n",
    "            return \"오답\"\n",
    "        elif \"모호\" in judge_full_output:\n",
    "            return \"모호\"\n",
    "        else:\n",
    "            return \"모호\" \n",
    "\n",
    "def slearn_llm_training_with_llm_judge_modular(\n",
    "    model_name: str = \"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "    data_path: str = \"./data/qna_data.json\",\n",
    "    output_dir: str = \"./model\",\n",
    "    max_seq_length: int = 512,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 2e-5,\n",
    "    num_train_epochs: int = 3,\n",
    "    eval_steps: int = 500,\n",
    "    save_steps: int = 500,\n",
    "    judge_llm_names: list = [\"./model/LLM/google_gemma-2b\", \"./model/LLM/google_gemma-2b-it\", \"./model/LLM/google_gemma-7b\", \"./model/LLM/google_gemma-7b-it\", \"./model/LLM/deepseek-ai_deepseek-llm-7b-base\"] # 5개의 LLM\n",
    "):\n",
    "    global _global_models \n",
    "\n",
    "    config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "    \n",
    "    if model_name not in _global_models:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                     torch_dtype=torch.bfloat16, \n",
    "                                                     device_map=\"auto\",\n",
    "                                                     quantization_config=config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = tokenizer.eos_token_id\n",
    "        _global_models[model_name] = {\"tokenizer\": tokenizer, \"model\": model, \"device\": model.device}\n",
    "    else:\n",
    "        loaded_data = _global_models[model_name]\n",
    "        tokenizer = loaded_data[\"tokenizer\"]\n",
    "        model = loaded_data[\"model\"]\n",
    "    \n",
    "    \n",
    "    llm_judges = [LLMJudge(judge_llm_name=name, max_seq_length=max_seq_length) for name in judge_llm_names]\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        qna_data = json.load(f)\n",
    "\n",
    "    generated_qna_data = []\n",
    "    print(\"--- 답변 생성 중 ---\")\n",
    "    for item in tqdm(qna_data, desc=\"답변 생성 중\"):\n",
    "        question = item['question']\n",
    "        # context = item['context']\n",
    "        true_answer = item['answer']\n",
    "\n",
    "        input_text_prompt = f\"질문 : {question}\\n답변 : \"\n",
    "        \n",
    "        # input_text_prompt = (\n",
    "        #                 f\"당신은 주어진 질문에 대해 답변할 수 있다면 상세하게 답변하고, \"\n",
    "        #                 f\"**만약 질문에 대한 답변이 불가능하거나 알 수 없다면 '죄송합니다. 이 질문에 대해서는 답변해 드릴 수 없습니다.'라고만 답변하세요.**\\n\\n\"\n",
    "        #                 f\"질문: {question}\\n\"\n",
    "        #                 f\"답변: \"\n",
    "        #             )\n",
    "        inputs = tokenizer(input_text_prompt, return_tensors=\"pt\", max_length=max_seq_length, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        generated_qna_data.append({\n",
    "            \"question\": question,\n",
    "            # \"context\": context,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"input_text_prompt\": input_text_prompt\n",
    "        })\n",
    "\n",
    "    incorrect_qna_pairs = [] \n",
    "    human_review_qna_pairs = [] \n",
    "    \n",
    "    output_results_dir = \"./data/slearn\"\n",
    "    os.makedirs(output_results_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- LLM Judge를 이용한 판단 진행 중 (다수결 투표) ---\")\n",
    "    for idx, item in enumerate(tqdm(generated_qna_data, desc=\"판단 진행 중\")):\n",
    "        judgements = []\n",
    "        for judge in llm_judges:\n",
    "            judgements.append(judge.judge_answer(\n",
    "                question=item['question'],\n",
    "                generated_answer=item['generated_answer'],\n",
    "                true_answer=item['true_answer']\n",
    "            ))\n",
    "        \n",
    "        vote_counts = Counter(judgements)\n",
    "        \n",
    "        final_decision = \"모호\" \n",
    "        if vote_counts['정답'] >= 3:\n",
    "            final_decision = \"정답\"\n",
    "        elif vote_counts['오답'] >= 3:\n",
    "            final_decision = \"오답\"\n",
    "        \n",
    "        if final_decision == \"오답\":\n",
    "            incorrect_qna_pairs.append({\n",
    "                \"question\": item['question'],\n",
    "                # \"context\": item['context'],\n",
    "                \"answer\": item['true_answer'],\n",
    "                \"input_text\": item['input_text_prompt'],\n",
    "                \"output_text\": item['true_answer']\n",
    "            })\n",
    "        elif final_decision == \"모호\":\n",
    "            human_review_qna_pairs.append({\n",
    "                \"question\": item['question'],\n",
    "                # \"context\": item['context'],\n",
    "                \"true_answer\": item['true_answer'],\n",
    "                \"generated_answer\": item['generated_answer'],\n",
    "                \"individual_judgements\": judgements \n",
    "            })\n",
    "        \n",
    "        file_name = os.path.join(output_results_dir, f\"slearn_result_{idx+1:05d}.txt\")\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"<질문>: {item['question']}\\n\\n\")\n",
    "            # f.write(f\"<문서>: {item['context']}\\n\\n\")\n",
    "            f.write(f\"<생성된 답변>: {item['generated_answer']}\\n\\n\")\n",
    "            f.write(f\"<기준 정답>: {item['true_answer']}\\n\\n\")\n",
    "            f.write(f\"<각 LLM 판단 결과>: {judgements}\\n\") \n",
    "            f.write(f\"<최종 판단 결과 (다수결)>: {final_decision}\\n\")\n",
    "\n",
    "    print(f\"\\n총 {len(qna_data)}개의 QA 쌍 중:\")\n",
    "    print(f\"- {len(incorrect_qna_pairs)}개가 '오답'으로 확정되어 재학습 대상입니다.\")\n",
    "    print(f\"- {len(human_review_qna_pairs)}개가 '모호'로 판단되어 사람의 검토가 필요합니다.\")\n",
    "    \n",
    "    \n",
    "    if incorrect_qna_pairs:\n",
    "        with open(os.path.join(output_results_dir, \"incorrect_qna_pairs.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(incorrect_qna_pairs, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"오답 데이터가 '{os.path.join(output_results_dir, 'incorrect_qna_pairs.json')}'에 저장되었습니다.\")\n",
    "    \n",
    "    \n",
    "    if human_review_qna_pairs:\n",
    "        with open(os.path.join(output_results_dir, \"human_review_qna_pairs.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(human_review_qna_pairs, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"모호 판단 데이터가 '{os.path.join(output_results_dir, 'human_review_qna_pairs.json')}'에 저장되었습니다.\")\n",
    "\n",
    "    \n",
    "    if not incorrect_qna_pairs:\n",
    "        print(\"재학습할 오답 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    filtered_data = []\n",
    "    for item in incorrect_qna_pairs:\n",
    "        filtered_data.append({\n",
    "            \"input_text\": item[\"input_text\"],\n",
    "            \"output_text\": item[\"output_text\"]\n",
    "        })\n",
    "\n",
    "    dataset = Dataset.from_list(filtered_data)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        full_text = [\n",
    "            f\"{examples['input_text'][i]}{examples['output_text'][i]}{tokenizer.eos_token}\"\n",
    "            for i in range(len(examples['input_text']))\n",
    "        ]\n",
    "        \n",
    "        tokenized_inputs = tokenizer(\n",
    "            full_text,\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i in range(len(examples['input_text'])):\n",
    "            input_prompt_len = len(tokenizer(examples['input_text'][i], truncation=False)['input_ids'])\n",
    "            \n",
    "            label = list(tokenized_inputs['input_ids'][i])\n",
    "            for j in range(input_prompt_len):\n",
    "                label[j] = -100\n",
    "            labels.append(label)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        evaluation_strategy=\"no\", \n",
    "        # eval_steps=eval_steps, \n",
    "        load_best_model_at_end=False, \n",
    "        # metric_for_best_model=\"loss\", \n",
    "        # greater_is_better=False,\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- 오답 데이터를 이용한 모델 재학습 시작 ---\")\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"\\n재학습된 모델이 '{output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    slearn_llm_training_with_llm_judge_modular(\n",
    "        model_name=\"./model/LLM/deepseek-qwen-bllossom-32b\",\n",
    "        data_path=\"./data/qna_data.json\",\n",
    "        output_dir=\"./model/LLM/slearn_llm_finetuned_llm_judge_modular\",\n",
    "        max_seq_length=512,\n",
    "        batch_size=2,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        judge_llm_names=[\n",
    "            \"./model/LLM/google_gemma-2b-it\", \n",
    "            \"./model/LLM/google_gemma-2b\", \n",
    "            \"./model/LLM/google_gemma-7b-it\", \n",
    "            \"./model/LLM/google_gemma-7b\", \n",
    "            \"./model/LLM/EleutherAI_gpt-neo-125m\" \n",
    "        ] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# models_to_download = [\n",
    "#     \"google/gemma-2b-it\",\n",
    "#     \"google/gemma-2b\",\n",
    "#     \"google/gemma-7b-it\",\n",
    "#     \"google/gemma-7b\",\n",
    "#     \"EleutherAI/gpt-neo-125m\"\n",
    "#     \"\"\n",
    "# ]\n",
    "\n",
    "# base_download_dir = \"./model/LLM\"\n",
    "\n",
    "# for model_name in models_to_download:\n",
    "#     save_path = os.path.join(base_download_dir, model_name.replace(\"/\", \"_\"))\n",
    "    \n",
    "#     print(f\"Downloading {model_name} to {save_path} (excluding .gguf files)...\")\n",
    "#     try:\n",
    "#         snapshot_download(\n",
    "#             repo_id=model_name,\n",
    "#             local_dir=save_path,\n",
    "#             allow_patterns=[\"*\"],  \n",
    "#             ignore_patterns=[\"*.gguf\"], \n",
    "#         )\n",
    "#         print(f\"Successfully downloaded {model_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading {model_name}: {e}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# print(\"\\nAll specified models have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e9c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
